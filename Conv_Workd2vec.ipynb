{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import useful library\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import sklearn as sk\n",
    "import pandas\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import os\n",
    "import collections\n",
    "import math\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "from sklearn import metrics\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from matplotlib import pylab\n",
    "\n",
    "# Allow print picture or graph in Ipython\n",
    "%matplotlib inline\n",
    "# Allow print all arry if command print called\n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from load_data import load_file, separate_data_label, label_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from text_cnn import TextCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from plotting import scatterplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load classified Document\n",
    "document1 = load_file(\"2007ChallengeTrainData.xml\")\n",
    "id_doc1, labels, words1 = separate_data_label(document1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoding words by one-hot encoding\n",
    "max_document_length = max([len(x.split(\" \")) for x in words1])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(words1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# One-hot-encoding label of each document\n",
    "# There are 978 documents and 45 labels\n",
    "y = label_encoding(labels)                                      # Encode data Label in DataFrame type\n",
    "y = y.as_matrix()                                               # Convert DataFrame type to numpy array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Randomly shuffle data\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 1551\n",
      "Train/Dev split: 782/196\n"
     ]
    }
   ],
   "source": [
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "# split data into training (80%) and test (20%)\n",
    "x_train, x_dev = train_test_split(x_shuffled, train_size = 0.8)\n",
    "y_train, y_dev = train_test_split(y_shuffled, train_size = 0.8)\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dim = 128\n",
    "filter_sizes = [3, 4, 5]\n",
    "num_filters = 128\n",
    "dropout_keep_prob = 0.5\n",
    "l2_reg_lambda = 0.0\n",
    "batch_size = 64\n",
    "num_epochs = 200\n",
    "evaluate_every = 100\n",
    "checkpoint_every = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1474624998\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "# ==================================================\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=True,\n",
    "      log_device_placement=False)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(\n",
    "            sequence_length=x_train.shape[1],\n",
    "            num_classes=45,\n",
    "            vocab_size=len(vocab_processor.vocabulary_),\n",
    "            embedding_size=embedding_dim,\n",
    "            filter_sizes=filter_sizes,\n",
    "            num_filters=num_filters,\n",
    "            l2_reg_lambda=l2_reg_lambda)\n",
    "\n",
    "        # Define Training procedure\n",
    "        global_step = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "\n",
    "        # Keep track of gradient values and sparsity (optional)\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "\n",
    "        # Output directory for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "\n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "\n",
    "        # Train Summaries\n",
    "        train_summary_op = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph)\n",
    "\n",
    "        # Dev summaries\n",
    "        dev_summary_op = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph)\n",
    "\n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver(tf.all_variables())\n",
    "\n",
    "        # Store vocabulary embedded\n",
    "        #dictionary.save(os.path.join(out_dir, \"vocab\"))\n",
    "\n",
    "        # Initialize all variables\n",
    "        sess.run(tf.initialize_all_variables())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort each data and label into one pair\n",
    "# then shuffle to ensure randomness \n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = batch_iter(\n",
    "        list(zip(x_train, y_train)), batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_step(x_batch, y_batch):\n",
    "    \"\"\"\n",
    "    A single training step\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        cnn.input_x: x_batch,\n",
    "        cnn.input_y: y_batch,\n",
    "        cnn.dropout_keep_prob: dropout_keep_prob\n",
    "    }\n",
    "    _, step, summaries, loss, accuracy = sess.run(\n",
    "        [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy],\n",
    "        feed_dict)\n",
    "    print(\"step {}, loss {:g}, acc {:g}\".format(step, loss, accuracy))\n",
    "    train_summary_writer.add_summary(summaries, step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dev_step(x_batch, y_batch, writer=None):\n",
    "    \"\"\"\n",
    "    Evaluates model on a dev set\n",
    "    \"\"\"\n",
    "    feed_dict = {\n",
    "        cnn.input_x: x_batch,\n",
    "        cnn.input_y: y_batch,\n",
    "        cnn.dropout_keep_prob: 1.0\n",
    "    }\n",
    "    step, summaries, loss, accuracy, y_p = sess.run(\n",
    "        [global_step, dev_summary_op, cnn.loss, cnn.accuracy, cnn.y_p],\n",
    "        feed_dict)\n",
    "    y_true = np.argmax(y_dev,1)\n",
    "    precision = sk.metrics.precision_score(y_true, y_p)\n",
    "    recall = sk.metrics.recall_score(y_true, y_p)\n",
    "    f1_score = sk.metrics.f1_score(y_true, y_p)\n",
    "    print(\"step {}, loss {:g}, acc {:g}, precison {}, recall {}, f1_score {}.\".format(step, loss, accuracy, precision, recall, f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1, loss 11.729, acc 0.03125\n",
      "step 2, loss 11.6866, acc 0.03125\n",
      "step 3, loss 10.0553, acc 0.03125\n",
      "step 4, loss 8.00012, acc 0.078125\n",
      "step 5, loss 7.73434, acc 0.09375\n",
      "step 6, loss 7.16628, acc 0.0625\n",
      "step 7, loss 8.47356, acc 0.09375\n",
      "step 8, loss 6.73954, acc 0.109375\n",
      "step 9, loss 7.83743, acc 0.046875\n",
      "step 10, loss 7.63223, acc 0.0625\n",
      "step 11, loss 7.29401, acc 0.15625\n",
      "step 12, loss 8.91101, acc 0.15625\n",
      "step 13, loss 7.00955, acc 0.142857\n",
      "step 14, loss 7.45694, acc 0.125\n",
      "step 15, loss 8.72629, acc 0.109375\n",
      "step 16, loss 7.1624, acc 0.09375\n",
      "step 17, loss 7.65224, acc 0.109375\n",
      "step 18, loss 7.25049, acc 0.109375\n",
      "step 19, loss 6.82871, acc 0.09375\n",
      "step 20, loss 7.06514, acc 0.125\n",
      "step 21, loss 6.52199, acc 0.15625\n",
      "step 22, loss 7.99467, acc 0.125\n",
      "step 23, loss 7.53479, acc 0.140625\n",
      "step 24, loss 6.13352, acc 0.171875\n",
      "step 25, loss 7.03645, acc 0.109375\n",
      "step 26, loss 7.87081, acc 0.0714286\n",
      "step 27, loss 7.75492, acc 0.109375\n",
      "step 28, loss 5.92623, acc 0.171875\n",
      "step 29, loss 6.23983, acc 0.125\n",
      "step 30, loss 6.18412, acc 0.25\n",
      "step 31, loss 7.06746, acc 0.125\n",
      "step 32, loss 7.78798, acc 0.078125\n",
      "step 33, loss 7.31525, acc 0.171875\n",
      "step 34, loss 7.26937, acc 0.109375\n",
      "step 35, loss 6.56249, acc 0.1875\n",
      "step 36, loss 6.22508, acc 0.15625\n",
      "step 37, loss 5.99299, acc 0.15625\n",
      "step 38, loss 5.65994, acc 0.234375\n",
      "step 39, loss 6.68081, acc 0.0714286\n",
      "step 40, loss 5.36157, acc 0.125\n",
      "step 41, loss 5.94501, acc 0.15625\n",
      "step 42, loss 7.39016, acc 0.140625\n",
      "step 43, loss 6.28975, acc 0.109375\n",
      "step 44, loss 5.26858, acc 0.234375\n",
      "step 45, loss 5.60414, acc 0.171875\n",
      "step 46, loss 5.65504, acc 0.15625\n",
      "step 47, loss 6.22138, acc 0.15625\n",
      "step 48, loss 5.87809, acc 0.140625\n",
      "step 49, loss 5.98271, acc 0.1875\n",
      "step 50, loss 6.86777, acc 0.171875\n",
      "step 51, loss 6.02305, acc 0.125\n",
      "step 52, loss 4.62556, acc 0.357143\n",
      "step 53, loss 5.47998, acc 0.15625\n",
      "step 54, loss 6.32886, acc 0.125\n",
      "step 55, loss 5.66416, acc 0.1875\n",
      "step 56, loss 5.26638, acc 0.171875\n",
      "step 57, loss 6.43767, acc 0.171875\n",
      "step 58, loss 4.83036, acc 0.28125\n",
      "step 59, loss 6.2851, acc 0.15625\n",
      "step 60, loss 4.62643, acc 0.234375\n",
      "step 61, loss 5.73841, acc 0.25\n",
      "step 62, loss 4.99711, acc 0.1875\n",
      "step 63, loss 4.43863, acc 0.1875\n",
      "step 64, loss 6.02749, acc 0.1875\n",
      "step 65, loss 5.78405, acc 0.214286\n",
      "step 66, loss 4.18466, acc 0.234375\n",
      "step 67, loss 5.34845, acc 0.234375\n",
      "step 68, loss 6.0443, acc 0.1875\n",
      "step 69, loss 6.71691, acc 0.15625\n",
      "step 70, loss 5.39917, acc 0.15625\n",
      "step 71, loss 6.19393, acc 0.109375\n",
      "step 72, loss 5.85875, acc 0.125\n",
      "step 73, loss 5.12217, acc 0.203125\n",
      "step 74, loss 5.51387, acc 0.203125\n",
      "step 75, loss 5.15352, acc 0.25\n",
      "step 76, loss 5.06355, acc 0.234375\n",
      "step 77, loss 5.15887, acc 0.21875\n",
      "step 78, loss 6.47077, acc 0.142857\n",
      "step 79, loss 5.5211, acc 0.375\n",
      "step 80, loss 6.14574, acc 0.171875\n",
      "step 81, loss 5.24352, acc 0.1875\n",
      "step 82, loss 6.26974, acc 0.09375\n",
      "step 83, loss 5.16262, acc 0.203125\n",
      "step 84, loss 5.37375, acc 0.140625\n",
      "step 85, loss 4.6506, acc 0.328125\n",
      "step 86, loss 5.3456, acc 0.265625\n",
      "step 87, loss 4.85765, acc 0.234375\n",
      "step 88, loss 5.26828, acc 0.203125\n",
      "step 89, loss 5.00677, acc 0.171875\n",
      "step 90, loss 4.6023, acc 0.203125\n",
      "step 91, loss 2.87244, acc 0.285714\n",
      "step 92, loss 5.07637, acc 0.1875\n",
      "step 93, loss 5.85386, acc 0.125\n",
      "step 94, loss 5.05765, acc 0.28125\n",
      "step 95, loss 5.01263, acc 0.1875\n",
      "step 96, loss 4.00687, acc 0.296875\n",
      "step 97, loss 4.6294, acc 0.25\n",
      "step 98, loss 4.98108, acc 0.171875\n",
      "step 99, loss 4.98944, acc 0.328125\n",
      "step 100, loss 4.71766, acc 0.265625\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1074: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 100, loss 5.52084, acc 0.147959, precison 0.0694134796776, recall 0.147959183673, f1_score 0.0754224270353.\n",
      "\n",
      "step 101, loss 5.66475, acc 0.21875\n",
      "step 102, loss 4.61649, acc 0.3125\n",
      "step 103, loss 4.96718, acc 0.3125\n",
      "step 104, loss 4.4648, acc 0.357143\n",
      "step 105, loss 4.79152, acc 0.25\n",
      "step 106, loss 5.17589, acc 0.265625\n",
      "step 107, loss 4.12967, acc 0.328125\n",
      "step 108, loss 5.31896, acc 0.265625\n",
      "step 109, loss 6.37365, acc 0.15625\n",
      "step 110, loss 5.59788, acc 0.234375\n",
      "step 111, loss 5.27519, acc 0.171875\n",
      "step 112, loss 5.54012, acc 0.140625\n",
      "step 113, loss 5.78326, acc 0.25\n",
      "step 114, loss 5.11888, acc 0.25\n",
      "step 115, loss 4.54182, acc 0.25\n",
      "step 116, loss 5.53017, acc 0.28125\n",
      "step 117, loss 3.1244, acc 0.285714\n",
      "step 118, loss 5.03389, acc 0.203125\n",
      "step 119, loss 5.81637, acc 0.25\n",
      "step 120, loss 4.81936, acc 0.25\n",
      "step 121, loss 4.49979, acc 0.28125\n",
      "step 122, loss 4.99756, acc 0.171875\n",
      "step 123, loss 4.2555, acc 0.328125\n",
      "step 124, loss 5.62062, acc 0.25\n",
      "step 125, loss 4.8206, acc 0.3125\n",
      "step 126, loss 5.26249, acc 0.171875\n",
      "step 127, loss 4.76126, acc 0.296875\n",
      "step 128, loss 5.26203, acc 0.265625\n",
      "step 129, loss 4.50147, acc 0.296875\n",
      "step 130, loss 4.89451, acc 0.214286\n",
      "step 131, loss 4.47951, acc 0.390625\n",
      "step 132, loss 3.92657, acc 0.28125\n",
      "step 133, loss 4.48591, acc 0.265625\n",
      "step 134, loss 4.80579, acc 0.21875\n",
      "step 135, loss 4.61771, acc 0.21875\n",
      "step 136, loss 5.77231, acc 0.1875\n",
      "step 137, loss 4.60238, acc 0.28125\n",
      "step 138, loss 4.80026, acc 0.25\n",
      "step 139, loss 5.06946, acc 0.359375\n",
      "step 140, loss 5.30402, acc 0.296875\n",
      "step 141, loss 5.5301, acc 0.28125\n",
      "step 142, loss 4.17924, acc 0.28125\n",
      "step 143, loss 5.36081, acc 0.357143\n",
      "step 144, loss 4.37151, acc 0.34375\n",
      "step 145, loss 4.93205, acc 0.296875\n",
      "step 146, loss 4.5147, acc 0.3125\n",
      "step 147, loss 5.14468, acc 0.28125\n",
      "step 148, loss 4.50781, acc 0.234375\n",
      "step 149, loss 4.83744, acc 0.265625\n",
      "step 150, loss 5.49221, acc 0.28125\n",
      "step 151, loss 5.83522, acc 0.25\n",
      "step 152, loss 5.95445, acc 0.140625\n",
      "step 153, loss 5.16797, acc 0.25\n",
      "step 154, loss 5.53881, acc 0.328125\n",
      "step 155, loss 6.02242, acc 0.21875\n",
      "step 156, loss 7.77907, acc 0.142857\n",
      "step 157, loss 5.55435, acc 0.25\n",
      "step 158, loss 5.01652, acc 0.28125\n",
      "step 159, loss 5.23924, acc 0.296875\n",
      "step 160, loss 5.32731, acc 0.25\n",
      "step 161, loss 5.75886, acc 0.21875\n",
      "step 162, loss 4.88935, acc 0.3125\n",
      "step 163, loss 5.76922, acc 0.296875\n",
      "step 164, loss 5.40511, acc 0.28125\n",
      "step 165, loss 4.74008, acc 0.296875\n",
      "step 166, loss 5.4808, acc 0.296875\n",
      "step 167, loss 5.97425, acc 0.25\n",
      "step 168, loss 5.26879, acc 0.3125\n",
      "step 169, loss 6.98168, acc 0.142857\n",
      "step 170, loss 5.46581, acc 0.21875\n",
      "step 171, loss 4.96062, acc 0.359375\n",
      "step 172, loss 6.10589, acc 0.25\n",
      "step 173, loss 6.6353, acc 0.265625\n",
      "step 174, loss 6.27828, acc 0.296875\n",
      "step 175, loss 5.18495, acc 0.265625\n",
      "step 176, loss 5.3737, acc 0.21875\n",
      "step 177, loss 4.60447, acc 0.265625\n",
      "step 178, loss 6.30873, acc 0.203125\n",
      "step 179, loss 5.70153, acc 0.296875\n",
      "step 180, loss 5.17466, acc 0.359375\n",
      "step 181, loss 6.73998, acc 0.203125\n",
      "step 182, loss 3.87557, acc 0.142857\n",
      "step 183, loss 5.78513, acc 0.375\n",
      "step 184, loss 4.42904, acc 0.28125\n",
      "step 185, loss 6.65555, acc 0.25\n",
      "step 186, loss 6.14354, acc 0.234375\n",
      "step 187, loss 6.64174, acc 0.21875\n",
      "step 188, loss 4.83064, acc 0.25\n",
      "step 189, loss 7.46526, acc 0.15625\n",
      "step 190, loss 6.38678, acc 0.328125\n",
      "step 191, loss 7.23449, acc 0.25\n",
      "step 192, loss 7.32073, acc 0.21875\n",
      "step 193, loss 7.12414, acc 0.265625\n",
      "step 194, loss 6.03364, acc 0.328125\n",
      "step 195, loss 6.23352, acc 0.214286\n",
      "step 196, loss 7.96236, acc 0.28125\n",
      "step 197, loss 5.47636, acc 0.28125\n",
      "step 198, loss 6.64437, acc 0.234375\n",
      "step 199, loss 5.11448, acc 0.25\n",
      "step 200, loss 6.26241, acc 0.171875\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 200, loss 8.78048, acc 0.142857, precison 0.0768164885002, recall 0.142857142857, f1_score 0.0752513756973.\n",
      "\n",
      "step 201, loss 6.71003, acc 0.1875\n",
      "step 202, loss 6.55135, acc 0.3125\n",
      "step 203, loss 6.23669, acc 0.3125\n",
      "step 204, loss 7.46163, acc 0.203125\n",
      "step 205, loss 7.03062, acc 0.28125\n",
      "step 206, loss 6.15906, acc 0.328125\n",
      "step 207, loss 7.38815, acc 0.203125\n",
      "step 208, loss 7.89977, acc 0.571429\n",
      "step 209, loss 6.4608, acc 0.25\n",
      "step 210, loss 7.61431, acc 0.34375\n",
      "step 211, loss 7.37469, acc 0.234375\n",
      "step 212, loss 5.39331, acc 0.15625\n",
      "step 213, loss 7.98626, acc 0.3125\n",
      "step 214, loss 4.99556, acc 0.328125\n",
      "step 215, loss 6.32854, acc 0.203125\n",
      "step 216, loss 7.53699, acc 0.234375\n",
      "step 217, loss 7.97011, acc 0.234375\n",
      "step 218, loss 8.67534, acc 0.21875\n",
      "step 219, loss 7.85673, acc 0.265625\n",
      "step 220, loss 7.52277, acc 0.28125\n",
      "step 221, loss 13.5907, acc 0\n",
      "step 222, loss 9.23382, acc 0.203125\n",
      "step 223, loss 9.26068, acc 0.25\n",
      "step 224, loss 10.3793, acc 0.203125\n",
      "step 225, loss 9.58157, acc 0.109375\n",
      "step 226, loss 10.2604, acc 0.1875\n",
      "step 227, loss 7.51857, acc 0.21875\n",
      "step 228, loss 9.80805, acc 0.1875\n",
      "step 229, loss 9.00964, acc 0.171875\n",
      "step 230, loss 9.9988, acc 0.1875\n",
      "step 231, loss 10.547, acc 0.28125\n",
      "step 232, loss 9.02103, acc 0.375\n",
      "step 233, loss 8.69695, acc 0.109375\n",
      "step 234, loss 7.66359, acc 0.357143\n",
      "step 235, loss 9.29443, acc 0.21875\n",
      "step 236, loss 12.3792, acc 0.203125\n",
      "step 237, loss 11.3152, acc 0.265625\n",
      "step 238, loss 12.5713, acc 0.265625\n",
      "step 239, loss 11.0819, acc 0.21875\n",
      "step 240, loss 10.7965, acc 0.171875\n",
      "step 241, loss 12.7578, acc 0.15625\n",
      "step 242, loss 9.37901, acc 0.171875\n",
      "step 243, loss 10.3497, acc 0.203125\n",
      "step 244, loss 9.62782, acc 0.15625\n",
      "step 245, loss 13.4532, acc 0.21875\n",
      "step 246, loss 13.7084, acc 0.203125\n",
      "step 247, loss 8.66576, acc 0.142857\n",
      "step 248, loss 10.7276, acc 0.265625\n",
      "step 249, loss 13.4188, acc 0.234375\n",
      "step 250, loss 12.7506, acc 0.28125\n",
      "step 251, loss 12.3161, acc 0.15625\n",
      "step 252, loss 10.5753, acc 0.265625\n",
      "step 253, loss 16.8753, acc 0.15625\n",
      "step 254, loss 15.3785, acc 0.234375\n",
      "step 255, loss 14.2048, acc 0.109375\n",
      "step 256, loss 15.7588, acc 0.140625\n",
      "step 257, loss 14.4704, acc 0.125\n",
      "step 258, loss 16.2297, acc 0.140625\n",
      "step 259, loss 15.654, acc 0.0625\n",
      "step 260, loss 9.80244, acc 0.142857\n",
      "step 261, loss 20.9023, acc 0.171875\n",
      "step 262, loss 16.022, acc 0.125\n",
      "step 263, loss 22.5175, acc 0.1875\n",
      "step 264, loss 20.3984, acc 0.21875\n",
      "step 265, loss 15.761, acc 0.234375\n",
      "step 266, loss 18.8581, acc 0.1875\n",
      "step 267, loss 16.8136, acc 0.203125\n",
      "step 268, loss 16.9304, acc 0.25\n",
      "step 269, loss 25.266, acc 0.234375\n",
      "step 270, loss 18.0764, acc 0.140625\n",
      "step 271, loss 22.3577, acc 0.140625\n",
      "step 272, loss 18.1806, acc 0.21875\n",
      "step 273, loss 18.753, acc 0.142857\n",
      "step 274, loss 19.5676, acc 0.15625\n",
      "step 275, loss 23.7516, acc 0.234375\n",
      "step 276, loss 17.4016, acc 0.203125\n",
      "step 277, loss 28.0159, acc 0.15625\n",
      "step 278, loss 21.5414, acc 0.171875\n",
      "step 279, loss 28.9123, acc 0.109375\n",
      "step 280, loss 26.7241, acc 0.0625\n",
      "step 281, loss 24.5132, acc 0.0625\n",
      "step 282, loss 23.1676, acc 0.203125\n",
      "step 283, loss 21.8344, acc 0.140625\n",
      "step 284, loss 26.8759, acc 0.234375\n",
      "step 285, loss 30.6997, acc 0.15625\n",
      "step 286, loss 21.1081, acc 0.142857\n",
      "step 287, loss 27.2514, acc 0.1875\n",
      "step 288, loss 22.6785, acc 0.140625\n",
      "step 289, loss 28.96, acc 0.1875\n",
      "step 290, loss 26.5584, acc 0.125\n",
      "step 291, loss 27.555, acc 0.203125\n",
      "step 292, loss 40.6716, acc 0.15625\n",
      "step 293, loss 34.9799, acc 0.109375\n",
      "step 294, loss 35.255, acc 0.109375\n",
      "step 295, loss 37.6372, acc 0.125\n",
      "step 296, loss 46.7227, acc 0.078125\n",
      "step 297, loss 36.0141, acc 0.140625\n",
      "step 298, loss 33.0194, acc 0.21875\n",
      "step 299, loss 36.0762, acc 0.0714286\n",
      "step 300, loss 34.7642, acc 0.203125\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 300, loss 62.8338, acc 0.163265, precison 0.0276295133438, recall 0.163265306122, f1_score 0.047261009667.\n",
      "\n",
      "step 301, loss 50.8042, acc 0.125\n",
      "step 302, loss 38.6909, acc 0.203125\n",
      "step 303, loss 44.6572, acc 0.109375\n",
      "step 304, loss 58.7583, acc 0.140625\n",
      "step 305, loss 44.4392, acc 0.0625\n",
      "step 306, loss 44.0408, acc 0.109375\n",
      "step 307, loss 33.1753, acc 0.171875\n",
      "step 308, loss 52.7302, acc 0.078125\n",
      "step 309, loss 48.0667, acc 0.09375\n",
      "step 310, loss 60.5225, acc 0.078125\n",
      "step 311, loss 48.3703, acc 0.125\n",
      "step 312, loss 67.8425, acc 0.142857\n",
      "step 313, loss 58.9102, acc 0.140625\n",
      "step 314, loss 64.3195, acc 0.15625\n",
      "step 315, loss 56.3616, acc 0.140625\n",
      "step 316, loss 49.4108, acc 0.125\n",
      "step 317, loss 72.1514, acc 0.046875\n",
      "step 318, loss 67.9442, acc 0.140625\n",
      "step 319, loss 72.5589, acc 0.09375\n",
      "step 320, loss 52.608, acc 0.140625\n",
      "step 321, loss 55.1581, acc 0.109375\n",
      "step 322, loss 57.2122, acc 0.125\n",
      "step 323, loss 75.5431, acc 0.140625\n",
      "step 324, loss 58.5798, acc 0.109375\n",
      "step 325, loss 107.026, acc 0.214286\n",
      "step 326, loss 57.414, acc 0.109375\n",
      "step 327, loss 70.0218, acc 0.046875\n",
      "step 328, loss 71.6767, acc 0.109375\n",
      "step 329, loss 64.9807, acc 0.046875\n",
      "step 330, loss 113.141, acc 0.140625\n",
      "step 331, loss 74.3982, acc 0.078125\n",
      "step 332, loss 80.021, acc 0.078125\n",
      "step 333, loss 60.6173, acc 0.09375\n",
      "step 334, loss 82.4269, acc 0.0625\n",
      "step 335, loss 89.3818, acc 0.09375\n",
      "step 336, loss 117.395, acc 0.0625\n",
      "step 337, loss 104.354, acc 0.15625\n",
      "step 338, loss 71.0125, acc 0.142857\n",
      "step 339, loss 74.4167, acc 0.21875\n",
      "step 340, loss 96.7379, acc 0.078125\n",
      "step 341, loss 82.0895, acc 0.125\n",
      "step 342, loss 98.3284, acc 0.09375\n",
      "step 343, loss 132.827, acc 0.109375\n",
      "step 344, loss 107.753, acc 0.0625\n",
      "step 345, loss 97.5728, acc 0.1875\n",
      "step 346, loss 128.971, acc 0.0625\n",
      "step 347, loss 123.113, acc 0.09375\n",
      "step 348, loss 122.005, acc 0.0625\n",
      "step 349, loss 120.741, acc 0.078125\n",
      "step 350, loss 130.305, acc 0.09375\n",
      "step 351, loss 125.676, acc 0.0714286\n",
      "step 352, loss 116.304, acc 0.15625\n",
      "step 353, loss 113.678, acc 0.140625\n",
      "step 354, loss 131.82, acc 0.078125\n",
      "step 355, loss 119.232, acc 0.125\n",
      "step 356, loss 150.845, acc 0.140625\n",
      "step 357, loss 136.695, acc 0.0625\n",
      "step 358, loss 152.7, acc 0.046875\n",
      "step 359, loss 150.638, acc 0.109375\n",
      "step 360, loss 144.763, acc 0.140625\n",
      "step 361, loss 119.78, acc 0.140625\n",
      "step 362, loss 128.285, acc 0.125\n",
      "step 363, loss 180.041, acc 0.0625\n",
      "step 364, loss 151.702, acc 0.0714286\n",
      "step 365, loss 164.954, acc 0.046875\n",
      "step 366, loss 156.408, acc 0.09375\n",
      "step 367, loss 151.097, acc 0.140625\n",
      "step 368, loss 165.677, acc 0.109375\n",
      "step 369, loss 187.792, acc 0.140625\n",
      "step 370, loss 146.934, acc 0.1875\n",
      "step 371, loss 167.643, acc 0.15625\n",
      "step 372, loss 205.819, acc 0.078125\n",
      "step 373, loss 192.358, acc 0.125\n",
      "step 374, loss 155.065, acc 0.046875\n",
      "step 375, loss 182.434, acc 0.140625\n",
      "step 376, loss 209.615, acc 0.109375\n",
      "step 377, loss 112.289, acc 0.0714286\n",
      "step 378, loss 237.238, acc 0.140625\n",
      "step 379, loss 184.107, acc 0.078125\n",
      "step 380, loss 215.885, acc 0.078125\n",
      "step 381, loss 151.687, acc 0.09375\n",
      "step 382, loss 169.646, acc 0.09375\n",
      "step 383, loss 183.696, acc 0.078125\n",
      "step 384, loss 259.559, acc 0.046875\n",
      "step 385, loss 209.691, acc 0.109375\n",
      "step 386, loss 275.206, acc 0.109375\n",
      "step 387, loss 210.619, acc 0.078125\n",
      "step 388, loss 241.048, acc 0.125\n",
      "step 389, loss 167.904, acc 0.09375\n",
      "step 390, loss 135.939, acc 0.0714286\n",
      "step 391, loss 188.668, acc 0.09375\n",
      "step 392, loss 284.182, acc 0.078125\n",
      "step 393, loss 269.773, acc 0.078125\n",
      "step 394, loss 309.218, acc 0.078125\n",
      "step 395, loss 225.385, acc 0.078125\n",
      "step 396, loss 242.935, acc 0.15625\n",
      "step 397, loss 282.753, acc 0.15625\n",
      "step 398, loss 286.192, acc 0.125\n",
      "step 399, loss 220.373, acc 0.078125\n",
      "step 400, loss 230.07, acc 0.109375\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 400, loss 446.496, acc 0.112245, precison 0.0125989171179, recall 0.112244897959, f1_score 0.022654933533.\n",
      "\n",
      "step 401, loss 268.983, acc 0.09375\n",
      "step 402, loss 221.913, acc 0.0625\n",
      "step 403, loss 454.08, acc 0\n",
      "step 404, loss 229.298, acc 0.171875\n",
      "step 405, loss 234.274, acc 0.171875\n",
      "step 406, loss 237.055, acc 0.1875\n",
      "step 407, loss 230.444, acc 0.125\n",
      "step 408, loss 227.068, acc 0.1875\n",
      "step 409, loss 289.893, acc 0.09375\n",
      "step 410, loss 382.791, acc 0.125\n",
      "step 411, loss 356.028, acc 0.015625\n",
      "step 412, loss 318.519, acc 0.109375\n",
      "step 413, loss 429.441, acc 0.046875\n",
      "step 414, loss 272.927, acc 0.09375\n",
      "step 415, loss 271.394, acc 0.078125\n",
      "step 416, loss 256.56, acc 0.0714286\n",
      "step 417, loss 277.856, acc 0.125\n",
      "step 418, loss 210.164, acc 0.140625\n",
      "step 419, loss 274.427, acc 0.078125\n",
      "step 420, loss 300.206, acc 0.140625\n",
      "step 421, loss 355.62, acc 0.0625\n",
      "step 422, loss 355.665, acc 0.109375\n",
      "step 423, loss 342.051, acc 0.09375\n",
      "step 424, loss 388.813, acc 0.03125\n",
      "step 425, loss 376.792, acc 0.046875\n",
      "step 426, loss 332.685, acc 0.0625\n",
      "step 427, loss 335.088, acc 0.03125\n",
      "step 428, loss 330.572, acc 0.0625\n",
      "step 429, loss 555.04, acc 0.0714286\n",
      "step 430, loss 430.955, acc 0.109375\n",
      "step 431, loss 356.444, acc 0.078125\n",
      "step 432, loss 336.105, acc 0.046875\n",
      "step 433, loss 410.716, acc 0.21875\n",
      "step 434, loss 426.215, acc 0.109375\n",
      "step 435, loss 299.947, acc 0.15625\n",
      "step 436, loss 416.921, acc 0.0625\n",
      "step 437, loss 353.08, acc 0.0625\n",
      "step 438, loss 325.479, acc 0.09375\n",
      "step 439, loss 321.471, acc 0.078125\n",
      "step 440, loss 394.231, acc 0.0625\n",
      "step 441, loss 428.465, acc 0.0625\n",
      "step 442, loss 191.434, acc 0.142857\n",
      "step 443, loss 381.24, acc 0.078125\n",
      "step 444, loss 439.636, acc 0.09375\n",
      "step 445, loss 370.931, acc 0.078125\n",
      "step 446, loss 350.628, acc 0.078125\n",
      "step 447, loss 572.709, acc 0.09375\n",
      "step 448, loss 401.682, acc 0.140625\n",
      "step 449, loss 386.524, acc 0.125\n",
      "step 450, loss 370.456, acc 0.078125\n",
      "step 451, loss 601.246, acc 0.078125\n",
      "step 452, loss 410.643, acc 0.046875\n",
      "step 453, loss 378.233, acc 0.0625\n",
      "step 454, loss 324.705, acc 0.125\n",
      "step 455, loss 242.98, acc 0.0714286\n",
      "step 456, loss 489.835, acc 0.125\n",
      "step 457, loss 479.017, acc 0.078125\n",
      "step 458, loss 510.696, acc 0.03125\n",
      "step 459, loss 396.779, acc 0.0625\n",
      "step 460, loss 402.195, acc 0.0625\n",
      "step 461, loss 629.125, acc 0.09375\n",
      "step 462, loss 554.166, acc 0.109375\n",
      "step 463, loss 423.052, acc 0.15625\n",
      "step 464, loss 485.976, acc 0.171875\n",
      "step 465, loss 399.075, acc 0.140625\n",
      "step 466, loss 602.521, acc 0.125\n",
      "step 467, loss 501.022, acc 0.09375\n",
      "step 468, loss 572.361, acc 0\n",
      "step 469, loss 681.253, acc 0.078125\n",
      "step 470, loss 454.675, acc 0.09375\n",
      "step 471, loss 513.014, acc 0.125\n",
      "step 472, loss 434.035, acc 0.171875\n",
      "step 473, loss 424.142, acc 0.171875\n",
      "step 474, loss 550.723, acc 0.09375\n",
      "step 475, loss 399.983, acc 0.109375\n",
      "step 476, loss 392.289, acc 0.21875\n",
      "step 477, loss 660.356, acc 0.078125\n",
      "step 478, loss 711.407, acc 0.09375\n",
      "step 479, loss 546.457, acc 0.03125\n",
      "step 480, loss 441.839, acc 0.109375\n",
      "step 481, loss 653.033, acc 0\n",
      "step 482, loss 492.034, acc 0.09375\n",
      "step 483, loss 508.359, acc 0.078125\n",
      "step 484, loss 648.846, acc 0.078125\n",
      "step 485, loss 515.582, acc 0.0625\n",
      "step 486, loss 464.794, acc 0.0625\n",
      "step 487, loss 556.907, acc 0.09375\n",
      "step 488, loss 704.018, acc 0.109375\n",
      "step 489, loss 717.405, acc 0.09375\n",
      "step 490, loss 455.696, acc 0.171875\n",
      "step 491, loss 617.764, acc 0.109375\n",
      "step 492, loss 695.223, acc 0.078125\n",
      "step 493, loss 725.669, acc 0.046875\n",
      "step 494, loss 449.148, acc 0.142857\n",
      "step 495, loss 568.097, acc 0.046875\n",
      "step 496, loss 550.59, acc 0.140625\n",
      "step 497, loss 550.36, acc 0.078125\n",
      "step 498, loss 670.349, acc 0.140625\n",
      "step 499, loss 672.381, acc 0.09375\n",
      "step 500, loss 624.696, acc 0.078125\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 500, loss 1299.4, acc 0.163265, precison 0.0280612244898, recall 0.163265306122, f1_score 0.0478911564626.\n",
      "\n",
      "step 501, loss 540.411, acc 0.109375\n",
      "step 502, loss 732.404, acc 0.125\n",
      "step 503, loss 702.097, acc 0.0625\n",
      "step 504, loss 610.19, acc 0.078125\n",
      "step 505, loss 746.254, acc 0.078125\n",
      "step 506, loss 666.519, acc 0.09375\n",
      "step 507, loss 421.236, acc 0.142857\n",
      "step 508, loss 404.238, acc 0.15625\n",
      "step 509, loss 758.005, acc 0.109375\n",
      "step 510, loss 711.049, acc 0.125\n",
      "step 511, loss 641.609, acc 0.046875\n",
      "step 512, loss 661.247, acc 0.09375\n",
      "step 513, loss 603.663, acc 0.078125\n",
      "step 514, loss 779.468, acc 0.09375\n",
      "step 515, loss 689.667, acc 0.09375\n",
      "step 516, loss 946.52, acc 0.140625\n",
      "step 517, loss 659.442, acc 0.09375\n",
      "step 518, loss 895.062, acc 0.109375\n",
      "step 519, loss 727.667, acc 0.140625\n",
      "step 520, loss 1020.54, acc 0.142857\n",
      "step 521, loss 936.536, acc 0.140625\n",
      "step 522, loss 857.667, acc 0.046875\n",
      "step 523, loss 894.272, acc 0.125\n",
      "step 524, loss 763.849, acc 0.078125\n",
      "step 525, loss 823.221, acc 0.0625\n",
      "step 526, loss 588.405, acc 0.0625\n",
      "step 527, loss 1092.01, acc 0.109375\n",
      "step 528, loss 817.025, acc 0.09375\n",
      "step 529, loss 694.099, acc 0.203125\n",
      "step 530, loss 659.483, acc 0.140625\n",
      "step 531, loss 627.64, acc 0.234375\n",
      "step 532, loss 811.694, acc 0.171875\n",
      "step 533, loss 1322.21, acc 0.142857\n",
      "step 534, loss 595.527, acc 0.078125\n",
      "step 535, loss 851.55, acc 0.15625\n",
      "step 536, loss 767.786, acc 0.15625\n",
      "step 537, loss 711.951, acc 0.109375\n",
      "step 538, loss 812.11, acc 0.125\n",
      "step 539, loss 746.017, acc 0.09375\n",
      "step 540, loss 664.836, acc 0.140625\n",
      "step 541, loss 931.311, acc 0.109375\n",
      "step 542, loss 925.004, acc 0.0625\n",
      "step 543, loss 1196.78, acc 0.0625\n",
      "step 544, loss 866.128, acc 0.015625\n",
      "step 545, loss 609.142, acc 0.015625\n",
      "step 546, loss 805.942, acc 0.142857\n",
      "step 547, loss 652.265, acc 0.109375\n",
      "step 548, loss 901.485, acc 0.078125\n",
      "step 549, loss 663.676, acc 0.109375\n",
      "step 550, loss 885.409, acc 0.046875\n",
      "step 551, loss 695.823, acc 0.078125\n",
      "step 552, loss 819.297, acc 0.0625\n",
      "step 553, loss 854.5, acc 0.0625\n",
      "step 554, loss 965.69, acc 0.09375\n",
      "step 555, loss 1007.21, acc 0.078125\n",
      "step 556, loss 1043.49, acc 0.0625\n",
      "step 557, loss 1090.07, acc 0.109375\n",
      "step 558, loss 861.582, acc 0.0625\n",
      "step 559, loss 1233.36, acc 0.142857\n",
      "step 560, loss 1107.96, acc 0.09375\n",
      "step 561, loss 676.561, acc 0.109375\n",
      "step 562, loss 696.251, acc 0.109375\n",
      "step 563, loss 949.51, acc 0.09375\n",
      "step 564, loss 783.492, acc 0.078125\n",
      "step 565, loss 728.529, acc 0.046875\n",
      "step 566, loss 840.987, acc 0.0625\n",
      "step 567, loss 828.586, acc 0.140625\n",
      "step 568, loss 1042.96, acc 0.09375\n",
      "step 569, loss 903.292, acc 0.125\n",
      "step 570, loss 846.313, acc 0.046875\n",
      "step 571, loss 1136.94, acc 0.09375\n",
      "step 572, loss 506.737, acc 0.142857\n",
      "step 573, loss 962.799, acc 0.125\n",
      "step 574, loss 824.52, acc 0.15625\n",
      "step 575, loss 1250.5, acc 0.171875\n",
      "step 576, loss 667.036, acc 0.125\n",
      "step 577, loss 1408.84, acc 0.078125\n",
      "step 578, loss 887.48, acc 0.125\n",
      "step 579, loss 944.479, acc 0\n",
      "step 580, loss 837.317, acc 0.03125\n",
      "step 581, loss 1082.72, acc 0.125\n",
      "step 582, loss 980.167, acc 0.109375\n",
      "step 583, loss 1077.79, acc 0.109375\n",
      "step 584, loss 1294.26, acc 0.078125\n",
      "step 585, loss 1236.63, acc 0.0714286\n",
      "step 586, loss 996.75, acc 0.0625\n",
      "step 587, loss 1236.84, acc 0.078125\n",
      "step 588, loss 979.058, acc 0.109375\n",
      "step 589, loss 1053.47, acc 0.15625\n",
      "step 590, loss 1160.06, acc 0.078125\n",
      "step 591, loss 965.185, acc 0.09375\n",
      "step 592, loss 788.214, acc 0.109375\n",
      "step 593, loss 1061.83, acc 0.125\n",
      "step 594, loss 1066.41, acc 0.0625\n",
      "step 595, loss 1221.6, acc 0.15625\n",
      "step 596, loss 961.337, acc 0.109375\n",
      "step 597, loss 884.72, acc 0\n",
      "step 598, loss 780.565, acc 0.0714286\n",
      "step 599, loss 786.716, acc 0.078125\n",
      "step 600, loss 995.402, acc 0.140625\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 600, loss 3046.28, acc 0.168367, precison 0.0283475635152, recall 0.168367346939, f1_score 0.0485250868907.\n",
      "\n",
      "step 601, loss 1061.11, acc 0.140625\n",
      "step 602, loss 1040.71, acc 0.109375\n",
      "step 603, loss 905.309, acc 0.078125\n",
      "step 604, loss 1054.52, acc 0.15625\n",
      "step 605, loss 1094.39, acc 0.09375\n",
      "step 606, loss 1437.5, acc 0.078125\n",
      "step 607, loss 1150.34, acc 0.0625\n",
      "step 608, loss 1114.9, acc 0.125\n",
      "step 609, loss 1321.93, acc 0.078125\n",
      "step 610, loss 1149.1, acc 0.15625\n",
      "step 611, loss 1631.24, acc 0\n",
      "step 612, loss 1248.42, acc 0.09375\n",
      "step 613, loss 1236.73, acc 0.140625\n",
      "step 614, loss 1045.8, acc 0.15625\n",
      "step 615, loss 1291.99, acc 0.171875\n",
      "step 616, loss 1300.1, acc 0.09375\n",
      "step 617, loss 1216.87, acc 0.09375\n",
      "step 618, loss 1265.48, acc 0.078125\n",
      "step 619, loss 1103.54, acc 0.109375\n",
      "step 620, loss 1375.15, acc 0.109375\n",
      "step 621, loss 1664.53, acc 0.15625\n",
      "step 622, loss 1151.52, acc 0.0625\n",
      "step 623, loss 1235.28, acc 0.046875\n",
      "step 624, loss 2076.99, acc 0\n",
      "step 625, loss 1377.88, acc 0.15625\n",
      "step 626, loss 1099.43, acc 0.109375\n",
      "step 627, loss 1060.5, acc 0.078125\n",
      "step 628, loss 848.886, acc 0.09375\n",
      "step 629, loss 771.042, acc 0.109375\n",
      "step 630, loss 1405.09, acc 0.046875\n",
      "step 631, loss 1393.5, acc 0.125\n",
      "step 632, loss 1821.13, acc 0.046875\n",
      "step 633, loss 1401.98, acc 0.109375\n",
      "step 634, loss 1245.95, acc 0.046875\n",
      "step 635, loss 1529.06, acc 0.0625\n",
      "step 636, loss 1152.11, acc 0.0625\n",
      "step 637, loss 1550.68, acc 0.0714286\n",
      "step 638, loss 1365.61, acc 0.125\n",
      "step 639, loss 1176.93, acc 0.1875\n",
      "step 640, loss 1680.16, acc 0.09375\n",
      "step 641, loss 1779.67, acc 0.0625\n",
      "step 642, loss 1471.08, acc 0.125\n",
      "step 643, loss 1465.82, acc 0.09375\n",
      "step 644, loss 1321.94, acc 0.109375\n",
      "step 645, loss 1328.42, acc 0.09375\n",
      "step 646, loss 1148.55, acc 0.15625\n",
      "step 647, loss 1008.2, acc 0.140625\n",
      "step 648, loss 1399.15, acc 0.09375\n",
      "step 649, loss 1353.72, acc 0.046875\n",
      "step 650, loss 1063.76, acc 0.142857\n",
      "step 651, loss 1736.95, acc 0.0625\n",
      "step 652, loss 1217.59, acc 0.0625\n",
      "step 653, loss 1324.19, acc 0.09375\n",
      "step 654, loss 1098.29, acc 0.15625\n",
      "step 655, loss 1532.92, acc 0.125\n",
      "step 656, loss 1302.75, acc 0.0625\n",
      "step 657, loss 1404.57, acc 0.09375\n",
      "step 658, loss 1437.32, acc 0.09375\n",
      "step 659, loss 1285.66, acc 0.09375\n",
      "step 660, loss 1112.03, acc 0.09375\n",
      "step 661, loss 1486.67, acc 0.140625\n",
      "step 662, loss 1162.13, acc 0.0625\n",
      "step 663, loss 2332.53, acc 0.0714286\n",
      "step 664, loss 1380.6, acc 0.109375\n",
      "step 665, loss 1537.73, acc 0.0625\n",
      "step 666, loss 1506.29, acc 0.109375\n",
      "step 667, loss 1069.92, acc 0.140625\n",
      "step 668, loss 1639.26, acc 0.0625\n",
      "step 669, loss 1512.81, acc 0.140625\n",
      "step 670, loss 1870.09, acc 0.0625\n",
      "step 671, loss 1396.85, acc 0.140625\n",
      "step 672, loss 1498.33, acc 0.109375\n",
      "step 673, loss 1442.44, acc 0.125\n",
      "step 674, loss 1994.03, acc 0.09375\n",
      "step 675, loss 1374.36, acc 0.109375\n",
      "step 676, loss 1996.26, acc 0.0714286\n",
      "step 677, loss 1331.62, acc 0.0625\n",
      "step 678, loss 1337.74, acc 0.109375\n",
      "step 679, loss 1260.31, acc 0.046875\n",
      "step 680, loss 1050.81, acc 0.09375\n",
      "step 681, loss 1704.75, acc 0.09375\n",
      "step 682, loss 1621.01, acc 0.109375\n",
      "step 683, loss 2064.35, acc 0.078125\n",
      "step 684, loss 1904.18, acc 0.03125\n",
      "step 685, loss 2368.82, acc 0.09375\n",
      "step 686, loss 1265.81, acc 0.171875\n",
      "step 687, loss 1418.45, acc 0.09375\n",
      "step 688, loss 1834.4, acc 0.0625\n",
      "step 689, loss 3863.53, acc 0\n",
      "step 690, loss 1918.3, acc 0.171875\n",
      "step 691, loss 1677.55, acc 0.03125\n",
      "step 692, loss 1397.24, acc 0.125\n",
      "step 693, loss 1678.47, acc 0.078125\n",
      "step 694, loss 2081.87, acc 0.15625\n",
      "step 695, loss 2594.86, acc 0.140625\n",
      "step 696, loss 2061.79, acc 0.09375\n",
      "step 697, loss 1994.7, acc 0.046875\n",
      "step 698, loss 1782.75, acc 0.109375\n",
      "step 699, loss 2079.35, acc 0.0625\n",
      "step 700, loss 1842.84, acc 0.03125\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 700, loss 5274.94, acc 0.168367, precison 0.0283475635152, recall 0.168367346939, f1_score 0.0485250868907.\n",
      "\n",
      "step 701, loss 1769.4, acc 0.078125\n",
      "step 702, loss 1275.36, acc 0\n",
      "step 703, loss 1589.54, acc 0.09375\n",
      "step 704, loss 1589.13, acc 0.125\n",
      "step 705, loss 1521.69, acc 0.0625\n",
      "step 706, loss 2071.84, acc 0.0625\n",
      "step 707, loss 1493.54, acc 0.09375\n",
      "step 708, loss 1353.25, acc 0.140625\n",
      "step 709, loss 1283.4, acc 0.078125\n",
      "step 710, loss 1674.37, acc 0.09375\n",
      "step 711, loss 1750.13, acc 0.09375\n",
      "step 712, loss 1729.4, acc 0.109375\n",
      "step 713, loss 2337.57, acc 0.0625\n",
      "step 714, loss 1479.6, acc 0.109375\n",
      "step 715, loss 1003.19, acc 0.142857\n",
      "step 716, loss 1517.18, acc 0.078125\n",
      "step 717, loss 1626.05, acc 0.015625\n",
      "step 718, loss 2278.35, acc 0.03125\n",
      "step 719, loss 1731.6, acc 0.09375\n",
      "step 720, loss 1293.49, acc 0.140625\n",
      "step 721, loss 1813.63, acc 0.140625\n",
      "step 722, loss 2303.27, acc 0.046875\n",
      "step 723, loss 1913.34, acc 0.09375\n",
      "step 724, loss 1167.71, acc 0.078125\n",
      "step 725, loss 2093.56, acc 0.03125\n",
      "step 726, loss 2232.74, acc 0.0625\n",
      "step 727, loss 1513.43, acc 0.109375\n",
      "step 728, loss 5042.75, acc 0\n",
      "step 729, loss 1933.09, acc 0.125\n",
      "step 730, loss 1755.69, acc 0.078125\n",
      "step 731, loss 1984.17, acc 0.0625\n",
      "step 732, loss 1818.92, acc 0.046875\n",
      "step 733, loss 1616.99, acc 0.0625\n",
      "step 734, loss 2317.88, acc 0.03125\n",
      "step 735, loss 1896.17, acc 0.09375\n",
      "step 736, loss 2252.7, acc 0.078125\n",
      "step 737, loss 2410.97, acc 0.046875\n",
      "step 738, loss 1819.08, acc 0.046875\n",
      "step 739, loss 1818.2, acc 0.125\n",
      "step 740, loss 1809.58, acc 0.109375\n",
      "step 741, loss 1309.9, acc 0.142857\n",
      "step 742, loss 2549.75, acc 0.078125\n",
      "step 743, loss 1639.9, acc 0.046875\n",
      "step 744, loss 1797.37, acc 0.125\n",
      "step 745, loss 1822.78, acc 0.1875\n",
      "step 746, loss 2088.24, acc 0.109375\n",
      "step 747, loss 1852.58, acc 0.03125\n",
      "step 748, loss 1330.19, acc 0.125\n",
      "step 749, loss 1715.42, acc 0.03125\n",
      "step 750, loss 1625.8, acc 0.21875\n",
      "step 751, loss 2005.34, acc 0.109375\n",
      "step 752, loss 1675.71, acc 0.140625\n",
      "step 753, loss 1406.53, acc 0.0625\n",
      "step 754, loss 1676.39, acc 0.0714286\n",
      "step 755, loss 1699.82, acc 0.078125\n",
      "step 756, loss 1955.58, acc 0.15625\n",
      "step 757, loss 1645.16, acc 0.171875\n",
      "step 758, loss 2636.15, acc 0.09375\n",
      "step 759, loss 2593.45, acc 0.046875\n",
      "step 760, loss 1644.38, acc 0.109375\n",
      "step 761, loss 1770.47, acc 0.078125\n",
      "step 762, loss 2476.47, acc 0.15625\n",
      "step 763, loss 1906.5, acc 0.078125\n",
      "step 764, loss 1568.24, acc 0.171875\n",
      "step 765, loss 1670.98, acc 0.046875\n",
      "step 766, loss 2279.25, acc 0.109375\n",
      "step 767, loss 2212.47, acc 0.142857\n",
      "step 768, loss 1221.65, acc 0.0625\n",
      "step 769, loss 1631.49, acc 0.125\n",
      "step 770, loss 1824.06, acc 0.171875\n",
      "step 771, loss 2541.42, acc 0.125\n",
      "step 772, loss 2200.96, acc 0.09375\n",
      "step 773, loss 1649.34, acc 0.0625\n",
      "step 774, loss 2040.33, acc 0.09375\n",
      "step 775, loss 1971.82, acc 0.078125\n",
      "step 776, loss 2186.64, acc 0.0625\n",
      "step 777, loss 1723.22, acc 0.109375\n",
      "step 778, loss 1857.7, acc 0.0625\n",
      "step 779, loss 2256.43, acc 0.078125\n",
      "step 780, loss 1620.71, acc 0.142857\n",
      "step 781, loss 2531.88, acc 0.03125\n",
      "step 782, loss 2336.6, acc 0.09375\n",
      "step 783, loss 2200.46, acc 0.125\n",
      "step 784, loss 1772.59, acc 0.125\n",
      "step 785, loss 2235.26, acc 0.125\n",
      "step 786, loss 2293.03, acc 0.203125\n",
      "step 787, loss 1463.43, acc 0.109375\n",
      "step 788, loss 1584.8, acc 0.125\n",
      "step 789, loss 2440.56, acc 0.078125\n",
      "step 790, loss 2042.13, acc 0.0625\n",
      "step 791, loss 1960.81, acc 0.078125\n",
      "step 792, loss 3625.3, acc 0.09375\n",
      "step 793, loss 3016.4, acc 0\n",
      "step 794, loss 2984.57, acc 0.078125\n",
      "step 795, loss 2067.02, acc 0.078125\n",
      "step 796, loss 2283.47, acc 0.0625\n",
      "step 797, loss 2009.37, acc 0.078125\n",
      "step 798, loss 2218.18, acc 0.234375\n",
      "step 799, loss 1906.94, acc 0.125\n",
      "step 800, loss 2640.21, acc 0.0625\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 800, loss 8644.8, acc 0.112245, precison 0.0132391418106, recall 0.112244897959, f1_score 0.0236847032391.\n",
      "\n",
      "step 801, loss 2445.61, acc 0.171875\n",
      "step 802, loss 2259.14, acc 0.078125\n",
      "step 803, loss 2122.53, acc 0.03125\n",
      "step 804, loss 2414.2, acc 0.125\n",
      "step 805, loss 1921.98, acc 0.046875\n",
      "step 806, loss 2312.36, acc 0\n",
      "step 807, loss 2852.12, acc 0.078125\n",
      "step 808, loss 1671.51, acc 0.125\n",
      "step 809, loss 2421.71, acc 0.078125\n",
      "step 810, loss 2516.28, acc 0.1875\n",
      "step 811, loss 1550.42, acc 0.0625\n",
      "step 812, loss 2071.87, acc 0.0625\n",
      "step 813, loss 2627.62, acc 0.09375\n",
      "step 814, loss 2099.51, acc 0.125\n",
      "step 815, loss 2017.87, acc 0.046875\n",
      "step 816, loss 1997.64, acc 0.046875\n",
      "step 817, loss 1900.53, acc 0.15625\n",
      "step 818, loss 2190.13, acc 0.140625\n",
      "step 819, loss 2073.89, acc 0.142857\n",
      "step 820, loss 2397.86, acc 0.0625\n",
      "step 821, loss 2633.22, acc 0.09375\n",
      "step 822, loss 2064.54, acc 0.109375\n",
      "step 823, loss 2020.35, acc 0.09375\n",
      "step 824, loss 2612.87, acc 0.015625\n",
      "step 825, loss 1439.34, acc 0.078125\n",
      "step 826, loss 2291.2, acc 0.09375\n",
      "step 827, loss 1417.72, acc 0.078125\n",
      "step 828, loss 2735.55, acc 0.046875\n",
      "step 829, loss 2974.09, acc 0.140625\n",
      "step 830, loss 2354.01, acc 0.1875\n",
      "step 831, loss 1932.41, acc 0.125\n",
      "step 832, loss 2990.05, acc 0.142857\n",
      "step 833, loss 1676.83, acc 0.078125\n",
      "step 834, loss 2428.19, acc 0.0625\n",
      "step 835, loss 2519.75, acc 0.140625\n",
      "step 836, loss 2145.96, acc 0.09375\n",
      "step 837, loss 2591.8, acc 0.078125\n",
      "step 838, loss 1970.33, acc 0.078125\n",
      "step 839, loss 2378.11, acc 0.125\n",
      "step 840, loss 2370.31, acc 0.046875\n",
      "step 841, loss 2881.08, acc 0.046875\n",
      "step 842, loss 2136.23, acc 0.109375\n",
      "step 843, loss 2654.3, acc 0.125\n",
      "step 844, loss 2174.27, acc 0.046875\n",
      "step 845, loss 4222.33, acc 0.0714286\n",
      "step 846, loss 2769.63, acc 0.046875\n",
      "step 847, loss 2205.29, acc 0.171875\n",
      "step 848, loss 1823.44, acc 0.078125\n",
      "step 849, loss 2541.37, acc 0.125\n",
      "step 850, loss 2557.07, acc 0.078125\n",
      "step 851, loss 2951.69, acc 0.09375\n",
      "step 852, loss 2208.66, acc 0.09375\n",
      "step 853, loss 2327.14, acc 0.03125\n",
      "step 854, loss 2791.06, acc 0.09375\n",
      "step 855, loss 2389.2, acc 0.140625\n",
      "step 856, loss 2155.06, acc 0.078125\n",
      "step 857, loss 1737.08, acc 0.078125\n",
      "step 858, loss 2480.62, acc 0.0714286\n",
      "step 859, loss 3451.11, acc 0.078125\n",
      "step 860, loss 1801.47, acc 0.078125\n",
      "step 861, loss 1682.82, acc 0.0625\n",
      "step 862, loss 2220.64, acc 0.046875\n",
      "step 863, loss 2498.73, acc 0.09375\n",
      "step 864, loss 2231.36, acc 0.078125\n",
      "step 865, loss 2535.88, acc 0.109375\n",
      "step 866, loss 1860.35, acc 0.109375\n",
      "step 867, loss 2858.34, acc 0.046875\n",
      "step 868, loss 1965.79, acc 0.078125\n",
      "step 869, loss 2017.48, acc 0.0625\n",
      "step 870, loss 3162.72, acc 0.09375\n",
      "step 871, loss 1991.54, acc 0.0714286\n",
      "step 872, loss 2673.52, acc 0.09375\n",
      "step 873, loss 2480.99, acc 0.078125\n",
      "step 874, loss 2902.69, acc 0.15625\n",
      "step 875, loss 2687.68, acc 0.109375\n",
      "step 876, loss 3790.44, acc 0.0625\n",
      "step 877, loss 2678.33, acc 0.046875\n",
      "step 878, loss 3013.51, acc 0.0625\n",
      "step 879, loss 2140.9, acc 0.125\n",
      "step 880, loss 3642.41, acc 0.125\n",
      "step 881, loss 2998.57, acc 0.03125\n",
      "step 882, loss 1984.52, acc 0.09375\n",
      "step 883, loss 3336.12, acc 0.09375\n",
      "step 884, loss 2601.55, acc 0.0714286\n",
      "step 885, loss 3007.23, acc 0.0625\n",
      "step 886, loss 1854.32, acc 0.140625\n",
      "step 887, loss 3340.78, acc 0.078125\n",
      "step 888, loss 2573.02, acc 0.25\n",
      "step 889, loss 3209.14, acc 0.0625\n",
      "step 890, loss 2705.9, acc 0.078125\n",
      "step 891, loss 2619.36, acc 0.0625\n",
      "step 892, loss 2857.55, acc 0.09375\n",
      "step 893, loss 2280.46, acc 0.09375\n",
      "step 894, loss 5054.13, acc 0.109375\n",
      "step 895, loss 4036.55, acc 0.140625\n",
      "step 896, loss 3534.67, acc 0.09375\n",
      "step 897, loss 3202.11, acc 0.0714286\n",
      "step 898, loss 2417.07, acc 0.109375\n",
      "step 899, loss 2498.59, acc 0.109375\n",
      "step 900, loss 3268.29, acc 0.125\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 900, loss 12237.7, acc 0.0969388, precison 0.0177750077304, recall 0.0969387755102, f1_score 0.0300206048066.\n",
      "\n",
      "step 901, loss 2733.57, acc 0.0625\n",
      "step 902, loss 2460.71, acc 0.0625\n",
      "step 903, loss 2062.39, acc 0.078125\n",
      "step 904, loss 3506.37, acc 0.15625\n",
      "step 905, loss 3386.07, acc 0.109375\n",
      "step 906, loss 3329.86, acc 0.109375\n",
      "step 907, loss 3174.69, acc 0.046875\n",
      "step 908, loss 2528.31, acc 0.078125\n",
      "step 909, loss 2978.13, acc 0.078125\n",
      "step 910, loss 2184.99, acc 0\n",
      "step 911, loss 3400.83, acc 0.0625\n",
      "step 912, loss 2046.43, acc 0.09375\n",
      "step 913, loss 3953.84, acc 0.0625\n",
      "step 914, loss 4270.64, acc 0.078125\n",
      "step 915, loss 3299.55, acc 0.109375\n",
      "step 916, loss 4152.53, acc 0.125\n",
      "step 917, loss 2321.96, acc 0.125\n",
      "step 918, loss 3133.11, acc 0.0625\n",
      "step 919, loss 3486.33, acc 0.109375\n",
      "step 920, loss 2504.11, acc 0.0625\n",
      "step 921, loss 2813.68, acc 0.078125\n",
      "step 922, loss 3988.69, acc 0.046875\n",
      "step 923, loss 1463.46, acc 0.285714\n",
      "step 924, loss 3115.58, acc 0.09375\n",
      "step 925, loss 3308.62, acc 0.125\n",
      "step 926, loss 2791.68, acc 0.0625\n",
      "step 927, loss 4283, acc 0.046875\n",
      "step 928, loss 2926.16, acc 0.109375\n",
      "step 929, loss 3193.25, acc 0.046875\n",
      "step 930, loss 3157.27, acc 0.125\n",
      "step 931, loss 2237.96, acc 0.09375\n",
      "step 932, loss 2278.87, acc 0.15625\n",
      "step 933, loss 2509.35, acc 0.0625\n",
      "step 934, loss 1900.84, acc 0.109375\n",
      "step 935, loss 3008.05, acc 0.09375\n",
      "step 936, loss 1486.46, acc 0.142857\n",
      "step 937, loss 3007.47, acc 0.125\n",
      "step 938, loss 2537.05, acc 0.109375\n",
      "step 939, loss 3293.07, acc 0.125\n",
      "step 940, loss 3344.48, acc 0.140625\n",
      "step 941, loss 4311.22, acc 0.0625\n",
      "step 942, loss 3179.93, acc 0.0625\n",
      "step 943, loss 3666.52, acc 0.09375\n",
      "step 944, loss 2313.5, acc 0.046875\n",
      "step 945, loss 2147.44, acc 0.078125\n",
      "step 946, loss 3702.06, acc 0.109375\n",
      "step 947, loss 3150.46, acc 0.171875\n",
      "step 948, loss 2891.71, acc 0.09375\n",
      "step 949, loss 5362.43, acc 0.142857\n",
      "step 950, loss 2771.58, acc 0.140625\n",
      "step 951, loss 3467.31, acc 0.09375\n",
      "step 952, loss 4121.61, acc 0.03125\n",
      "step 953, loss 3940.16, acc 0.0625\n",
      "step 954, loss 3719.67, acc 0.046875\n",
      "step 955, loss 3311.82, acc 0.078125\n",
      "step 956, loss 3088.44, acc 0.0625\n",
      "step 957, loss 2734.84, acc 0.125\n",
      "step 958, loss 2703.5, acc 0.09375\n",
      "step 959, loss 3531.97, acc 0.125\n",
      "step 960, loss 3636.78, acc 0.140625\n",
      "step 961, loss 3841.38, acc 0.125\n",
      "step 962, loss 2651.93, acc 0.0714286\n",
      "step 963, loss 2631.79, acc 0.140625\n",
      "step 964, loss 2207.22, acc 0.125\n",
      "step 965, loss 2092.27, acc 0.0625\n",
      "step 966, loss 3882.51, acc 0.046875\n",
      "step 967, loss 3181.77, acc 0.078125\n",
      "step 968, loss 3575.56, acc 0.0625\n",
      "step 969, loss 2763.51, acc 0.125\n",
      "step 970, loss 3370.93, acc 0.125\n",
      "step 971, loss 4450.66, acc 0.140625\n",
      "step 972, loss 2573.33, acc 0.1875\n",
      "step 973, loss 3638.39, acc 0.03125\n",
      "step 974, loss 3035.46, acc 0.0625\n",
      "step 975, loss 4493.6, acc 0\n",
      "step 976, loss 2932.95, acc 0.109375\n",
      "step 977, loss 3224.7, acc 0.078125\n",
      "step 978, loss 3105.22, acc 0.0625\n",
      "step 979, loss 3964.97, acc 0.03125\n",
      "step 980, loss 2421.18, acc 0.03125\n",
      "step 981, loss 3603.88, acc 0.03125\n",
      "step 982, loss 3659.86, acc 0.015625\n",
      "step 983, loss 2357.02, acc 0.125\n",
      "step 984, loss 4184.27, acc 0.171875\n",
      "step 985, loss 2419.62, acc 0.09375\n",
      "step 986, loss 3241.13, acc 0.109375\n",
      "step 987, loss 4459.12, acc 0.046875\n",
      "step 988, loss 2300.52, acc 0.0714286\n",
      "step 989, loss 3680.35, acc 0.09375\n",
      "step 990, loss 2605.84, acc 0.078125\n",
      "step 991, loss 2353.35, acc 0.234375\n",
      "step 992, loss 4275.92, acc 0.125\n",
      "step 993, loss 2674.79, acc 0.140625\n",
      "step 994, loss 3162.15, acc 0.078125\n",
      "step 995, loss 4526.29, acc 0.09375\n",
      "step 996, loss 3185.57, acc 0.078125\n",
      "step 997, loss 3679.43, acc 0.1875\n",
      "step 998, loss 3514.28, acc 0.078125\n",
      "step 999, loss 2900.77, acc 0.078125\n",
      "step 1000, loss 3668.66, acc 0.09375\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1000, loss 16938.1, acc 0.0816327, precison 0.0157397663273, recall 0.0816326530612, f1_score 0.0262595081179.\n",
      "\n",
      "step 1001, loss 2852.92, acc 0.214286\n",
      "step 1002, loss 3240.99, acc 0.09375\n",
      "step 1003, loss 4026.62, acc 0.15625\n",
      "step 1004, loss 2807.6, acc 0.046875\n",
      "step 1005, loss 3787.15, acc 0.125\n",
      "step 1006, loss 3140.59, acc 0.09375\n",
      "step 1007, loss 3540.43, acc 0.0625\n",
      "step 1008, loss 2880.26, acc 0.046875\n",
      "step 1009, loss 3821.04, acc 0.15625\n",
      "step 1010, loss 3331.94, acc 0.109375\n",
      "step 1011, loss 3366.54, acc 0.109375\n",
      "step 1012, loss 2663, acc 0.078125\n",
      "step 1013, loss 3236.65, acc 0.09375\n",
      "step 1014, loss 3383.88, acc 0\n",
      "step 1015, loss 3250.61, acc 0.109375\n",
      "step 1016, loss 3299, acc 0.0625\n",
      "step 1017, loss 3549.52, acc 0.046875\n",
      "step 1018, loss 3817.85, acc 0.03125\n",
      "step 1019, loss 2757.62, acc 0.09375\n",
      "step 1020, loss 4531.41, acc 0.109375\n",
      "step 1021, loss 4179.04, acc 0.171875\n",
      "step 1022, loss 4040.49, acc 0.140625\n",
      "step 1023, loss 3419.99, acc 0.140625\n",
      "step 1024, loss 3446.37, acc 0.125\n",
      "step 1025, loss 3938.77, acc 0.078125\n",
      "step 1026, loss 3587.57, acc 0.109375\n",
      "step 1027, loss 1507.4, acc 0.214286\n",
      "step 1028, loss 3754.38, acc 0.03125\n",
      "step 1029, loss 3456.17, acc 0.125\n",
      "step 1030, loss 3740.17, acc 0.078125\n",
      "step 1031, loss 3724.97, acc 0.046875\n",
      "step 1032, loss 4048.94, acc 0.0625\n",
      "step 1033, loss 2866.2, acc 0.125\n",
      "step 1034, loss 3428.86, acc 0.140625\n",
      "step 1035, loss 4227.82, acc 0.109375\n",
      "step 1036, loss 4546.67, acc 0.0625\n",
      "step 1037, loss 3987.15, acc 0.046875\n",
      "step 1038, loss 2155.24, acc 0.125\n",
      "step 1039, loss 3774.44, acc 0.09375\n",
      "step 1040, loss 2885.99, acc 0.214286\n",
      "step 1041, loss 3767.46, acc 0.09375\n",
      "step 1042, loss 2861.8, acc 0.125\n",
      "step 1043, loss 4771.46, acc 0.109375\n",
      "step 1044, loss 2534.43, acc 0.09375\n",
      "step 1045, loss 3769.74, acc 0.078125\n",
      "step 1046, loss 3061.88, acc 0.125\n",
      "step 1047, loss 3858.45, acc 0.140625\n",
      "step 1048, loss 4372.44, acc 0.09375\n",
      "step 1049, loss 3593.23, acc 0.109375\n",
      "step 1050, loss 4177.82, acc 0.0625\n",
      "step 1051, loss 2874.23, acc 0.046875\n",
      "step 1052, loss 3973.49, acc 0.140625\n",
      "step 1053, loss 5067.68, acc 0\n",
      "step 1054, loss 2953.93, acc 0.03125\n",
      "step 1055, loss 3857.62, acc 0.078125\n",
      "step 1056, loss 3489.31, acc 0.078125\n",
      "step 1057, loss 3511.25, acc 0.03125\n",
      "step 1058, loss 3969.65, acc 0.078125\n",
      "step 1059, loss 4038.63, acc 0.171875\n",
      "step 1060, loss 5184.59, acc 0.0625\n",
      "step 1061, loss 3480.03, acc 0.078125\n",
      "step 1062, loss 2635.27, acc 0.09375\n",
      "step 1063, loss 3953.99, acc 0.0625\n",
      "step 1064, loss 3469.82, acc 0.046875\n",
      "step 1065, loss 4561.21, acc 0.125\n",
      "step 1066, loss 3967.49, acc 0.0714286\n",
      "step 1067, loss 4084.67, acc 0.078125\n",
      "step 1068, loss 4801.41, acc 0.09375\n",
      "step 1069, loss 3775.25, acc 0.0625\n",
      "step 1070, loss 3778.75, acc 0.0625\n",
      "step 1071, loss 3915.43, acc 0.03125\n",
      "step 1072, loss 3862.03, acc 0.09375\n",
      "step 1073, loss 3310.5, acc 0.09375\n",
      "step 1074, loss 5503.06, acc 0.0625\n",
      "step 1075, loss 2083.42, acc 0.03125\n",
      "step 1076, loss 3275.16, acc 0.0625\n",
      "step 1077, loss 3360.46, acc 0.15625\n",
      "step 1078, loss 3535.53, acc 0.0625\n",
      "step 1079, loss 3778.04, acc 0\n",
      "step 1080, loss 3814.99, acc 0.046875\n",
      "step 1081, loss 3676.26, acc 0.109375\n",
      "step 1082, loss 3444.86, acc 0.078125\n",
      "step 1083, loss 3528.83, acc 0.0625\n",
      "step 1084, loss 4548.68, acc 0.03125\n",
      "step 1085, loss 3270.81, acc 0.125\n",
      "step 1086, loss 3403.69, acc 0.046875\n",
      "step 1087, loss 4475.71, acc 0.078125\n",
      "step 1088, loss 3511.27, acc 0.0625\n",
      "step 1089, loss 3126.86, acc 0.09375\n",
      "step 1090, loss 3640.67, acc 0.109375\n",
      "step 1091, loss 3271.81, acc 0.140625\n",
      "step 1092, loss 1987.31, acc 0.142857\n",
      "step 1093, loss 2349.37, acc 0.078125\n",
      "step 1094, loss 3276.96, acc 0.109375\n",
      "step 1095, loss 3694.11, acc 0.0625\n",
      "step 1096, loss 3946, acc 0.125\n",
      "step 1097, loss 3719.79, acc 0.109375\n",
      "step 1098, loss 2787.92, acc 0.09375\n",
      "step 1099, loss 3668.96, acc 0.078125\n",
      "step 1100, loss 4070.68, acc 0.09375\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1100, loss 22882.7, acc 0.112245, precison 0.0126635269492, recall 0.112244897959, f1_score 0.0227593341484.\n",
      "\n",
      "step 1101, loss 4346.21, acc 0.125\n",
      "step 1102, loss 3980.2, acc 0.171875\n",
      "step 1103, loss 4252.94, acc 0.078125\n",
      "step 1104, loss 4470.22, acc 0.03125\n",
      "step 1105, loss 7208.47, acc 0.142857\n",
      "step 1106, loss 4048.46, acc 0.078125\n",
      "step 1107, loss 4333.67, acc 0.109375\n",
      "step 1108, loss 4394.4, acc 0.078125\n",
      "step 1109, loss 3044.3, acc 0.09375\n",
      "step 1110, loss 3421.31, acc 0.125\n",
      "step 1111, loss 2674.5, acc 0.0625\n",
      "step 1112, loss 3936.15, acc 0.0625\n",
      "step 1113, loss 5228.22, acc 0.125\n",
      "step 1114, loss 4117.25, acc 0.0625\n",
      "step 1115, loss 4026.18, acc 0.046875\n",
      "step 1116, loss 3098.89, acc 0.109375\n",
      "step 1117, loss 2949.29, acc 0.15625\n",
      "step 1118, loss 6092.07, acc 0.142857\n",
      "step 1119, loss 4296.25, acc 0.140625\n",
      "step 1120, loss 3871.71, acc 0.046875\n",
      "step 1121, loss 3410.82, acc 0.140625\n",
      "step 1122, loss 3373.4, acc 0.078125\n",
      "step 1123, loss 4896.36, acc 0.046875\n",
      "step 1124, loss 2443.87, acc 0.109375\n",
      "step 1125, loss 5248.51, acc 0.046875\n",
      "step 1126, loss 3804.34, acc 0.09375\n",
      "step 1127, loss 3135.97, acc 0.0625\n",
      "step 1128, loss 3966.29, acc 0.0625\n",
      "step 1129, loss 3585.85, acc 0.09375\n",
      "step 1130, loss 6242.99, acc 0.03125\n",
      "step 1131, loss 2774.39, acc 0.0714286\n",
      "step 1132, loss 5150.08, acc 0.046875\n",
      "step 1133, loss 4955.36, acc 0.09375\n",
      "step 1134, loss 4195.95, acc 0.09375\n",
      "step 1135, loss 3636.41, acc 0.109375\n",
      "step 1136, loss 4062.26, acc 0.0625\n",
      "step 1137, loss 3693.57, acc 0.171875\n",
      "step 1138, loss 3014.89, acc 0.15625\n",
      "step 1139, loss 3715.3, acc 0.09375\n",
      "step 1140, loss 4286.02, acc 0.03125\n",
      "step 1141, loss 3060.73, acc 0.09375\n",
      "step 1142, loss 4026.03, acc 0.15625\n",
      "step 1143, loss 3293.72, acc 0.09375\n",
      "step 1144, loss 3126.79, acc 0.0714286\n",
      "step 1145, loss 4368.38, acc 0.109375\n",
      "step 1146, loss 3874.18, acc 0.21875\n",
      "step 1147, loss 3287.02, acc 0.15625\n",
      "step 1148, loss 3330.61, acc 0.078125\n",
      "step 1149, loss 4122.42, acc 0.125\n",
      "step 1150, loss 4215.73, acc 0.046875\n",
      "step 1151, loss 3994.47, acc 0.09375\n",
      "step 1152, loss 4099.93, acc 0.09375\n",
      "step 1153, loss 4700.63, acc 0.125\n",
      "step 1154, loss 3989.01, acc 0.125\n",
      "step 1155, loss 3623.7, acc 0.0625\n",
      "step 1156, loss 4246.46, acc 0.078125\n",
      "step 1157, loss 5674.49, acc 0\n",
      "step 1158, loss 3300.73, acc 0.09375\n",
      "step 1159, loss 3520.38, acc 0.140625\n",
      "step 1160, loss 5008.73, acc 0.0625\n",
      "step 1161, loss 5636.03, acc 0.0625\n",
      "step 1162, loss 3680.47, acc 0.078125\n",
      "step 1163, loss 4318.41, acc 0.078125\n",
      "step 1164, loss 3566.05, acc 0.125\n",
      "step 1165, loss 4480.35, acc 0.109375\n",
      "step 1166, loss 3781.71, acc 0.140625\n",
      "step 1167, loss 3234.58, acc 0.0625\n",
      "step 1168, loss 4608.59, acc 0.046875\n",
      "step 1169, loss 3657, acc 0.09375\n",
      "step 1170, loss 2674.51, acc 0.0714286\n",
      "step 1171, loss 3750.03, acc 0.09375\n",
      "step 1172, loss 5093.04, acc 0.09375\n",
      "step 1173, loss 5639.4, acc 0.0625\n",
      "step 1174, loss 4125.74, acc 0.09375\n",
      "step 1175, loss 3223.15, acc 0.078125\n",
      "step 1176, loss 3970.25, acc 0.046875\n",
      "step 1177, loss 3166.87, acc 0.078125\n",
      "step 1178, loss 3583.32, acc 0.09375\n",
      "step 1179, loss 3908.2, acc 0.140625\n",
      "step 1180, loss 4046.04, acc 0.109375\n",
      "step 1181, loss 4915.17, acc 0.078125\n",
      "step 1182, loss 3852.17, acc 0.0625\n",
      "step 1183, loss 3052.72, acc 0.214286\n",
      "step 1184, loss 3809.67, acc 0.078125\n",
      "step 1185, loss 5411.04, acc 0.0625\n",
      "step 1186, loss 3090.5, acc 0.0625\n",
      "step 1187, loss 3879.79, acc 0.046875\n",
      "step 1188, loss 4514.54, acc 0.03125\n",
      "step 1189, loss 4036.36, acc 0.171875\n",
      "step 1190, loss 4862.06, acc 0.109375\n",
      "step 1191, loss 4991.74, acc 0.140625\n",
      "step 1192, loss 4467.68, acc 0.15625\n",
      "step 1193, loss 3204.73, acc 0.0625\n",
      "step 1194, loss 4974.43, acc 0.171875\n",
      "step 1195, loss 3796.8, acc 0.09375\n",
      "step 1196, loss 6669.51, acc 0.0714286\n",
      "step 1197, loss 3168.46, acc 0.046875\n",
      "step 1198, loss 4222.2, acc 0.15625\n",
      "step 1199, loss 4808.57, acc 0.078125\n",
      "step 1200, loss 4257.88, acc 0.09375\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1200, loss 29670.3, acc 0.168367, precison 0.0443443334781, recall 0.168367346939, f1_score 0.0598679032052.\n",
      "\n",
      "step 1201, loss 4931.31, acc 0.109375\n",
      "step 1202, loss 5166.06, acc 0.078125\n",
      "step 1203, loss 5295.39, acc 0.078125\n",
      "step 1204, loss 4040.04, acc 0.0625\n",
      "step 1205, loss 4513.49, acc 0.125\n",
      "step 1206, loss 4179.36, acc 0.09375\n",
      "step 1207, loss 4966.5, acc 0.15625\n",
      "step 1208, loss 4043.95, acc 0.0625\n",
      "step 1209, loss 3921.01, acc 0.0714286\n",
      "step 1210, loss 5107.34, acc 0.09375\n",
      "step 1211, loss 5839.52, acc 0.125\n",
      "step 1212, loss 3672.46, acc 0.140625\n",
      "step 1213, loss 4269.98, acc 0.078125\n",
      "step 1214, loss 3699.74, acc 0.03125\n",
      "step 1215, loss 4534.87, acc 0.046875\n",
      "step 1216, loss 4341.9, acc 0.15625\n",
      "step 1217, loss 4374.17, acc 0.0625\n",
      "step 1218, loss 5427.1, acc 0.125\n",
      "step 1219, loss 3650.93, acc 0.09375\n",
      "step 1220, loss 4891.69, acc 0.0625\n",
      "step 1221, loss 4993.23, acc 0.09375\n",
      "step 1222, loss 3729.08, acc 0.214286\n",
      "step 1223, loss 5261.49, acc 0.125\n",
      "step 1224, loss 4461.85, acc 0.09375\n",
      "step 1225, loss 4207.79, acc 0.046875\n",
      "step 1226, loss 6193.5, acc 0.09375\n",
      "step 1227, loss 4884.69, acc 0.0625\n",
      "step 1228, loss 3527.68, acc 0.0625\n",
      "step 1229, loss 5225.82, acc 0.109375\n",
      "step 1230, loss 4815.01, acc 0.078125\n",
      "step 1231, loss 5366.38, acc 0.046875\n",
      "step 1232, loss 2728.67, acc 0.0625\n",
      "step 1233, loss 6659.73, acc 0.0625\n",
      "step 1234, loss 4093.92, acc 0.046875\n",
      "step 1235, loss 3303.34, acc 0\n",
      "step 1236, loss 6530.37, acc 0.1875\n",
      "step 1237, loss 4152.04, acc 0.09375\n",
      "step 1238, loss 5097.49, acc 0.0625\n",
      "step 1239, loss 4271.21, acc 0.109375\n",
      "step 1240, loss 6471.59, acc 0.109375\n",
      "step 1241, loss 4679.75, acc 0.078125\n",
      "step 1242, loss 4541.11, acc 0.078125\n",
      "step 1243, loss 5459, acc 0.046875\n",
      "step 1244, loss 6449.23, acc 0.046875\n",
      "step 1245, loss 3514.2, acc 0.15625\n",
      "step 1246, loss 3922.49, acc 0.09375\n",
      "step 1247, loss 6632.2, acc 0.125\n",
      "step 1248, loss 3956.14, acc 0.0714286\n",
      "step 1249, loss 4471.62, acc 0.078125\n",
      "step 1250, loss 6508.96, acc 0.0625\n",
      "step 1251, loss 4974.13, acc 0.078125\n",
      "step 1252, loss 3075.24, acc 0.078125\n",
      "step 1253, loss 5304.39, acc 0.03125\n",
      "step 1254, loss 5030.39, acc 0.109375\n",
      "step 1255, loss 4974.34, acc 0.140625\n",
      "step 1256, loss 4638.38, acc 0.109375\n",
      "step 1257, loss 4708.93, acc 0.078125\n",
      "step 1258, loss 6853.54, acc 0.03125\n",
      "step 1259, loss 3973.48, acc 0.171875\n",
      "step 1260, loss 6743.01, acc 0.09375\n",
      "step 1261, loss 2717.88, acc 0.0714286\n",
      "step 1262, loss 5517.98, acc 0.09375\n",
      "step 1263, loss 6360.63, acc 0.046875\n",
      "step 1264, loss 5634.54, acc 0.09375\n",
      "step 1265, loss 4513.42, acc 0.15625\n",
      "step 1266, loss 4385.17, acc 0.078125\n",
      "step 1267, loss 4706.7, acc 0.140625\n",
      "step 1268, loss 6247.93, acc 0.15625\n",
      "step 1269, loss 5431.53, acc 0.046875\n",
      "step 1270, loss 5069.61, acc 0.125\n",
      "step 1271, loss 3420.6, acc 0.140625\n",
      "step 1272, loss 4797.69, acc 0.171875\n",
      "step 1273, loss 5680.1, acc 0.03125\n",
      "step 1274, loss 6227.36, acc 0.142857\n",
      "step 1275, loss 6001.35, acc 0.125\n",
      "step 1276, loss 4539.16, acc 0.15625\n",
      "step 1277, loss 5627.97, acc 0.046875\n",
      "step 1278, loss 5487.4, acc 0.03125\n",
      "step 1279, loss 5564.47, acc 0.046875\n",
      "step 1280, loss 6500.45, acc 0.125\n",
      "step 1281, loss 5478.9, acc 0.046875\n",
      "step 1282, loss 4009.27, acc 0.109375\n",
      "step 1283, loss 5222.34, acc 0.09375\n",
      "step 1284, loss 6344.29, acc 0.09375\n",
      "step 1285, loss 5362.51, acc 0.109375\n",
      "step 1286, loss 6038.34, acc 0.109375\n",
      "step 1287, loss 8066.69, acc 0\n",
      "step 1288, loss 5356.82, acc 0.09375\n",
      "step 1289, loss 5556.91, acc 0.078125\n",
      "step 1290, loss 4856.75, acc 0.109375\n",
      "step 1291, loss 5351.68, acc 0.09375\n",
      "step 1292, loss 7118.9, acc 0.09375\n",
      "step 1293, loss 5754.1, acc 0.0625\n",
      "step 1294, loss 5623.2, acc 0.03125\n",
      "step 1295, loss 3856.45, acc 0.09375\n",
      "step 1296, loss 5804.44, acc 0.015625\n",
      "step 1297, loss 5964.84, acc 0.09375\n",
      "step 1298, loss 6349.44, acc 0.09375\n",
      "step 1299, loss 4240.52, acc 0.140625\n",
      "step 1300, loss 2261, acc 0.285714\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1300, loss 38244.7, acc 0.168367, precison 0.0283475635152, recall 0.168367346939, f1_score 0.0485250868907.\n",
      "\n",
      "step 1301, loss 6429.85, acc 0.0625\n",
      "step 1302, loss 5479.22, acc 0.140625\n",
      "step 1303, loss 5367.64, acc 0.078125\n",
      "step 1304, loss 3825.21, acc 0.09375\n",
      "step 1305, loss 3987.11, acc 0.0625\n",
      "step 1306, loss 5925.25, acc 0.03125\n",
      "step 1307, loss 7060.84, acc 0.03125\n",
      "step 1308, loss 5038.38, acc 0.078125\n",
      "step 1309, loss 5345.48, acc 0.109375\n",
      "step 1310, loss 6019.28, acc 0.078125\n",
      "step 1311, loss 4872.47, acc 0.125\n",
      "step 1312, loss 4293.67, acc 0.09375\n",
      "step 1313, loss 3662.12, acc 0.142857\n",
      "step 1314, loss 5421.31, acc 0.09375\n",
      "step 1315, loss 5582.98, acc 0.09375\n",
      "step 1316, loss 4459.83, acc 0.125\n",
      "step 1317, loss 5307.33, acc 0.109375\n",
      "step 1318, loss 4682.79, acc 0.09375\n",
      "step 1319, loss 6762.84, acc 0.125\n",
      "step 1320, loss 4816.34, acc 0.140625\n",
      "step 1321, loss 5347.46, acc 0.109375\n",
      "step 1322, loss 4791.68, acc 0.09375\n",
      "step 1323, loss 3904.29, acc 0.1875\n",
      "step 1324, loss 6397.09, acc 0.09375\n",
      "step 1325, loss 6995.74, acc 0.03125\n",
      "step 1326, loss 8112.69, acc 0\n",
      "step 1327, loss 6051.07, acc 0.03125\n",
      "step 1328, loss 5051.98, acc 0.0625\n",
      "step 1329, loss 5692.53, acc 0.078125\n",
      "step 1330, loss 4800.58, acc 0.1875\n",
      "step 1331, loss 6718.3, acc 0.109375\n",
      "step 1332, loss 6689.96, acc 0.0625\n",
      "step 1333, loss 4655.94, acc 0.109375\n",
      "step 1334, loss 6783.23, acc 0.109375\n",
      "step 1335, loss 4086.57, acc 0.03125\n",
      "step 1336, loss 5703.89, acc 0.03125\n",
      "step 1337, loss 6170.59, acc 0.125\n",
      "step 1338, loss 4216.47, acc 0.0625\n",
      "step 1339, loss 8900.1, acc 0.142857\n",
      "step 1340, loss 7175.19, acc 0.046875\n",
      "step 1341, loss 5784.55, acc 0.046875\n",
      "step 1342, loss 4769.07, acc 0.109375\n",
      "step 1343, loss 4776.54, acc 0.09375\n",
      "step 1344, loss 5851.65, acc 0.125\n",
      "step 1345, loss 5809.76, acc 0.09375\n",
      "step 1346, loss 6028.29, acc 0.09375\n",
      "step 1347, loss 5233.55, acc 0.140625\n",
      "step 1348, loss 5516.44, acc 0.09375\n",
      "step 1349, loss 4749.89, acc 0.109375\n",
      "step 1350, loss 5903.34, acc 0.09375\n",
      "step 1351, loss 5327.18, acc 0.03125\n",
      "step 1352, loss 3807.69, acc 0.0714286\n",
      "step 1353, loss 5501.43, acc 0.09375\n",
      "step 1354, loss 4257.15, acc 0.171875\n",
      "step 1355, loss 5275.13, acc 0.109375\n",
      "step 1356, loss 6276.91, acc 0.046875\n",
      "step 1357, loss 5967.96, acc 0.03125\n",
      "step 1358, loss 6171.49, acc 0.078125\n",
      "step 1359, loss 5623.32, acc 0.078125\n",
      "step 1360, loss 6666.64, acc 0.015625\n",
      "step 1361, loss 7362.18, acc 0.046875\n",
      "step 1362, loss 4935.58, acc 0.109375\n",
      "step 1363, loss 5885.32, acc 0.09375\n",
      "step 1364, loss 6614.42, acc 0.109375\n",
      "step 1365, loss 5875.87, acc 0.0714286\n",
      "step 1366, loss 5706.71, acc 0.078125\n",
      "step 1367, loss 6097.8, acc 0.0625\n",
      "step 1368, loss 4289.48, acc 0.15625\n",
      "step 1369, loss 6755.04, acc 0.171875\n",
      "step 1370, loss 6928.8, acc 0.125\n",
      "step 1371, loss 6175.26, acc 0.03125\n",
      "step 1372, loss 5554.02, acc 0.046875\n",
      "step 1373, loss 5288.55, acc 0.078125\n",
      "step 1374, loss 6653.93, acc 0.015625\n",
      "step 1375, loss 6121.62, acc 0.0625\n",
      "step 1376, loss 6081.95, acc 0.0625\n",
      "step 1377, loss 8583.62, acc 0.078125\n",
      "step 1378, loss 6085.9, acc 0\n",
      "step 1379, loss 9239.31, acc 0.109375\n",
      "step 1380, loss 6551.93, acc 0.15625\n",
      "step 1381, loss 6928.62, acc 0.125\n",
      "step 1382, loss 6221.63, acc 0.09375\n",
      "step 1383, loss 4615.71, acc 0.0625\n",
      "step 1384, loss 5767.1, acc 0.0625\n",
      "step 1385, loss 4677.03, acc 0.09375\n",
      "step 1386, loss 6737.59, acc 0.0625\n",
      "step 1387, loss 5278.14, acc 0.03125\n",
      "step 1388, loss 6737.66, acc 0.046875\n",
      "step 1389, loss 6161.1, acc 0.0625\n",
      "step 1390, loss 6210.71, acc 0.078125\n",
      "step 1391, loss 4390.13, acc 0.0714286\n",
      "step 1392, loss 5076.67, acc 0.1875\n",
      "step 1393, loss 6962.65, acc 0.09375\n",
      "step 1394, loss 6004.99, acc 0.09375\n",
      "step 1395, loss 5001.04, acc 0.15625\n",
      "step 1396, loss 5174.7, acc 0.046875\n",
      "step 1397, loss 4174.17, acc 0.125\n",
      "step 1398, loss 6573.39, acc 0.09375\n",
      "step 1399, loss 6363.41, acc 0.109375\n",
      "step 1400, loss 7153.35, acc 0.125\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1400, loss 46271.1, acc 0.168367, precison 0.0283475635152, recall 0.168367346939, f1_score 0.0485250868907.\n",
      "\n",
      "step 1401, loss 6916.39, acc 0.1875\n",
      "step 1402, loss 5935.54, acc 0.171875\n",
      "step 1403, loss 7703.2, acc 0.125\n",
      "step 1404, loss 4232.95, acc 0.142857\n",
      "step 1405, loss 4990.15, acc 0.09375\n",
      "step 1406, loss 5290.49, acc 0.078125\n",
      "step 1407, loss 7072.63, acc 0.140625\n",
      "step 1408, loss 7688.57, acc 0.140625\n",
      "step 1409, loss 5616.26, acc 0.125\n",
      "step 1410, loss 8008.12, acc 0.078125\n",
      "step 1411, loss 4678.8, acc 0.03125\n",
      "step 1412, loss 4037.4, acc 0.109375\n",
      "step 1413, loss 6726.52, acc 0.0625\n",
      "step 1414, loss 5665.89, acc 0.109375\n",
      "step 1415, loss 5666.62, acc 0.140625\n",
      "step 1416, loss 7534.31, acc 0.09375\n",
      "step 1417, loss 3227.84, acc 0.214286\n",
      "step 1418, loss 4811.29, acc 0.0625\n",
      "step 1419, loss 6644.9, acc 0.125\n",
      "step 1420, loss 6901.64, acc 0.046875\n",
      "step 1421, loss 5777.66, acc 0.125\n",
      "step 1422, loss 4827.54, acc 0.140625\n",
      "step 1423, loss 6064.29, acc 0.0625\n",
      "step 1424, loss 7793.68, acc 0.0625\n",
      "step 1425, loss 4301.43, acc 0.09375\n",
      "step 1426, loss 4507.97, acc 0.203125\n",
      "step 1427, loss 6099.66, acc 0.0625\n",
      "step 1428, loss 5177.95, acc 0.09375\n",
      "step 1429, loss 7480.72, acc 0.078125\n",
      "step 1430, loss 2751.46, acc 0\n",
      "step 1431, loss 7355.18, acc 0.109375\n",
      "step 1432, loss 5507.98, acc 0.140625\n",
      "step 1433, loss 5557.34, acc 0.109375\n",
      "step 1434, loss 6311.91, acc 0.109375\n",
      "step 1435, loss 5786.47, acc 0.078125\n",
      "step 1436, loss 6361.11, acc 0.078125\n",
      "step 1437, loss 4510.75, acc 0.0625\n",
      "step 1438, loss 5026.77, acc 0.15625\n",
      "step 1439, loss 6081.26, acc 0.09375\n",
      "step 1440, loss 4343.82, acc 0.09375\n",
      "step 1441, loss 5317.18, acc 0.09375\n",
      "step 1442, loss 5498.88, acc 0.09375\n",
      "step 1443, loss 2806.95, acc 0.214286\n",
      "step 1444, loss 5447.66, acc 0.03125\n",
      "step 1445, loss 5989.79, acc 0.140625\n",
      "step 1446, loss 6346.54, acc 0.140625\n",
      "step 1447, loss 6776.85, acc 0.078125\n",
      "step 1448, loss 6457.5, acc 0.0625\n",
      "step 1449, loss 4377.14, acc 0.09375\n",
      "step 1450, loss 6161.11, acc 0.078125\n",
      "step 1451, loss 5061.69, acc 0.078125\n",
      "step 1452, loss 6399.88, acc 0.046875\n",
      "step 1453, loss 6114.71, acc 0.078125\n",
      "step 1454, loss 7889.44, acc 0.03125\n",
      "step 1455, loss 5214.23, acc 0.0625\n",
      "step 1456, loss 6866.48, acc 0.0714286\n",
      "step 1457, loss 5372.54, acc 0.046875\n",
      "step 1458, loss 4111.73, acc 0.09375\n",
      "step 1459, loss 6461.75, acc 0.078125\n",
      "step 1460, loss 5985.74, acc 0.0625\n",
      "step 1461, loss 5999.57, acc 0.046875\n",
      "step 1462, loss 9340.4, acc 0.125\n",
      "step 1463, loss 7393.43, acc 0.046875\n",
      "step 1464, loss 7482.99, acc 0.09375\n",
      "step 1465, loss 6121.28, acc 0.078125\n",
      "step 1466, loss 4592.07, acc 0.109375\n",
      "step 1467, loss 4420.36, acc 0.15625\n",
      "step 1468, loss 5851.36, acc 0.09375\n",
      "step 1469, loss 7781.47, acc 0.0714286\n",
      "step 1470, loss 4708.15, acc 0.046875\n",
      "step 1471, loss 5780.42, acc 0.09375\n",
      "step 1472, loss 5246.24, acc 0.109375\n",
      "step 1473, loss 6208.66, acc 0.0625\n",
      "step 1474, loss 8654.69, acc 0.0625\n",
      "step 1475, loss 6132.13, acc 0.078125\n",
      "step 1476, loss 7137.42, acc 0.078125\n",
      "step 1477, loss 11135.8, acc 0.109375\n",
      "step 1478, loss 7361.88, acc 0.109375\n",
      "step 1479, loss 5866.61, acc 0.125\n",
      "step 1480, loss 9802.24, acc 0.109375\n",
      "step 1481, loss 7320.63, acc 0.09375\n",
      "step 1482, loss 4857.13, acc 0.0714286\n",
      "step 1483, loss 8111.13, acc 0.0625\n",
      "step 1484, loss 7098.52, acc 0.0625\n",
      "step 1485, loss 6926.09, acc 0.09375\n",
      "step 1486, loss 8773.77, acc 0.078125\n",
      "step 1487, loss 8120.12, acc 0.09375\n",
      "step 1488, loss 5303.62, acc 0.0625\n",
      "step 1489, loss 7108.37, acc 0.046875\n",
      "step 1490, loss 6182.48, acc 0.09375\n",
      "step 1491, loss 6760.04, acc 0.15625\n",
      "step 1492, loss 7702.78, acc 0.0625\n",
      "step 1493, loss 10740.1, acc 0.140625\n",
      "step 1494, loss 6868.35, acc 0.09375\n",
      "step 1495, loss 4041.2, acc 0\n",
      "step 1496, loss 6436.95, acc 0.0625\n",
      "step 1497, loss 9314.1, acc 0.015625\n",
      "step 1498, loss 6599.33, acc 0.0625\n",
      "step 1499, loss 8803.47, acc 0.109375\n",
      "step 1500, loss 6467.82, acc 0.15625\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1500, loss 56677.8, acc 0.168367, precison 0.0283475635152, recall 0.168367346939, f1_score 0.0485250868907.\n",
      "\n",
      "step 1501, loss 5494.83, acc 0.15625\n",
      "step 1502, loss 7298.92, acc 0.078125\n",
      "step 1503, loss 6489.57, acc 0.125\n",
      "step 1504, loss 6846.06, acc 0.125\n",
      "step 1505, loss 8088.57, acc 0.015625\n",
      "step 1506, loss 8242.53, acc 0.09375\n",
      "step 1507, loss 8748.43, acc 0.1875\n",
      "step 1508, loss 6341.31, acc 0.142857\n",
      "step 1509, loss 7250.06, acc 0.03125\n",
      "step 1510, loss 7832.01, acc 0.1875\n",
      "step 1511, loss 7937.91, acc 0.09375\n",
      "step 1512, loss 10170.2, acc 0.078125\n",
      "step 1513, loss 5812.33, acc 0.125\n",
      "step 1514, loss 8390.62, acc 0.03125\n",
      "step 1515, loss 7774.05, acc 0.078125\n",
      "step 1516, loss 4655.88, acc 0.15625\n",
      "step 1517, loss 6621.02, acc 0.109375\n",
      "step 1518, loss 7523.92, acc 0.078125\n",
      "step 1519, loss 7516.71, acc 0.09375\n",
      "step 1520, loss 4648.54, acc 0.078125\n",
      "step 1521, loss 3967.98, acc 0.142857\n",
      "step 1522, loss 6166.76, acc 0.140625\n",
      "step 1523, loss 9812.33, acc 0.015625\n",
      "step 1524, loss 7252.84, acc 0.0625\n",
      "step 1525, loss 4784.64, acc 0.09375\n",
      "step 1526, loss 6460.95, acc 0.125\n",
      "step 1527, loss 5363.35, acc 0.09375\n",
      "step 1528, loss 8944.82, acc 0.0625\n",
      "step 1529, loss 5976.71, acc 0.09375\n",
      "step 1530, loss 7293.71, acc 0.125\n",
      "step 1531, loss 6463.13, acc 0.125\n",
      "step 1532, loss 7435.52, acc 0.0625\n",
      "step 1533, loss 8648.2, acc 0.0625\n",
      "step 1534, loss 7612.98, acc 0.0714286\n",
      "step 1535, loss 7470.49, acc 0.09375\n",
      "step 1536, loss 11066, acc 0.140625\n",
      "step 1537, loss 5896.84, acc 0.09375\n",
      "step 1538, loss 7767.14, acc 0.078125\n",
      "step 1539, loss 6203.91, acc 0.046875\n",
      "step 1540, loss 7061.16, acc 0.109375\n",
      "step 1541, loss 6911.31, acc 0.15625\n",
      "step 1542, loss 6674.36, acc 0.078125\n",
      "step 1543, loss 6545.2, acc 0.078125\n",
      "step 1544, loss 6514.3, acc 0.0625\n",
      "step 1545, loss 7338.27, acc 0.078125\n",
      "step 1546, loss 4715.37, acc 0.140625\n",
      "step 1547, loss 6116.99, acc 0.285714\n",
      "step 1548, loss 9059.93, acc 0.15625\n",
      "step 1549, loss 6269.56, acc 0.140625\n",
      "step 1550, loss 8211.78, acc 0.046875\n",
      "step 1551, loss 5653.55, acc 0.046875\n",
      "step 1552, loss 9814.75, acc 0.078125\n",
      "step 1553, loss 5742.21, acc 0.046875\n",
      "step 1554, loss 4921.89, acc 0.109375\n",
      "step 1555, loss 6499.04, acc 0.203125\n",
      "step 1556, loss 7002.26, acc 0.046875\n",
      "step 1557, loss 6630.18, acc 0.0625\n",
      "step 1558, loss 6318.19, acc 0.03125\n",
      "step 1559, loss 5254.6, acc 0.015625\n",
      "step 1560, loss 9689.12, acc 0\n",
      "step 1561, loss 5288.47, acc 0.125\n",
      "step 1562, loss 4923.21, acc 0.078125\n",
      "step 1563, loss 5782.55, acc 0.125\n",
      "step 1564, loss 7111.25, acc 0.09375\n",
      "step 1565, loss 7696.94, acc 0.15625\n",
      "step 1566, loss 8187.85, acc 0.1875\n",
      "step 1567, loss 6983.84, acc 0.109375\n",
      "step 1568, loss 9675.64, acc 0.078125\n",
      "step 1569, loss 6059.64, acc 0.03125\n",
      "step 1570, loss 6201.1, acc 0.09375\n",
      "step 1571, loss 7718.68, acc 0.078125\n",
      "step 1572, loss 5458.18, acc 0.109375\n",
      "step 1573, loss 10304.2, acc 0.142857\n",
      "step 1574, loss 8271.31, acc 0.140625\n",
      "step 1575, loss 6600.49, acc 0.125\n",
      "step 1576, loss 8536.25, acc 0.109375\n",
      "step 1577, loss 7251.28, acc 0.09375\n",
      "step 1578, loss 6431.79, acc 0.078125\n",
      "step 1579, loss 6267.79, acc 0.09375\n",
      "step 1580, loss 8026.76, acc 0\n",
      "step 1581, loss 6241.38, acc 0.015625\n",
      "step 1582, loss 6918.65, acc 0.03125\n",
      "step 1583, loss 6370.6, acc 0.0625\n",
      "step 1584, loss 7980.7, acc 0.125\n",
      "step 1585, loss 8490.85, acc 0.109375\n",
      "step 1586, loss 6550.42, acc 0.142857\n",
      "step 1587, loss 8142.21, acc 0.109375\n",
      "step 1588, loss 8558.55, acc 0.140625\n",
      "step 1589, loss 7483.64, acc 0.125\n",
      "step 1590, loss 7959.33, acc 0.09375\n",
      "step 1591, loss 10686.3, acc 0.03125\n",
      "step 1592, loss 7765.17, acc 0.046875\n",
      "step 1593, loss 5796.89, acc 0.09375\n",
      "step 1594, loss 5618.24, acc 0.1875\n",
      "step 1595, loss 6239.61, acc 0.140625\n",
      "step 1596, loss 9792.28, acc 0.140625\n",
      "step 1597, loss 7839.35, acc 0.125\n",
      "step 1598, loss 9648.05, acc 0.078125\n",
      "step 1599, loss 4814.44, acc 0.0714286\n",
      "step 1600, loss 7895.53, acc 0.078125\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1600, loss 69697.9, acc 0.168367, precison 0.0283475635152, recall 0.168367346939, f1_score 0.0485250868907.\n",
      "\n",
      "step 1601, loss 10825.4, acc 0.078125\n",
      "step 1602, loss 6968.72, acc 0.046875\n",
      "step 1603, loss 8857.1, acc 0.09375\n",
      "step 1604, loss 7935.29, acc 0.0625\n",
      "step 1605, loss 7094.2, acc 0.125\n",
      "step 1606, loss 7204.38, acc 0.171875\n",
      "step 1607, loss 6817.9, acc 0.15625\n",
      "step 1608, loss 8124.98, acc 0.0625\n",
      "step 1609, loss 9162.42, acc 0.03125\n",
      "step 1610, loss 5845.28, acc 0.078125\n",
      "step 1611, loss 8944.97, acc 0.046875\n",
      "step 1612, loss 11084.4, acc 0\n",
      "step 1613, loss 7116.61, acc 0.078125\n",
      "step 1614, loss 9339.14, acc 0.203125\n",
      "step 1615, loss 8368.64, acc 0.046875\n",
      "step 1616, loss 5966.9, acc 0.046875\n",
      "step 1617, loss 10800, acc 0.078125\n",
      "step 1618, loss 7546.47, acc 0.09375\n",
      "step 1619, loss 4642.72, acc 0.140625\n",
      "step 1620, loss 5573.04, acc 0.140625\n",
      "step 1621, loss 7559.73, acc 0.078125\n",
      "step 1622, loss 6391.9, acc 0.078125\n",
      "step 1623, loss 6223.83, acc 0.0625\n",
      "step 1624, loss 5311.55, acc 0.0625\n",
      "step 1625, loss 6031.74, acc 0\n",
      "step 1626, loss 9362.53, acc 0.015625\n",
      "step 1627, loss 9756.71, acc 0.03125\n",
      "step 1628, loss 8241.66, acc 0.125\n",
      "step 1629, loss 6676.3, acc 0.171875\n",
      "step 1630, loss 8917.98, acc 0.140625\n",
      "step 1631, loss 7754.58, acc 0.15625\n",
      "step 1632, loss 9396.67, acc 0.046875\n",
      "step 1633, loss 7448.91, acc 0.140625\n",
      "step 1634, loss 7483.8, acc 0.109375\n",
      "step 1635, loss 7651.45, acc 0.125\n",
      "step 1636, loss 9686.08, acc 0.109375\n",
      "step 1637, loss 8067.05, acc 0.0625\n",
      "step 1638, loss 3925.48, acc 0.214286\n",
      "step 1639, loss 8997.51, acc 0.0625\n",
      "step 1640, loss 7006.91, acc 0.046875\n",
      "step 1641, loss 10977, acc 0.015625\n",
      "step 1642, loss 8464.51, acc 0\n",
      "step 1643, loss 7752.17, acc 0.09375\n",
      "step 1644, loss 7858.48, acc 0.109375\n",
      "step 1645, loss 7867.75, acc 0.109375\n",
      "step 1646, loss 9249.73, acc 0.0625\n",
      "step 1647, loss 6122.96, acc 0.046875\n",
      "step 1648, loss 7825.13, acc 0.125\n",
      "step 1649, loss 10361.9, acc 0\n",
      "step 1650, loss 7534.32, acc 0.171875\n",
      "step 1651, loss 4158.45, acc 0.142857\n",
      "step 1652, loss 6932.54, acc 0.078125\n",
      "step 1653, loss 9640.4, acc 0.0625\n",
      "step 1654, loss 8197.7, acc 0.09375\n",
      "step 1655, loss 6477.43, acc 0.15625\n",
      "step 1656, loss 8872.47, acc 0.125\n",
      "step 1657, loss 9488.09, acc 0.078125\n",
      "step 1658, loss 7255.93, acc 0.09375\n",
      "step 1659, loss 6201.11, acc 0.0625\n",
      "step 1660, loss 11234.3, acc 0.03125\n",
      "step 1661, loss 8384.14, acc 0.078125\n",
      "step 1662, loss 6670.24, acc 0.0625\n",
      "step 1663, loss 9200.46, acc 0.03125\n",
      "step 1664, loss 3657.18, acc 0.0714286\n",
      "step 1665, loss 6890.04, acc 0.109375\n",
      "step 1666, loss 8421.42, acc 0.140625\n",
      "step 1667, loss 8704.22, acc 0.125\n",
      "step 1668, loss 6614.7, acc 0.109375\n",
      "step 1669, loss 10259.3, acc 0.046875\n",
      "step 1670, loss 6776.62, acc 0.09375\n",
      "step 1671, loss 9719.04, acc 0.109375\n",
      "step 1672, loss 6449.82, acc 0.140625\n",
      "step 1673, loss 7333.16, acc 0.09375\n",
      "step 1674, loss 8396.5, acc 0.078125\n",
      "step 1675, loss 7285.59, acc 0.015625\n",
      "step 1676, loss 10095.8, acc 0.09375\n",
      "step 1677, loss 3252.64, acc 0.142857\n",
      "step 1678, loss 5755, acc 0.125\n",
      "step 1679, loss 7343.12, acc 0.09375\n",
      "step 1680, loss 6571.14, acc 0.0625\n",
      "step 1681, loss 8408.52, acc 0.09375\n",
      "step 1682, loss 8581.46, acc 0.046875\n",
      "step 1683, loss 6481.56, acc 0.0625\n",
      "step 1684, loss 8379.93, acc 0.078125\n",
      "step 1685, loss 6376.77, acc 0.125\n",
      "step 1686, loss 8025.66, acc 0.15625\n",
      "step 1687, loss 9471.07, acc 0.140625\n",
      "step 1688, loss 6691.16, acc 0.0625\n",
      "step 1689, loss 9293.67, acc 0.078125\n",
      "step 1690, loss 9965.95, acc 0\n",
      "step 1691, loss 9800.07, acc 0.109375\n",
      "step 1692, loss 9507.03, acc 0.0625\n",
      "step 1693, loss 6762.7, acc 0.09375\n",
      "step 1694, loss 8302.67, acc 0.109375\n",
      "step 1695, loss 10490.6, acc 0.140625\n",
      "step 1696, loss 8189.88, acc 0.0625\n",
      "step 1697, loss 11576.1, acc 0.0625\n",
      "step 1698, loss 10533.4, acc 0.0625\n",
      "step 1699, loss 8446.42, acc 0.109375\n",
      "step 1700, loss 7380.9, acc 0.0625\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1700, loss 78291.2, acc 0.168367, precison 0.0283475635152, recall 0.168367346939, f1_score 0.0485250868907.\n",
      "\n",
      "step 1701, loss 9797.67, acc 0.078125\n",
      "step 1702, loss 6784.7, acc 0.109375\n",
      "step 1703, loss 6640.41, acc 0.142857\n",
      "step 1704, loss 12650.6, acc 0.15625\n",
      "step 1705, loss 10924.8, acc 0.109375\n",
      "step 1706, loss 9530.8, acc 0.078125\n",
      "step 1707, loss 7400.73, acc 0.078125\n",
      "step 1708, loss 8036.87, acc 0.046875\n",
      "step 1709, loss 5471.58, acc 0.125\n",
      "step 1710, loss 7554.08, acc 0.0625\n",
      "step 1711, loss 6948.34, acc 0.125\n",
      "step 1712, loss 8402.12, acc 0.140625\n",
      "step 1713, loss 7465.15, acc 0.140625\n",
      "step 1714, loss 5629.87, acc 0.1875\n",
      "step 1715, loss 8579.11, acc 0.109375\n",
      "step 1716, loss 4666.29, acc 0.142857\n",
      "step 1717, loss 8958.03, acc 0.078125\n",
      "step 1718, loss 6514.17, acc 0.0625\n",
      "step 1719, loss 7780.26, acc 0.09375\n",
      "step 1720, loss 11067.4, acc 0.125\n",
      "step 1721, loss 7214.62, acc 0.078125\n",
      "step 1722, loss 7702.19, acc 0.03125\n",
      "step 1723, loss 7862.96, acc 0.03125\n",
      "step 1724, loss 4312.96, acc 0.078125\n",
      "step 1725, loss 7944.9, acc 0.125\n",
      "step 1726, loss 9551.06, acc 0.109375\n",
      "step 1727, loss 7928.36, acc 0.078125\n",
      "step 1728, loss 6407.44, acc 0.125\n",
      "step 1729, loss 11395.8, acc 0.214286\n",
      "step 1730, loss 6145.5, acc 0.078125\n",
      "step 1731, loss 8534.79, acc 0.078125\n",
      "step 1732, loss 7722.23, acc 0.046875\n",
      "step 1733, loss 9940.91, acc 0.078125\n",
      "step 1734, loss 8830.29, acc 0.015625\n",
      "step 1735, loss 11621.1, acc 0.078125\n",
      "step 1736, loss 6538.87, acc 0.203125\n",
      "step 1737, loss 9703.54, acc 0.140625\n",
      "step 1738, loss 5723.12, acc 0.171875\n",
      "step 1739, loss 9921.04, acc 0.046875\n",
      "step 1740, loss 9986.85, acc 0.125\n",
      "step 1741, loss 7471.62, acc 0.15625\n",
      "step 1742, loss 6031.39, acc 0.214286\n",
      "step 1743, loss 7964.04, acc 0.15625\n",
      "step 1744, loss 5718.88, acc 0.109375\n",
      "step 1745, loss 7449.47, acc 0.09375\n",
      "step 1746, loss 9782.97, acc 0.0625\n",
      "step 1747, loss 6776.43, acc 0.078125\n",
      "step 1748, loss 8033.61, acc 0.171875\n",
      "step 1749, loss 6544.36, acc 0.1875\n",
      "step 1750, loss 6916.45, acc 0.0625\n",
      "step 1751, loss 7892.27, acc 0.046875\n",
      "step 1752, loss 10605, acc 0.125\n",
      "step 1753, loss 9806.45, acc 0.0625\n",
      "step 1754, loss 7815.93, acc 0.171875\n",
      "step 1755, loss 4400.61, acc 0.142857\n",
      "step 1756, loss 8223.75, acc 0.09375\n",
      "step 1757, loss 8130.89, acc 0.09375\n",
      "step 1758, loss 7545.51, acc 0.125\n",
      "step 1759, loss 8013.45, acc 0.046875\n",
      "step 1760, loss 8684.34, acc 0.09375\n",
      "step 1761, loss 6295.06, acc 0.125\n",
      "step 1762, loss 6872.32, acc 0.078125\n",
      "step 1763, loss 6848.61, acc 0.109375\n",
      "step 1764, loss 7940.9, acc 0.125\n",
      "step 1765, loss 10272.3, acc 0.0625\n",
      "step 1766, loss 6166.2, acc 0.078125\n",
      "step 1767, loss 6656.27, acc 0.046875\n",
      "step 1768, loss 6344.11, acc 0\n",
      "step 1769, loss 8706.34, acc 0.09375\n",
      "step 1770, loss 7232, acc 0.234375\n",
      "step 1771, loss 8472.72, acc 0.09375\n",
      "step 1772, loss 7538.35, acc 0.09375\n",
      "step 1773, loss 7725.78, acc 0.125\n",
      "step 1774, loss 8377.74, acc 0.0625\n",
      "step 1775, loss 7761.1, acc 0.078125\n",
      "step 1776, loss 9284.03, acc 0.125\n",
      "step 1777, loss 7578.14, acc 0.046875\n",
      "step 1778, loss 5733.31, acc 0.15625\n",
      "step 1779, loss 8751.51, acc 0.046875\n",
      "step 1780, loss 10991.9, acc 0.078125\n",
      "step 1781, loss 11549.2, acc 0.142857\n",
      "step 1782, loss 7075.75, acc 0.09375\n",
      "step 1783, loss 5672.19, acc 0.0625\n",
      "step 1784, loss 8399.22, acc 0.046875\n",
      "step 1785, loss 7114.63, acc 0.21875\n",
      "step 1786, loss 9170.95, acc 0.078125\n",
      "step 1787, loss 8455.56, acc 0.046875\n",
      "step 1788, loss 7942.68, acc 0.046875\n",
      "step 1789, loss 5575.31, acc 0.078125\n",
      "step 1790, loss 10306.6, acc 0.078125\n",
      "step 1791, loss 9033.99, acc 0.140625\n",
      "step 1792, loss 6683.67, acc 0.078125\n",
      "step 1793, loss 8961.08, acc 0.125\n",
      "step 1794, loss 8564.11, acc 0.0714286\n",
      "step 1795, loss 7657.55, acc 0.09375\n",
      "step 1796, loss 8297.98, acc 0.078125\n",
      "step 1797, loss 6937.89, acc 0.0625\n",
      "step 1798, loss 7400.94, acc 0.0625\n",
      "step 1799, loss 6651.64, acc 0.046875\n",
      "step 1800, loss 10343, acc 0.078125\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1800, loss 92834.2, acc 0.112245, precison 0.0125989171179, recall 0.112244897959, f1_score 0.022654933533.\n",
      "\n",
      "step 1801, loss 10942.4, acc 0.140625\n",
      "step 1802, loss 7527.46, acc 0.15625\n",
      "step 1803, loss 8461.51, acc 0.109375\n",
      "step 1804, loss 9164.02, acc 0.1875\n",
      "step 1805, loss 9650.56, acc 0.09375\n",
      "step 1806, loss 10037.6, acc 0.09375\n",
      "step 1807, loss 7954.41, acc 0\n",
      "step 1808, loss 8888.15, acc 0.09375\n",
      "step 1809, loss 8956.58, acc 0.078125\n",
      "step 1810, loss 7116.25, acc 0.125\n",
      "step 1811, loss 8818.06, acc 0.0625\n",
      "step 1812, loss 7535.35, acc 0.109375\n",
      "step 1813, loss 8150.85, acc 0.09375\n",
      "step 1814, loss 9551.82, acc 0.09375\n",
      "step 1815, loss 11603.4, acc 0.109375\n",
      "step 1816, loss 7211.96, acc 0.15625\n",
      "step 1817, loss 9655.74, acc 0.078125\n",
      "step 1818, loss 11139.6, acc 0.046875\n",
      "step 1819, loss 7363.61, acc 0.078125\n",
      "step 1820, loss 12479.6, acc 0.142857\n",
      "step 1821, loss 8584.48, acc 0.0625\n",
      "step 1822, loss 7856.4, acc 0.125\n",
      "step 1823, loss 7645.55, acc 0.0625\n",
      "step 1824, loss 9337.97, acc 0.03125\n",
      "step 1825, loss 7128.2, acc 0.0625\n",
      "step 1826, loss 10795.6, acc 0.0625\n",
      "step 1827, loss 11779, acc 0.0625\n",
      "step 1828, loss 5454.99, acc 0.109375\n",
      "step 1829, loss 9199.36, acc 0.171875\n",
      "step 1830, loss 10741.1, acc 0.109375\n",
      "step 1831, loss 8871.3, acc 0.125\n",
      "step 1832, loss 10920.5, acc 0.0625\n",
      "step 1833, loss 5676.46, acc 0.0714286\n",
      "step 1834, loss 10352.4, acc 0.0625\n",
      "step 1835, loss 12511.7, acc 0.109375\n",
      "step 1836, loss 8851.27, acc 0.09375\n",
      "step 1837, loss 7957.63, acc 0.09375\n",
      "step 1838, loss 6993.95, acc 0.09375\n",
      "step 1839, loss 7948.49, acc 0.078125\n",
      "step 1840, loss 10766.2, acc 0.109375\n",
      "step 1841, loss 11170.2, acc 0.078125\n",
      "step 1842, loss 9772.66, acc 0.09375\n",
      "step 1843, loss 10945.5, acc 0.09375\n",
      "step 1844, loss 10876.2, acc 0.046875\n",
      "step 1845, loss 11063.8, acc 0.125\n",
      "step 1846, loss 5765.98, acc 0.142857\n",
      "step 1847, loss 8756.32, acc 0.0625\n",
      "step 1848, loss 7724.84, acc 0.046875\n",
      "step 1849, loss 7569.02, acc 0.046875\n",
      "step 1850, loss 10506.9, acc 0.109375\n",
      "step 1851, loss 10031.1, acc 0.140625\n",
      "step 1852, loss 7960.11, acc 0.078125\n",
      "step 1853, loss 11004.4, acc 0.078125\n",
      "step 1854, loss 9723.11, acc 0.09375\n",
      "step 1855, loss 8466.98, acc 0.09375\n",
      "step 1856, loss 10416.9, acc 0.09375\n",
      "step 1857, loss 11802.3, acc 0.078125\n",
      "step 1858, loss 7280.91, acc 0.171875\n",
      "step 1859, loss 10455.6, acc 0.0714286\n",
      "step 1860, loss 12699.9, acc 0.078125\n",
      "step 1861, loss 8532.24, acc 0.078125\n",
      "step 1862, loss 9823.32, acc 0.046875\n",
      "step 1863, loss 7922.36, acc 0.078125\n",
      "step 1864, loss 6295.16, acc 0.140625\n",
      "step 1865, loss 8620.82, acc 0.125\n",
      "step 1866, loss 13285.6, acc 0.0625\n",
      "step 1867, loss 7212.95, acc 0.125\n",
      "step 1868, loss 10341.6, acc 0.09375\n",
      "step 1869, loss 12819.4, acc 0.078125\n",
      "step 1870, loss 7357.26, acc 0.0625\n",
      "step 1871, loss 7571.74, acc 0.078125\n",
      "step 1872, loss 6195.45, acc 0.0714286\n",
      "step 1873, loss 7852.63, acc 0.125\n",
      "step 1874, loss 9655.35, acc 0.078125\n",
      "step 1875, loss 10700.8, acc 0.109375\n",
      "step 1876, loss 7767.91, acc 0.1875\n",
      "step 1877, loss 8997.26, acc 0.0625\n",
      "step 1878, loss 9022.28, acc 0.109375\n",
      "step 1879, loss 8560.9, acc 0.109375\n",
      "step 1880, loss 8990.18, acc 0.03125\n",
      "step 1881, loss 13235.5, acc 0.15625\n",
      "step 1882, loss 9526.74, acc 0.0625\n",
      "step 1883, loss 7985.18, acc 0.140625\n",
      "step 1884, loss 9906.05, acc 0.046875\n",
      "step 1885, loss 6061.14, acc 0\n",
      "step 1886, loss 11210.8, acc 0.078125\n",
      "step 1887, loss 9714.71, acc 0.109375\n",
      "step 1888, loss 7853.11, acc 0.0625\n",
      "step 1889, loss 8071.41, acc 0.15625\n",
      "step 1890, loss 8135.15, acc 0.15625\n",
      "step 1891, loss 7408.65, acc 0.03125\n",
      "step 1892, loss 11247, acc 0.09375\n",
      "step 1893, loss 9688.76, acc 0.078125\n",
      "step 1894, loss 7066.33, acc 0.09375\n",
      "step 1895, loss 8836.43, acc 0.046875\n",
      "step 1896, loss 10956.8, acc 0.125\n",
      "step 1897, loss 8985.66, acc 0.109375\n",
      "step 1898, loss 5878.11, acc 0.142857\n",
      "step 1899, loss 10524.2, acc 0.03125\n",
      "step 1900, loss 7128.87, acc 0.046875\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1900, loss 103610, acc 0.173469, precison 0.0513806507825, recall 0.173469387755, f1_score 0.0701379365976.\n",
      "\n",
      "step 1901, loss 11056.5, acc 0.0625\n",
      "step 1902, loss 8454.89, acc 0.0625\n",
      "step 1903, loss 8476.3, acc 0.078125\n",
      "step 1904, loss 10270.6, acc 0.09375\n",
      "step 1905, loss 6750.11, acc 0.03125\n",
      "step 1906, loss 10463.8, acc 0.140625\n",
      "step 1907, loss 9387.39, acc 0.078125\n",
      "step 1908, loss 8624.17, acc 0.21875\n",
      "step 1909, loss 12568.7, acc 0.078125\n",
      "step 1910, loss 12445.6, acc 0.078125\n",
      "step 1911, loss 21137.9, acc 0.0714286\n",
      "step 1912, loss 9803.88, acc 0.078125\n",
      "step 1913, loss 7170.24, acc 0.15625\n",
      "step 1914, loss 11626.2, acc 0.015625\n",
      "step 1915, loss 9048.37, acc 0.078125\n",
      "step 1916, loss 12246.6, acc 0.078125\n",
      "step 1917, loss 9119.96, acc 0.078125\n",
      "step 1918, loss 13701.5, acc 0.046875\n",
      "step 1919, loss 9229.89, acc 0.125\n",
      "step 1920, loss 9356.89, acc 0.046875\n",
      "step 1921, loss 8780.61, acc 0.046875\n",
      "step 1922, loss 10464.2, acc 0.109375\n",
      "step 1923, loss 11719.4, acc 0.078125\n",
      "step 1924, loss 7710.95, acc 0\n",
      "step 1925, loss 10113.3, acc 0.109375\n",
      "step 1926, loss 12910, acc 0.140625\n",
      "step 1927, loss 8049.58, acc 0.078125\n",
      "step 1928, loss 13419.1, acc 0.078125\n",
      "step 1929, loss 10822.1, acc 0.0625\n",
      "step 1930, loss 10934.8, acc 0.21875\n",
      "step 1931, loss 9059.01, acc 0.125\n",
      "step 1932, loss 8693.99, acc 0.125\n",
      "step 1933, loss 10518.9, acc 0.09375\n",
      "step 1934, loss 8978.41, acc 0.109375\n",
      "step 1935, loss 13174.1, acc 0.0625\n",
      "step 1936, loss 13570.7, acc 0.140625\n",
      "step 1937, loss 11171.8, acc 0.142857\n",
      "step 1938, loss 11261.5, acc 0.125\n",
      "step 1939, loss 8979.32, acc 0.046875\n",
      "step 1940, loss 9488.41, acc 0.0625\n",
      "step 1941, loss 10769.2, acc 0.078125\n",
      "step 1942, loss 10476.3, acc 0.03125\n",
      "step 1943, loss 10253.2, acc 0.078125\n",
      "step 1944, loss 9750.71, acc 0.140625\n",
      "step 1945, loss 10223.8, acc 0.140625\n",
      "step 1946, loss 9866.93, acc 0.140625\n",
      "step 1947, loss 6911.37, acc 0.0625\n",
      "step 1948, loss 11842.9, acc 0.03125\n",
      "step 1949, loss 12766.4, acc 0.046875\n",
      "step 1950, loss 14324, acc 0.0714286\n",
      "step 1951, loss 9920.69, acc 0.109375\n",
      "step 1952, loss 15960.9, acc 0.109375\n",
      "step 1953, loss 11196, acc 0.125\n",
      "step 1954, loss 13961.2, acc 0.03125\n",
      "step 1955, loss 10705.4, acc 0.078125\n",
      "step 1956, loss 10480.6, acc 0.03125\n",
      "step 1957, loss 8056.82, acc 0.125\n",
      "step 1958, loss 8963.31, acc 0.171875\n",
      "step 1959, loss 9652.4, acc 0.078125\n",
      "step 1960, loss 13651.4, acc 0.09375\n",
      "step 1961, loss 8574.58, acc 0.109375\n",
      "step 1962, loss 9772.67, acc 0.0625\n",
      "step 1963, loss 9860.02, acc 0.0714286\n",
      "step 1964, loss 8984.4, acc 0.09375\n",
      "step 1965, loss 13145.4, acc 0.03125\n",
      "step 1966, loss 9349.09, acc 0.125\n",
      "step 1967, loss 10802.5, acc 0.15625\n",
      "step 1968, loss 11192.5, acc 0.125\n",
      "step 1969, loss 9337.75, acc 0.046875\n",
      "step 1970, loss 12461.5, acc 0.015625\n",
      "step 1971, loss 8905.68, acc 0.046875\n",
      "step 1972, loss 10039.2, acc 0.125\n",
      "step 1973, loss 11616.8, acc 0.125\n",
      "step 1974, loss 10140.8, acc 0.140625\n",
      "step 1975, loss 9578.84, acc 0.078125\n",
      "step 1976, loss 14063.4, acc 0\n",
      "step 1977, loss 11684, acc 0.03125\n",
      "step 1978, loss 14434, acc 0.140625\n",
      "step 1979, loss 8220.2, acc 0.078125\n",
      "step 1980, loss 10623.6, acc 0.09375\n",
      "step 1981, loss 12512.2, acc 0.046875\n",
      "step 1982, loss 9151.78, acc 0\n",
      "step 1983, loss 9577.18, acc 0.046875\n",
      "step 1984, loss 8055.51, acc 0.078125\n",
      "step 1985, loss 9646.07, acc 0.109375\n",
      "step 1986, loss 8657.95, acc 0.09375\n",
      "step 1987, loss 11437.8, acc 0.046875\n",
      "step 1988, loss 13032.6, acc 0.078125\n",
      "step 1989, loss 8322.04, acc 0.0714286\n",
      "step 1990, loss 8686.04, acc 0.140625\n",
      "step 1991, loss 11884.1, acc 0.078125\n",
      "step 1992, loss 8507.77, acc 0.140625\n",
      "step 1993, loss 9605.42, acc 0.125\n",
      "step 1994, loss 9118.48, acc 0.078125\n",
      "step 1995, loss 14751.1, acc 0.046875\n",
      "step 1996, loss 10275.8, acc 0.125\n",
      "step 1997, loss 10810.9, acc 0.0625\n",
      "step 1998, loss 8120.16, acc 0.109375\n",
      "step 1999, loss 12892.3, acc 0.125\n",
      "step 2000, loss 10736.9, acc 0.125\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2000, loss 119834, acc 0.0765306, precison 0.022211281119, recall 0.0765306122449, f1_score 0.0335526330607.\n",
      "\n",
      "step 2001, loss 10434.1, acc 0.109375\n",
      "step 2002, loss 11736.1, acc 0.0714286\n",
      "step 2003, loss 7939.55, acc 0.0625\n",
      "step 2004, loss 10980.9, acc 0.0625\n",
      "step 2005, loss 7905.7, acc 0.078125\n",
      "step 2006, loss 8114.86, acc 0.109375\n",
      "step 2007, loss 9251.95, acc 0.046875\n",
      "step 2008, loss 9328.98, acc 0.03125\n",
      "step 2009, loss 8277.33, acc 0.046875\n",
      "step 2010, loss 13555.2, acc 0.015625\n",
      "step 2011, loss 13411.3, acc 0.125\n",
      "step 2012, loss 10459.3, acc 0.109375\n",
      "step 2013, loss 10827.7, acc 0.109375\n",
      "step 2014, loss 7345.79, acc 0.125\n",
      "step 2015, loss 11152.9, acc 0.285714\n",
      "step 2016, loss 11492.1, acc 0.09375\n",
      "step 2017, loss 9553.28, acc 0.140625\n",
      "step 2018, loss 11343.8, acc 0.0625\n",
      "step 2019, loss 5975.91, acc 0.140625\n",
      "step 2020, loss 10979.5, acc 0.09375\n",
      "step 2021, loss 7075.05, acc 0.078125\n",
      "step 2022, loss 15803, acc 0.046875\n",
      "step 2023, loss 6168.32, acc 0.140625\n",
      "step 2024, loss 9085.99, acc 0.046875\n",
      "step 2025, loss 7628.26, acc 0.125\n",
      "step 2026, loss 8857.5, acc 0.046875\n",
      "step 2027, loss 9906.88, acc 0.09375\n",
      "step 2028, loss 10371.1, acc 0.142857\n",
      "step 2029, loss 8898.1, acc 0.15625\n",
      "step 2030, loss 10227.8, acc 0.0625\n",
      "step 2031, loss 11579.7, acc 0.078125\n",
      "step 2032, loss 8882.78, acc 0.0625\n",
      "step 2033, loss 12670.1, acc 0.109375\n",
      "step 2034, loss 7616.24, acc 0.109375\n",
      "step 2035, loss 13307.3, acc 0.171875\n",
      "step 2036, loss 8807.11, acc 0.109375\n",
      "step 2037, loss 7727.44, acc 0.09375\n",
      "step 2038, loss 12762.3, acc 0.09375\n",
      "step 2039, loss 8724.46, acc 0.09375\n",
      "step 2040, loss 8843.17, acc 0.0625\n",
      "step 2041, loss 10120.1, acc 0.142857\n",
      "step 2042, loss 14288.1, acc 0.078125\n",
      "step 2043, loss 8324.34, acc 0.09375\n",
      "step 2044, loss 7465.34, acc 0.109375\n",
      "step 2045, loss 10044.8, acc 0.09375\n",
      "step 2046, loss 11373.4, acc 0.03125\n",
      "step 2047, loss 11176.4, acc 0.078125\n",
      "step 2048, loss 9399.91, acc 0.078125\n",
      "step 2049, loss 11214.9, acc 0.078125\n",
      "step 2050, loss 8772.75, acc 0.09375\n",
      "step 2051, loss 12438.6, acc 0.09375\n",
      "step 2052, loss 11676.2, acc 0.0625\n",
      "step 2053, loss 6912.33, acc 0.1875\n",
      "step 2054, loss 5108.34, acc 0.0714286\n",
      "step 2055, loss 8059.11, acc 0.125\n",
      "step 2056, loss 11814.6, acc 0.015625\n",
      "step 2057, loss 9548.04, acc 0.09375\n",
      "step 2058, loss 9154.47, acc 0.03125\n",
      "step 2059, loss 10637.2, acc 0.09375\n",
      "step 2060, loss 9806.91, acc 0.109375\n",
      "step 2061, loss 9668.11, acc 0.078125\n",
      "step 2062, loss 11671.8, acc 0.140625\n",
      "step 2063, loss 10745.4, acc 0.140625\n",
      "step 2064, loss 10027.1, acc 0.140625\n",
      "step 2065, loss 12499.2, acc 0.125\n",
      "step 2066, loss 9240.77, acc 0.078125\n",
      "step 2067, loss 11571.3, acc 0.214286\n",
      "step 2068, loss 5726.46, acc 0.140625\n",
      "step 2069, loss 11377, acc 0.09375\n",
      "step 2070, loss 7847.27, acc 0.078125\n",
      "step 2071, loss 6996.25, acc 0.046875\n",
      "step 2072, loss 10706.9, acc 0.125\n",
      "step 2073, loss 11200.7, acc 0.125\n",
      "step 2074, loss 11689.1, acc 0.109375\n",
      "step 2075, loss 10932.9, acc 0.15625\n",
      "step 2076, loss 8780.44, acc 0.109375\n",
      "step 2077, loss 11815.5, acc 0.0625\n",
      "step 2078, loss 12486, acc 0.03125\n",
      "step 2079, loss 6950.08, acc 0.078125\n",
      "step 2080, loss 12066.2, acc 0.0714286\n",
      "step 2081, loss 12809.3, acc 0.078125\n",
      "step 2082, loss 8975.16, acc 0.1875\n",
      "step 2083, loss 9320.68, acc 0.09375\n",
      "step 2084, loss 9246, acc 0.0625\n",
      "step 2085, loss 8758.62, acc 0.1875\n",
      "step 2086, loss 10642.2, acc 0.109375\n",
      "step 2087, loss 10417.6, acc 0.015625\n",
      "step 2088, loss 13758.9, acc 0.03125\n",
      "step 2089, loss 11899.9, acc 0.140625\n",
      "step 2090, loss 7843.33, acc 0.109375\n",
      "step 2091, loss 10231.4, acc 0.125\n",
      "step 2092, loss 14532, acc 0.171875\n",
      "step 2093, loss 10315.9, acc 0.0714286\n",
      "step 2094, loss 9870.8, acc 0\n",
      "step 2095, loss 8365.75, acc 0.078125\n",
      "step 2096, loss 10958.5, acc 0\n",
      "step 2097, loss 11621.1, acc 0.0625\n",
      "step 2098, loss 12619.5, acc 0.109375\n",
      "step 2099, loss 9892.05, acc 0.046875\n",
      "step 2100, loss 9723.39, acc 0.078125\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2100, loss 136825, acc 0.0816327, precison 0.0130350088296, recall 0.0816326530612, f1_score 0.0194515306122.\n",
      "\n",
      "step 2101, loss 12032.4, acc 0.078125\n",
      "step 2102, loss 8307.96, acc 0.078125\n",
      "step 2103, loss 11917.8, acc 0.109375\n",
      "step 2104, loss 8927.52, acc 0.09375\n",
      "step 2105, loss 8686.36, acc 0.09375\n",
      "step 2106, loss 14224, acc 0\n",
      "step 2107, loss 11966.2, acc 0.078125\n",
      "step 2108, loss 8733.14, acc 0.140625\n",
      "step 2109, loss 8669.89, acc 0.109375\n",
      "step 2110, loss 12356.8, acc 0.046875\n",
      "step 2111, loss 11422.3, acc 0.109375\n",
      "step 2112, loss 8718.71, acc 0.140625\n",
      "step 2113, loss 10251.5, acc 0.078125\n",
      "step 2114, loss 13166.6, acc 0.125\n",
      "step 2115, loss 9543.11, acc 0.015625\n",
      "step 2116, loss 8047.68, acc 0.125\n",
      "step 2117, loss 9467.59, acc 0.046875\n",
      "step 2118, loss 8629.84, acc 0.0625\n",
      "step 2119, loss 4617.64, acc 0.142857\n",
      "step 2120, loss 11131.9, acc 0.109375\n",
      "step 2121, loss 11954.3, acc 0.125\n",
      "step 2122, loss 9941.74, acc 0.140625\n",
      "step 2123, loss 9034.44, acc 0.09375\n",
      "step 2124, loss 9921.8, acc 0.140625\n",
      "step 2125, loss 10052.2, acc 0.1875\n",
      "step 2126, loss 14017, acc 0.09375\n",
      "step 2127, loss 10484.9, acc 0.09375\n",
      "step 2128, loss 10893.1, acc 0.0625\n",
      "step 2129, loss 9385.97, acc 0.078125\n",
      "step 2130, loss 13695.6, acc 0.09375\n",
      "step 2131, loss 10778.7, acc 0.109375\n",
      "step 2132, loss 10279, acc 0.285714\n",
      "step 2133, loss 8562.96, acc 0.09375\n",
      "step 2134, loss 11712.7, acc 0.09375\n",
      "step 2135, loss 11617.9, acc 0.09375\n",
      "step 2136, loss 12459.2, acc 0.109375\n",
      "step 2137, loss 7721.35, acc 0.046875\n",
      "step 2138, loss 10573.6, acc 0.125\n",
      "step 2139, loss 11472.3, acc 0.203125\n",
      "step 2140, loss 8244.62, acc 0.09375\n",
      "step 2141, loss 9590.69, acc 0.0625\n",
      "step 2142, loss 10900.8, acc 0.078125\n",
      "step 2143, loss 15842.6, acc 0.125\n",
      "step 2144, loss 11698.1, acc 0.109375\n",
      "step 2145, loss 11356.3, acc 0.142857\n",
      "step 2146, loss 11414, acc 0.078125\n",
      "step 2147, loss 13531.2, acc 0.09375\n",
      "step 2148, loss 8048.95, acc 0.140625\n",
      "step 2149, loss 14080.9, acc 0.078125\n",
      "step 2150, loss 10241.9, acc 0.109375\n",
      "step 2151, loss 11181.7, acc 0.046875\n",
      "step 2152, loss 10603.7, acc 0.078125\n",
      "step 2153, loss 9379.25, acc 0.09375\n",
      "step 2154, loss 16387.4, acc 0.078125\n",
      "step 2155, loss 15029.3, acc 0.109375\n",
      "step 2156, loss 12639.1, acc 0.125\n",
      "step 2157, loss 10323.6, acc 0.078125\n",
      "step 2158, loss 9432.82, acc 0.0714286\n",
      "step 2159, loss 11448.4, acc 0.0625\n",
      "step 2160, loss 8398.39, acc 0.0625\n",
      "step 2161, loss 14477.5, acc 0.046875\n",
      "step 2162, loss 8815.57, acc 0.078125\n",
      "step 2163, loss 8606.11, acc 0.125\n",
      "step 2164, loss 7782.25, acc 0.109375\n",
      "step 2165, loss 12041.2, acc 0.09375\n",
      "step 2166, loss 12377.3, acc 0.078125\n",
      "step 2167, loss 11927.8, acc 0.09375\n",
      "step 2168, loss 10483.7, acc 0.0625\n",
      "step 2169, loss 15208.6, acc 0.125\n",
      "step 2170, loss 11773.4, acc 0.125\n",
      "step 2171, loss 8868.79, acc 0.142857\n",
      "step 2172, loss 10816.7, acc 0.046875\n",
      "step 2173, loss 13908.5, acc 0.078125\n",
      "step 2174, loss 9326.95, acc 0.078125\n",
      "step 2175, loss 10524.1, acc 0.046875\n",
      "step 2176, loss 11117.5, acc 0.15625\n",
      "step 2177, loss 10276.4, acc 0.140625\n",
      "step 2178, loss 14845.8, acc 0.09375\n",
      "step 2179, loss 9736.37, acc 0.0625\n",
      "step 2180, loss 12616.8, acc 0.046875\n",
      "step 2181, loss 10820.1, acc 0.046875\n",
      "step 2182, loss 12411.5, acc 0.109375\n",
      "step 2183, loss 11581.7, acc 0.078125\n",
      "step 2184, loss 7559.96, acc 0\n",
      "step 2185, loss 9695.12, acc 0.15625\n",
      "step 2186, loss 13559.4, acc 0.09375\n",
      "step 2187, loss 9762.13, acc 0.109375\n",
      "step 2188, loss 10473.5, acc 0.140625\n",
      "step 2189, loss 13930.7, acc 0.078125\n",
      "step 2190, loss 14424.7, acc 0.03125\n",
      "step 2191, loss 9043.74, acc 0.09375\n",
      "step 2192, loss 10422.9, acc 0.078125\n",
      "step 2193, loss 11678.3, acc 0.0625\n",
      "step 2194, loss 8533.78, acc 0.046875\n",
      "step 2195, loss 10145.3, acc 0.078125\n",
      "step 2196, loss 6663.19, acc 0.15625\n",
      "step 2197, loss 13947.9, acc 0.0714286\n",
      "step 2198, loss 8153.77, acc 0.140625\n",
      "step 2199, loss 10688.3, acc 0.046875\n",
      "step 2200, loss 11839.2, acc 0.0625\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2200, loss 153040, acc 0.122449, precison 0.182208267923, recall 0.122448979592, f1_score 0.0346652422437.\n",
      "\n",
      "step 2201, loss 8359.84, acc 0.140625\n",
      "step 2202, loss 11960.7, acc 0.15625\n",
      "step 2203, loss 13073.6, acc 0.140625\n",
      "step 2204, loss 9315.77, acc 0.09375\n",
      "step 2205, loss 10739.4, acc 0.09375\n",
      "step 2206, loss 11519.3, acc 0.109375\n",
      "step 2207, loss 13239.7, acc 0.078125\n",
      "step 2208, loss 10361.5, acc 0.09375\n",
      "step 2209, loss 11044.1, acc 0.046875\n",
      "step 2210, loss 8538.71, acc 0.0714286\n",
      "step 2211, loss 14159.6, acc 0.09375\n",
      "step 2212, loss 7927.78, acc 0.125\n",
      "step 2213, loss 10014.4, acc 0.078125\n",
      "step 2214, loss 9450.18, acc 0.046875\n",
      "step 2215, loss 10904.9, acc 0.046875\n",
      "step 2216, loss 11827.8, acc 0.15625\n",
      "step 2217, loss 9186.72, acc 0.015625\n",
      "step 2218, loss 11115.3, acc 0.09375\n",
      "step 2219, loss 10569.1, acc 0.09375\n",
      "step 2220, loss 8992.24, acc 0.1875\n",
      "step 2221, loss 11394.9, acc 0.03125\n",
      "step 2222, loss 10700, acc 0.109375\n",
      "step 2223, loss 17210.3, acc 0.0714286\n",
      "step 2224, loss 12099.4, acc 0.078125\n",
      "step 2225, loss 12077, acc 0.03125\n",
      "step 2226, loss 12947.9, acc 0.078125\n",
      "step 2227, loss 10796, acc 0.046875\n",
      "step 2228, loss 8731.24, acc 0.046875\n",
      "step 2229, loss 9517.45, acc 0.078125\n",
      "step 2230, loss 11437.1, acc 0.078125\n",
      "step 2231, loss 13219.4, acc 0.171875\n",
      "step 2232, loss 12302.5, acc 0.15625\n",
      "step 2233, loss 9900.03, acc 0.078125\n",
      "step 2234, loss 10058.3, acc 0.046875\n",
      "step 2235, loss 12177.4, acc 0.03125\n",
      "step 2236, loss 19884.3, acc 0.0714286\n",
      "step 2237, loss 9537.41, acc 0.078125\n",
      "step 2238, loss 13316.9, acc 0.09375\n",
      "step 2239, loss 15386.8, acc 0.140625\n",
      "step 2240, loss 13224.6, acc 0.03125\n",
      "step 2241, loss 12642.9, acc 0.09375\n",
      "step 2242, loss 12889.9, acc 0.078125\n",
      "step 2243, loss 11125.1, acc 0.0625\n",
      "step 2244, loss 14287.9, acc 0.125\n",
      "step 2245, loss 15381.2, acc 0.125\n",
      "step 2246, loss 15129.4, acc 0.046875\n",
      "step 2247, loss 11375.6, acc 0.09375\n",
      "step 2248, loss 12248.5, acc 0.15625\n",
      "step 2249, loss 10638.7, acc 0.0714286\n",
      "step 2250, loss 10887.1, acc 0.09375\n",
      "step 2251, loss 10585.4, acc 0.03125\n",
      "step 2252, loss 9332.56, acc 0.046875\n",
      "step 2253, loss 10519.3, acc 0.15625\n",
      "step 2254, loss 7433.48, acc 0.125\n",
      "step 2255, loss 17408.6, acc 0.078125\n",
      "step 2256, loss 13998.1, acc 0.109375\n",
      "step 2257, loss 15373.9, acc 0.140625\n",
      "step 2258, loss 10990.2, acc 0.0625\n",
      "step 2259, loss 11222.1, acc 0.125\n",
      "step 2260, loss 10972.7, acc 0.15625\n",
      "step 2261, loss 15375.4, acc 0.046875\n",
      "step 2262, loss 8975.46, acc 0\n",
      "step 2263, loss 10548.9, acc 0.09375\n",
      "step 2264, loss 11675.7, acc 0.078125\n",
      "step 2265, loss 17241.9, acc 0.09375\n",
      "step 2266, loss 15092.6, acc 0.03125\n",
      "step 2267, loss 12064.5, acc 0.109375\n",
      "step 2268, loss 12305, acc 0.15625\n",
      "step 2269, loss 13293.9, acc 0.125\n",
      "step 2270, loss 10347.7, acc 0.03125\n",
      "step 2271, loss 11283.3, acc 0.046875\n",
      "step 2272, loss 16129.2, acc 0.03125\n",
      "step 2273, loss 10092.4, acc 0.1875\n",
      "step 2274, loss 11965.2, acc 0.140625\n",
      "step 2275, loss 11570.4, acc 0\n",
      "step 2276, loss 9300.77, acc 0.21875\n",
      "step 2277, loss 9634.22, acc 0.140625\n",
      "step 2278, loss 16011.5, acc 0.109375\n",
      "step 2279, loss 12828.4, acc 0.078125\n",
      "step 2280, loss 14036.4, acc 0.015625\n",
      "step 2281, loss 11457.3, acc 0.109375\n",
      "step 2282, loss 9980.98, acc 0.046875\n",
      "step 2283, loss 12766.1, acc 0.015625\n",
      "step 2284, loss 14037.6, acc 0.125\n",
      "step 2285, loss 13700.1, acc 0.140625\n",
      "step 2286, loss 14428.5, acc 0.109375\n",
      "step 2287, loss 14066.1, acc 0.109375\n",
      "step 2288, loss 22787.7, acc 0.0714286\n",
      "step 2289, loss 16158.4, acc 0.0625\n",
      "step 2290, loss 11411, acc 0.03125\n",
      "step 2291, loss 9916.13, acc 0.078125\n",
      "step 2292, loss 11047.2, acc 0.0625\n",
      "step 2293, loss 9778.09, acc 0.0625\n",
      "step 2294, loss 12918.9, acc 0.15625\n",
      "step 2295, loss 15093.5, acc 0.109375\n",
      "step 2296, loss 18646.2, acc 0.125\n",
      "step 2297, loss 14542.4, acc 0.078125\n",
      "step 2298, loss 10730.6, acc 0.078125\n",
      "step 2299, loss 12692.5, acc 0.0625\n",
      "step 2300, loss 13291.4, acc 0.078125\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2300, loss 174034, acc 0.168367, precison 0.0283475635152, recall 0.168367346939, f1_score 0.0485250868907.\n",
      "\n",
      "step 2301, loss 7098, acc 0.214286\n",
      "step 2302, loss 9770.75, acc 0.15625\n",
      "step 2303, loss 17056, acc 0.125\n",
      "step 2304, loss 16909.5, acc 0.125\n",
      "step 2305, loss 18337.4, acc 0.09375\n",
      "step 2306, loss 16152.2, acc 0.046875\n",
      "step 2307, loss 23539.7, acc 0.0625\n",
      "step 2308, loss 16426.5, acc 0.09375\n",
      "step 2309, loss 17468.9, acc 0.0625\n",
      "step 2310, loss 14132.7, acc 0.09375\n",
      "step 2311, loss 16572.8, acc 0.09375\n",
      "step 2312, loss 11452.8, acc 0.0625\n",
      "step 2313, loss 11827.6, acc 0.125\n",
      "step 2314, loss 11541.7, acc 0.214286\n",
      "step 2315, loss 16942.6, acc 0.078125\n",
      "step 2316, loss 16357.9, acc 0.046875\n",
      "step 2317, loss 9968.64, acc 0.125\n",
      "step 2318, loss 14174.5, acc 0.140625\n",
      "step 2319, loss 12467.2, acc 0.046875\n",
      "step 2320, loss 10866.5, acc 0.15625\n",
      "step 2321, loss 11384.5, acc 0.15625\n",
      "step 2322, loss 11329.6, acc 0.09375\n",
      "step 2323, loss 14367.2, acc 0.03125\n",
      "step 2324, loss 12660.7, acc 0.09375\n",
      "step 2325, loss 13583.1, acc 0.078125\n",
      "step 2326, loss 14324.7, acc 0.15625\n",
      "step 2327, loss 12815, acc 0\n",
      "step 2328, loss 9251.57, acc 0.09375\n",
      "step 2329, loss 12707.8, acc 0.0625\n",
      "step 2330, loss 10070.4, acc 0.0625\n",
      "step 2331, loss 14629.8, acc 0.046875\n",
      "step 2332, loss 11194.3, acc 0.171875\n",
      "step 2333, loss 14904.3, acc 0.0625\n",
      "step 2334, loss 7901.95, acc 0.140625\n",
      "step 2335, loss 16373.2, acc 0.0625\n",
      "step 2336, loss 11377.9, acc 0.140625\n",
      "step 2337, loss 15671.4, acc 0.078125\n",
      "step 2338, loss 17225, acc 0.046875\n",
      "step 2339, loss 13499.5, acc 0.0625\n",
      "step 2340, loss 12425.5, acc 0.0714286\n",
      "step 2341, loss 15849.4, acc 0.03125\n",
      "step 2342, loss 13156.9, acc 0.046875\n",
      "step 2343, loss 10860.7, acc 0.0625\n",
      "step 2344, loss 17058.2, acc 0.046875\n",
      "step 2345, loss 16521.9, acc 0.109375\n",
      "step 2346, loss 11536.9, acc 0.140625\n",
      "step 2347, loss 12425.3, acc 0.046875\n",
      "step 2348, loss 9417.76, acc 0\n",
      "step 2349, loss 15212.7, acc 0.140625\n",
      "step 2350, loss 11787.7, acc 0.109375\n",
      "step 2351, loss 11075.4, acc 0.140625\n",
      "step 2352, loss 9537.68, acc 0.09375\n",
      "step 2353, loss 10622.3, acc 0.0714286\n",
      "step 2354, loss 11110.7, acc 0.125\n",
      "step 2355, loss 11383.5, acc 0.15625\n",
      "step 2356, loss 10076.4, acc 0.078125\n",
      "step 2357, loss 11956.2, acc 0.0625\n",
      "step 2358, loss 15461, acc 0.125\n",
      "step 2359, loss 13308.7, acc 0.078125\n",
      "step 2360, loss 9473.64, acc 0.09375\n",
      "step 2361, loss 12336.5, acc 0.046875\n",
      "step 2362, loss 13306.4, acc 0.046875\n",
      "step 2363, loss 13328.9, acc 0.078125\n",
      "step 2364, loss 12785, acc 0.0625\n",
      "step 2365, loss 16047.2, acc 0.078125\n",
      "step 2366, loss 26759.2, acc 0.0714286\n",
      "step 2367, loss 12610.1, acc 0.125\n",
      "step 2368, loss 14049.2, acc 0.0625\n",
      "step 2369, loss 10360.8, acc 0.046875\n",
      "step 2370, loss 18767.7, acc 0.046875\n",
      "step 2371, loss 11154.7, acc 0.0625\n",
      "step 2372, loss 11144.3, acc 0.078125\n",
      "step 2373, loss 10791.6, acc 0.15625\n",
      "step 2374, loss 16313.7, acc 0.046875\n",
      "step 2375, loss 13217, acc 0.125\n",
      "step 2376, loss 8507.63, acc 0.109375\n",
      "step 2377, loss 11043.9, acc 0.078125\n",
      "step 2378, loss 10811.7, acc 0.03125\n",
      "step 2379, loss 12845.3, acc 0\n",
      "step 2380, loss 11393.9, acc 0.09375\n",
      "step 2381, loss 14939.3, acc 0.09375\n",
      "step 2382, loss 9490.41, acc 0.0625\n",
      "step 2383, loss 14864.5, acc 0.0625\n",
      "step 2384, loss 10775.6, acc 0.03125\n",
      "step 2385, loss 10419.6, acc 0.109375\n",
      "step 2386, loss 19171.1, acc 0.09375\n",
      "step 2387, loss 10510.6, acc 0.046875\n",
      "step 2388, loss 9945.07, acc 0.109375\n",
      "step 2389, loss 11838.8, acc 0.109375\n",
      "step 2390, loss 15828.9, acc 0.125\n",
      "step 2391, loss 12371, acc 0.078125\n",
      "step 2392, loss 17746.5, acc 0\n",
      "step 2393, loss 16999.5, acc 0.09375\n",
      "step 2394, loss 17923.8, acc 0.09375\n",
      "step 2395, loss 11553.2, acc 0.078125\n",
      "step 2396, loss 13233.8, acc 0.0625\n",
      "step 2397, loss 18287.7, acc 0.125\n",
      "step 2398, loss 14611.9, acc 0.078125\n",
      "step 2399, loss 13823.3, acc 0.140625\n",
      "step 2400, loss 11833.9, acc 0.125\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2400, loss 192387, acc 0.112245, precison 0.0134205856256, recall 0.112244897959, f1_score 0.023974638399.\n",
      "\n",
      "step 2401, loss 14114.9, acc 0.0625\n",
      "step 2402, loss 10380.5, acc 0.109375\n",
      "step 2403, loss 12004.8, acc 0.078125\n",
      "step 2404, loss 14953.2, acc 0.078125\n",
      "step 2405, loss 8494.36, acc 0\n",
      "step 2406, loss 20987.2, acc 0.0625\n",
      "step 2407, loss 18618.7, acc 0.171875\n",
      "step 2408, loss 15948.5, acc 0.09375\n",
      "step 2409, loss 15540.3, acc 0.046875\n",
      "step 2410, loss 11837.6, acc 0.046875\n",
      "step 2411, loss 18136.1, acc 0.078125\n",
      "step 2412, loss 10997.4, acc 0.140625\n",
      "step 2413, loss 11561, acc 0.171875\n",
      "step 2414, loss 11335.4, acc 0.125\n",
      "step 2415, loss 12304.6, acc 0.140625\n",
      "step 2416, loss 16675.9, acc 0.078125\n",
      "step 2417, loss 16439.4, acc 0.171875\n",
      "step 2418, loss 9676.21, acc 0.0714286\n",
      "step 2419, loss 11897.1, acc 0.078125\n",
      "step 2420, loss 19866.4, acc 0.0625\n",
      "step 2421, loss 11072.5, acc 0.0625\n",
      "step 2422, loss 17346.2, acc 0.125\n",
      "step 2423, loss 16161, acc 0.140625\n",
      "step 2424, loss 14178.9, acc 0.125\n",
      "step 2425, loss 22019, acc 0.078125\n",
      "step 2426, loss 11445.4, acc 0.078125\n",
      "step 2427, loss 20298.7, acc 0.0625\n",
      "step 2428, loss 15032.5, acc 0.078125\n",
      "step 2429, loss 13557.8, acc 0.046875\n",
      "step 2430, loss 15496.9, acc 0.078125\n",
      "step 2431, loss 6246.82, acc 0.142857\n",
      "step 2432, loss 13046.8, acc 0.125\n",
      "step 2433, loss 16976, acc 0.0625\n",
      "step 2434, loss 11701.9, acc 0.140625\n",
      "step 2435, loss 11518.9, acc 0.140625\n",
      "step 2436, loss 17439.1, acc 0.078125\n",
      "step 2437, loss 13614.7, acc 0.0625\n",
      "step 2438, loss 13075, acc 0.03125\n",
      "step 2439, loss 11272.6, acc 0.078125\n",
      "step 2440, loss 14774.7, acc 0.0625\n",
      "step 2441, loss 15665.1, acc 0.0625\n",
      "step 2442, loss 12034.1, acc 0.109375\n",
      "step 2443, loss 14339.1, acc 0.125\n",
      "step 2444, loss 12294.8, acc 0.357143\n",
      "step 2445, loss 15452.1, acc 0.109375\n",
      "step 2446, loss 14940.4, acc 0.125\n",
      "step 2447, loss 12913.2, acc 0.140625\n",
      "step 2448, loss 16592.4, acc 0.125\n",
      "step 2449, loss 13487.5, acc 0.078125\n",
      "step 2450, loss 11210.3, acc 0.09375\n",
      "step 2451, loss 12944.8, acc 0.078125\n",
      "step 2452, loss 14527, acc 0.109375\n",
      "step 2453, loss 21890, acc 0.0625\n",
      "step 2454, loss 13926.7, acc 0.109375\n",
      "step 2455, loss 17823, acc 0.078125\n",
      "step 2456, loss 15511.5, acc 0.0625\n",
      "step 2457, loss 6434.57, acc 0.0714286\n",
      "step 2458, loss 17369.6, acc 0.0625\n",
      "step 2459, loss 15914.5, acc 0.0625\n",
      "step 2460, loss 16374.7, acc 0.1875\n",
      "step 2461, loss 14988.3, acc 0.15625\n",
      "step 2462, loss 13564.8, acc 0.125\n",
      "step 2463, loss 13829.1, acc 0.03125\n",
      "step 2464, loss 11510.9, acc 0.09375\n",
      "step 2465, loss 11848.2, acc 0.046875\n",
      "step 2466, loss 13120.8, acc 0.0625\n",
      "step 2467, loss 13165.3, acc 0.234375\n",
      "step 2468, loss 11004.2, acc 0.125\n",
      "step 2469, loss 13738.3, acc 0.140625\n",
      "step 2470, loss 19589.5, acc 0.0714286\n",
      "step 2471, loss 18376.2, acc 0.125\n",
      "step 2472, loss 13156.9, acc 0.078125\n",
      "step 2473, loss 17109.6, acc 0.046875\n",
      "step 2474, loss 14339.9, acc 0.046875\n",
      "step 2475, loss 14028.1, acc 0.0625\n",
      "step 2476, loss 12919.7, acc 0.0625\n",
      "step 2477, loss 20900.2, acc 0.0625\n",
      "step 2478, loss 13733.2, acc 0.125\n",
      "step 2479, loss 15901.9, acc 0.171875\n",
      "step 2480, loss 11838.9, acc 0.21875\n",
      "step 2481, loss 17345.2, acc 0.078125\n",
      "step 2482, loss 12708.4, acc 0.109375\n",
      "step 2483, loss 10465.2, acc 0\n",
      "step 2484, loss 12715.8, acc 0.125\n",
      "step 2485, loss 15119, acc 0.09375\n",
      "step 2486, loss 10031.7, acc 0.078125\n",
      "step 2487, loss 20612.7, acc 0.046875\n",
      "step 2488, loss 14116.2, acc 0.15625\n",
      "step 2489, loss 12971.7, acc 0.109375\n",
      "step 2490, loss 8851.33, acc 0.125\n",
      "step 2491, loss 18705.5, acc 0.078125\n",
      "step 2492, loss 19568.7, acc 0.09375\n",
      "step 2493, loss 11294.5, acc 0.03125\n",
      "step 2494, loss 11575, acc 0.078125\n",
      "step 2495, loss 14486.9, acc 0.15625\n",
      "step 2496, loss 8128.36, acc 0.214286\n",
      "step 2497, loss 13406.3, acc 0.09375\n",
      "step 2498, loss 14369.4, acc 0.078125\n",
      "step 2499, loss 11861.5, acc 0.0625\n",
      "step 2500, loss 11076.4, acc 0.125\n",
      "\n",
      "Evaluation:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2500, loss 212866, acc 0.163265, precison 0.0372280780444, recall 0.163265306122, f1_score 0.0568123890383.\n",
      "\n",
      "step 2501, loss 14746.7, acc 0.078125\n",
      "step 2502, loss 20310.7, acc 0.0625\n",
      "step 2503, loss 18198.7, acc 0.15625\n",
      "step 2504, loss 13170.5, acc 0.078125\n",
      "step 2505, loss 16548.2, acc 0.125\n",
      "step 2506, loss 15954.3, acc 0.046875\n",
      "step 2507, loss 13537.2, acc 0.09375\n",
      "step 2508, loss 9131.97, acc 0.140625\n",
      "step 2509, loss 8456, acc 0.142857\n",
      "step 2510, loss 15596.6, acc 0.09375\n",
      "step 2511, loss 13242.7, acc 0.0625\n",
      "step 2512, loss 14231.9, acc 0.078125\n",
      "step 2513, loss 13952, acc 0.09375\n",
      "step 2514, loss 11577.9, acc 0.09375\n",
      "step 2515, loss 14743.4, acc 0.078125\n",
      "step 2516, loss 14666.2, acc 0.109375\n",
      "step 2517, loss 13954.1, acc 0.0625\n",
      "step 2518, loss 19236.8, acc 0.109375\n",
      "step 2519, loss 14802.2, acc 0.046875\n",
      "step 2520, loss 14170.4, acc 0.046875\n",
      "step 2521, loss 15232.8, acc 0.0625\n",
      "step 2522, loss 16295.4, acc 0.214286\n",
      "step 2523, loss 17168.2, acc 0.0625\n",
      "step 2524, loss 10418.8, acc 0.015625\n",
      "step 2525, loss 10521.4, acc 0.125\n",
      "step 2526, loss 14516.3, acc 0.09375\n",
      "step 2527, loss 14464.6, acc 0.109375\n",
      "step 2528, loss 16584.9, acc 0.03125\n",
      "step 2529, loss 13348.3, acc 0.171875\n",
      "step 2530, loss 15274.8, acc 0.0625\n",
      "step 2531, loss 13590.8, acc 0.0625\n",
      "step 2532, loss 13839.9, acc 0.078125\n",
      "step 2533, loss 15705, acc 0.125\n",
      "step 2534, loss 20167.2, acc 0.09375\n",
      "step 2535, loss 10460, acc 0.214286\n",
      "step 2536, loss 17932.7, acc 0.0625\n",
      "step 2537, loss 16088.1, acc 0.109375\n",
      "step 2538, loss 13076.7, acc 0.09375\n",
      "step 2539, loss 12519.1, acc 0.0625\n",
      "step 2540, loss 15278, acc 0.140625\n",
      "step 2541, loss 11176.6, acc 0.0625\n",
      "step 2542, loss 13633.2, acc 0.078125\n",
      "step 2543, loss 14245.4, acc 0.0625\n",
      "step 2544, loss 15091.6, acc 0.046875\n",
      "step 2545, loss 16206.6, acc 0.078125\n",
      "step 2546, loss 12840.3, acc 0.125\n",
      "step 2547, loss 14794.3, acc 0.15625\n",
      "step 2548, loss 22371.1, acc 0.142857\n",
      "step 2549, loss 12771.3, acc 0.125\n",
      "step 2550, loss 23553.8, acc 0.03125\n",
      "step 2551, loss 15242.5, acc 0.0625\n",
      "step 2552, loss 13014.2, acc 0.0625\n",
      "step 2553, loss 14686.3, acc 0.125\n",
      "step 2554, loss 13370.2, acc 0.140625\n",
      "step 2555, loss 12651.4, acc 0.125\n",
      "step 2556, loss 19482.4, acc 0.125\n",
      "step 2557, loss 13859.8, acc 0.078125\n",
      "step 2558, loss 13271.5, acc 0.125\n",
      "step 2559, loss 13833.4, acc 0.109375\n",
      "step 2560, loss 13100, acc 0.09375\n",
      "step 2561, loss 25112.2, acc 0.0714286\n",
      "step 2562, loss 14326.2, acc 0.109375\n",
      "step 2563, loss 13796.2, acc 0.0625\n",
      "step 2564, loss 16454.2, acc 0.0625\n",
      "step 2565, loss 14235.6, acc 0.09375\n",
      "step 2566, loss 17010.6, acc 0.171875\n",
      "step 2567, loss 12613.4, acc 0.09375\n",
      "step 2568, loss 18294.3, acc 0.09375\n",
      "step 2569, loss 19603.7, acc 0.03125\n",
      "step 2570, loss 13144, acc 0.078125\n",
      "step 2571, loss 12946.1, acc 0.15625\n",
      "step 2572, loss 18183.3, acc 0.078125\n",
      "step 2573, loss 13026, acc 0.265625\n",
      "step 2574, loss 13508.8, acc 0\n",
      "step 2575, loss 16540.2, acc 0.0625\n",
      "step 2576, loss 13939.9, acc 0.046875\n",
      "step 2577, loss 13506.3, acc 0.078125\n",
      "step 2578, loss 20067, acc 0.0625\n",
      "step 2579, loss 13631.3, acc 0.21875\n",
      "step 2580, loss 13251.1, acc 0.09375\n",
      "step 2581, loss 14564.8, acc 0.046875\n",
      "step 2582, loss 14857.4, acc 0.0625\n",
      "step 2583, loss 17948.7, acc 0.125\n",
      "step 2584, loss 12323.9, acc 0.046875\n",
      "step 2585, loss 12712.9, acc 0.015625\n",
      "step 2586, loss 14550.6, acc 0.1875\n",
      "step 2587, loss 26047.7, acc 0.142857\n",
      "step 2588, loss 11210.3, acc 0.125\n",
      "step 2589, loss 13810.1, acc 0.078125\n",
      "step 2590, loss 15046.7, acc 0.078125\n",
      "step 2591, loss 16505.3, acc 0.078125\n",
      "step 2592, loss 19424.9, acc 0.0625\n",
      "step 2593, loss 17193.1, acc 0.171875\n",
      "step 2594, loss 14310.5, acc 0.171875\n",
      "step 2595, loss 12867.3, acc 0.078125\n",
      "step 2596, loss 15553.9, acc 0.03125\n",
      "step 2597, loss 16638.8, acc 0.109375\n",
      "step 2598, loss 15523.2, acc 0.078125\n",
      "step 2599, loss 16477, acc 0.09375\n",
      "step 2600, loss 13678.9, acc 0.0714286\n",
      "\n",
      "Evaluation:\n",
      "step 2600, loss 234503, acc 0.0255102, precison 0.000650770512287, recall 0.0255102040816, f1_score 0.00126916438217.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1203: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:1304: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n",
      "/usr/local/lib/python2.7/dist-packages/sklearn/metrics/classification.py:756: DeprecationWarning: The default `weighted` averaging is deprecated, and from version 0.18, use of precision, recall or F-score with multiclass or multilabel data or pos_label=None will result in an exception. Please set an explicit value for `average`, one of (None, 'micro', 'macro', 'weighted', 'samples'). In cross validation use, for instance, scoring=\"f1_weighted\" instead of scoring=\"f1\".\n",
      "  sample_weight=sample_weight)\n"
     ]
    }
   ],
   "source": [
    "for batch in batches:\n",
    "    x_batch, y_batch = zip(*batch)\n",
    "    train_step(x_batch, y_batch)\n",
    "    current_step = tf.train.global_step(sess, global_step)\n",
    "    if current_step % evaluate_every == 0:\n",
    "        print(\"\\nEvaluation:\")\n",
    "        dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "        print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "for epoch in xrange(150):\n",
    "        for i in xrange(total_batch):\n",
    "                train_step.run(feed_dict = {x: train_arrays, y: train_labels})\n",
    "                avg_cost += sess.run(cost, feed_dict={x: train_arrays, y: train_labels})/total_batch         \n",
    "        if epoch % display_step == 0:\n",
    "                print \"Epoch:\", '%04d' % (epoch+1), \"cost=\", \"{:.9f}\".format(avg_cost)\n",
    "\n",
    "#metrics\n",
    "y_p = tf.argmax(pred, 1)\n",
    "val_accuracy, y_pred = sess.run([accuracy, y_p], feed_dict={x:test_arrays, y:test_label})\n",
    "\n",
    "print \"validation accuracy:\", val_accuracy\n",
    "y_true = np.argmax(test_label,1)\n",
    "print \"Precision\", sk.metrics.precision_score(y_true, y_pred)\n",
    "print \"Recall\", sk.metrics.recall_score(y_true, y_pred)\n",
    "print \"f1_score\", sk.metrics.f1_score(y_true, y_pred)\n",
    "print \"confusion_matrix\"\n",
    "print sk.metrics.confusion_matrix(y_true, y_pred)\n",
    "fpr, tpr, tresholds = sk.metrics.roc_curve(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Normal Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batches = batch_iter(\n",
    "        list(zip(x_train, y_train)), batch_size, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016-09-11T09:20:38.721420: step 1, loss 9.62068, acc 0.078125\n",
      "2016-09-11T09:20:39.893012: step 2, loss 10.6179, acc 0.046875\n",
      "2016-09-11T09:20:40.907889: step 3, loss 8.47935, acc 0.046875\n",
      "2016-09-11T09:20:41.906054: step 4, loss 8.49473, acc 0.09375\n",
      "2016-09-11T09:20:42.918021: step 5, loss 7.4866, acc 0.09375\n",
      "2016-09-11T09:20:44.104501: step 6, loss 7.46709, acc 0.078125\n",
      "2016-09-11T09:20:45.104881: step 7, loss 8.63967, acc 0.09375\n",
      "2016-09-11T09:20:46.109960: step 8, loss 6.89962, acc 0.109375\n",
      "2016-09-11T09:20:47.260477: step 9, loss 7.13895, acc 0.1875\n",
      "2016-09-11T09:20:48.279823: step 10, loss 7.81613, acc 0.09375\n",
      "2016-09-11T09:20:49.291971: step 11, loss 8.04364, acc 0.171875\n",
      "2016-09-11T09:20:50.271658: step 12, loss 8.34044, acc 0.171875\n",
      "2016-09-11T09:20:50.506905: step 13, loss 8.10609, acc 0\n",
      "2016-09-11T09:20:51.515571: step 14, loss 7.09213, acc 0.15625\n",
      "2016-09-11T09:20:52.542369: step 15, loss 7.51027, acc 0.09375\n",
      "2016-09-11T09:20:53.516869: step 16, loss 7.71323, acc 0.171875\n",
      "2016-09-11T09:20:54.554849: step 17, loss 7.52269, acc 0.140625\n",
      "2016-09-11T09:20:55.532775: step 18, loss 7.15974, acc 0.1875\n",
      "2016-09-11T09:20:56.685142: step 19, loss 6.20365, acc 0.15625\n",
      "2016-09-11T09:20:57.716044: step 20, loss 6.80513, acc 0.15625\n",
      "2016-09-11T09:20:58.718652: step 21, loss 6.9493, acc 0.125\n",
      "2016-09-11T09:20:59.730163: step 22, loss 7.07196, acc 0.21875\n",
      "2016-09-11T09:21:00.729658: step 23, loss 7.98491, acc 0.09375\n",
      "2016-09-11T09:21:01.723895: step 24, loss 6.28133, acc 0.15625\n",
      "2016-09-11T09:21:02.884072: step 25, loss 6.44624, acc 0.078125\n",
      "2016-09-11T09:21:03.105825: step 26, loss 6.53422, acc 0.142857\n",
      "2016-09-11T09:21:04.260203: step 27, loss 6.69818, acc 0.09375\n",
      "2016-09-11T09:21:05.263413: step 28, loss 6.18831, acc 0.140625\n",
      "2016-09-11T09:21:06.335192: step 29, loss 6.29025, acc 0.09375\n",
      "2016-09-11T09:21:07.322044: step 30, loss 6.60724, acc 0.21875\n",
      "2016-09-11T09:21:08.475581: step 31, loss 6.85433, acc 0.109375\n",
      "2016-09-11T09:21:09.478079: step 32, loss 7.23248, acc 0.0625\n",
      "2016-09-11T09:21:10.500969: step 33, loss 6.6201, acc 0.203125\n",
      "2016-09-11T09:21:11.518500: step 34, loss 6.87936, acc 0.171875\n",
      "2016-09-11T09:21:12.549103: step 35, loss 6.11722, acc 0.1875\n",
      "2016-09-11T09:21:13.662280: step 36, loss 6.11835, acc 0.109375\n",
      "2016-09-11T09:21:14.659877: step 37, loss 7.39618, acc 0.078125\n",
      "2016-09-11T09:21:15.683269: step 38, loss 5.26861, acc 0.203125\n",
      "2016-09-11T09:21:15.892431: step 39, loss 6.40522, acc 0.142857\n",
      "2016-09-11T09:21:16.914564: step 40, loss 5.39225, acc 0.125\n",
      "2016-09-11T09:21:17.937258: step 41, loss 5.58472, acc 0.171875\n",
      "2016-09-11T09:21:18.923554: step 42, loss 7.45302, acc 0.140625\n",
      "2016-09-11T09:21:20.093489: step 43, loss 6.50349, acc 0.1875\n",
      "2016-09-11T09:21:21.071935: step 44, loss 6.1217, acc 0.109375\n",
      "2016-09-11T09:21:22.086245: step 45, loss 6.21439, acc 0.078125\n",
      "2016-09-11T09:21:23.096696: step 46, loss 6.52808, acc 0.15625\n",
      "2016-09-11T09:21:24.127812: step 47, loss 5.65008, acc 0.15625\n",
      "2016-09-11T09:21:25.290268: step 48, loss 5.39739, acc 0.21875\n",
      "2016-09-11T09:21:26.294735: step 49, loss 6.26269, acc 0.203125\n",
      "2016-09-11T09:21:27.288133: step 50, loss 7.02641, acc 0.140625\n",
      "2016-09-11T09:21:28.125677: step 51, loss 5.39953, acc 0.21875\n",
      "2016-09-11T09:21:28.473394: step 52, loss 4.11639, acc 0.214286\n",
      "2016-09-11T09:21:29.333979: step 53, loss 5.9874, acc 0.125\n",
      "2016-09-11T09:21:30.463105: step 54, loss 5.80638, acc 0.203125\n",
      "2016-09-11T09:21:31.515566: step 55, loss 5.51799, acc 0.234375\n",
      "2016-09-11T09:21:32.507694: step 56, loss 5.0462, acc 0.171875\n",
      "2016-09-11T09:21:33.512770: step 57, loss 7.24314, acc 0.1875\n",
      "2016-09-11T09:21:34.660380: step 58, loss 5.44917, acc 0.171875\n",
      "2016-09-11T09:21:35.663821: step 59, loss 5.25039, acc 0.171875\n",
      "2016-09-11T09:21:36.686801: step 60, loss 6.25646, acc 0.09375\n",
      "2016-09-11T09:21:37.559235: step 61, loss 5.24688, acc 0.125\n",
      "2016-09-11T09:21:38.526981: step 62, loss 5.39418, acc 0.21875\n",
      "2016-09-11T09:21:39.677591: step 63, loss 4.49914, acc 0.203125\n",
      "2016-09-11T09:21:40.680720: step 64, loss 6.65559, acc 0.1875\n",
      "2016-09-11T09:21:40.919613: step 65, loss 5.95694, acc 0.0714286\n",
      "2016-09-11T09:21:41.897576: step 66, loss 5.08792, acc 0.21875\n",
      "2016-09-11T09:21:42.903629: step 67, loss 5.45981, acc 0.171875\n",
      "2016-09-11T09:21:43.912629: step 68, loss 4.80452, acc 0.265625\n",
      "2016-09-11T09:21:44.916631: step 69, loss 6.24753, acc 0.203125\n",
      "2016-09-11T09:21:46.059544: step 70, loss 5.71089, acc 0.171875\n",
      "2016-09-11T09:21:47.095098: step 71, loss 6.24242, acc 0.25\n",
      "2016-09-11T09:21:48.095725: step 72, loss 6.50166, acc 0.171875\n",
      "2016-09-11T09:21:49.093978: step 73, loss 5.65652, acc 0.109375\n",
      "2016-09-11T09:21:50.146352: step 74, loss 5.98423, acc 0.203125\n",
      "2016-09-11T09:21:51.083044: step 75, loss 3.98886, acc 0.25\n",
      "2016-09-11T09:21:52.088457: step 76, loss 4.81327, acc 0.265625\n",
      "2016-09-11T09:21:53.106953: step 77, loss 6.07649, acc 0.109375\n",
      "2016-09-11T09:21:53.316005: step 78, loss 4.9633, acc 0.142857\n",
      "2016-09-11T09:21:54.326342: step 79, loss 5.85951, acc 0.1875\n",
      "2016-09-11T09:21:55.321609: step 80, loss 5.61915, acc 0.171875\n",
      "2016-09-11T09:21:56.481443: step 81, loss 5.01772, acc 0.203125\n",
      "2016-09-11T09:21:57.490257: step 82, loss 5.57972, acc 0.203125\n",
      "2016-09-11T09:21:58.503356: step 83, loss 5.09835, acc 0.203125\n",
      "2016-09-11T09:21:59.567903: step 84, loss 5.02686, acc 0.21875\n",
      "2016-09-11T09:22:00.682090: step 85, loss 5.1076, acc 0.203125\n",
      "2016-09-11T09:22:01.696353: step 86, loss 5.37875, acc 0.15625\n",
      "2016-09-11T09:22:02.711678: step 87, loss 5.95735, acc 0.171875\n",
      "2016-09-11T09:22:03.742085: step 88, loss 4.83729, acc 0.21875\n",
      "2016-09-11T09:22:04.877166: step 89, loss 4.74231, acc 0.359375\n",
      "2016-09-11T09:22:05.902451: step 90, loss 4.5079, acc 0.34375\n",
      "2016-09-11T09:22:06.115680: step 91, loss 3.73785, acc 0.214286\n",
      "2016-09-11T09:22:07.277168: step 92, loss 5.23707, acc 0.25\n",
      "2016-09-11T09:22:08.281514: step 93, loss 5.40867, acc 0.234375\n",
      "2016-09-11T09:22:09.292097: step 94, loss 4.43747, acc 0.25\n",
      "2016-09-11T09:22:10.313479: step 95, loss 4.6727, acc 0.21875\n",
      "2016-09-11T09:22:11.321467: step 96, loss 4.0431, acc 0.328125\n",
      "2016-09-11T09:22:12.293642: step 97, loss 6.01265, acc 0.25\n",
      "2016-09-11T09:22:13.321866: step 98, loss 6.26463, acc 0.140625\n",
      "2016-09-11T09:22:14.310633: step 99, loss 5.05926, acc 0.25\n",
      "2016-09-11T09:22:15.322668: step 100, loss 4.69054, acc 0.34375\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:22:16.320687: step 100, loss 5.60785, acc 0.158163\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-100\n",
      "\n",
      "2016-09-11T09:22:18.321319: step 101, loss 5.5419, acc 0.234375\n",
      "2016-09-11T09:22:19.330362: step 102, loss 5.22426, acc 0.203125\n",
      "2016-09-11T09:22:20.315318: step 103, loss 4.85781, acc 0.234375\n",
      "2016-09-11T09:22:20.567172: step 104, loss 5.98325, acc 0.142857\n",
      "2016-09-11T09:22:21.655464: step 105, loss 4.59135, acc 0.28125\n",
      "2016-09-11T09:22:22.672760: step 106, loss 5.50258, acc 0.1875\n",
      "2016-09-11T09:22:23.679637: step 107, loss 4.1019, acc 0.25\n",
      "2016-09-11T09:22:24.711549: step 108, loss 5.43937, acc 0.265625\n",
      "2016-09-11T09:22:25.702045: step 109, loss 5.79209, acc 0.171875\n",
      "2016-09-11T09:22:26.710668: step 110, loss 4.89199, acc 0.21875\n",
      "2016-09-11T09:22:27.890095: step 111, loss 4.99553, acc 0.203125\n",
      "2016-09-11T09:22:28.923022: step 112, loss 6.17449, acc 0.203125\n",
      "2016-09-11T09:22:30.130898: step 113, loss 5.19703, acc 0.28125\n",
      "2016-09-11T09:22:31.267364: step 114, loss 5.37539, acc 0.25\n",
      "2016-09-11T09:22:32.281336: step 115, loss 4.87584, acc 0.234375\n",
      "2016-09-11T09:22:33.287483: step 116, loss 5.20443, acc 0.265625\n",
      "2016-09-11T09:22:33.503679: step 117, loss 4.66607, acc 0.285714\n",
      "2016-09-11T09:22:34.689002: step 118, loss 5.02918, acc 0.25\n",
      "2016-09-11T09:22:35.707162: step 119, loss 5.49755, acc 0.25\n",
      "2016-09-11T09:22:36.856412: step 120, loss 4.59006, acc 0.234375\n",
      "2016-09-11T09:22:37.876544: step 121, loss 4.20405, acc 0.359375\n",
      "2016-09-11T09:22:38.887047: step 122, loss 5.12397, acc 0.203125\n",
      "2016-09-11T09:22:39.910624: step 123, loss 4.56911, acc 0.296875\n",
      "2016-09-11T09:22:40.980073: step 124, loss 5.30587, acc 0.203125\n",
      "2016-09-11T09:22:42.106915: step 125, loss 4.71574, acc 0.28125\n",
      "2016-09-11T09:22:43.096560: step 126, loss 5.50877, acc 0.203125\n",
      "2016-09-11T09:22:44.132750: step 127, loss 5.12818, acc 0.21875\n",
      "2016-09-11T09:22:45.147607: step 128, loss 4.92797, acc 0.234375\n",
      "2016-09-11T09:22:46.128098: step 129, loss 4.95172, acc 0.234375\n",
      "2016-09-11T09:22:46.458463: step 130, loss 6.6112, acc 0.214286\n",
      "2016-09-11T09:22:47.462242: step 131, loss 4.60627, acc 0.203125\n",
      "2016-09-11T09:22:48.489560: step 132, loss 3.82903, acc 0.3125\n",
      "2016-09-11T09:22:49.484779: step 133, loss 5.06735, acc 0.265625\n",
      "2016-09-11T09:22:50.502346: step 134, loss 4.72408, acc 0.359375\n",
      "2016-09-11T09:22:51.510111: step 135, loss 5.05552, acc 0.234375\n",
      "2016-09-11T09:22:52.540898: step 136, loss 5.75637, acc 0.1875\n",
      "2016-09-11T09:22:53.590640: step 137, loss 4.63832, acc 0.28125\n",
      "2016-09-11T09:22:54.527008: step 138, loss 5.0019, acc 0.234375\n",
      "2016-09-11T09:22:55.518090: step 139, loss 5.69517, acc 0.21875\n",
      "2016-09-11T09:22:56.536014: step 140, loss 4.79899, acc 0.21875\n",
      "2016-09-11T09:22:57.506647: step 141, loss 5.33114, acc 0.203125\n",
      "2016-09-11T09:22:58.511309: step 142, loss 4.98123, acc 0.203125\n",
      "2016-09-11T09:22:58.740703: step 143, loss 6.38412, acc 0.142857\n",
      "2016-09-11T09:22:59.748616: step 144, loss 4.59841, acc 0.328125\n",
      "2016-09-11T09:23:00.742592: step 145, loss 5.16242, acc 0.234375\n",
      "2016-09-11T09:23:01.869185: step 146, loss 4.65557, acc 0.328125\n",
      "2016-09-11T09:23:02.859887: step 147, loss 5.09904, acc 0.265625\n",
      "2016-09-11T09:23:03.860572: step 148, loss 4.68252, acc 0.3125\n",
      "2016-09-11T09:23:04.927354: step 149, loss 5.58933, acc 0.21875\n",
      "2016-09-11T09:23:06.101374: step 150, loss 5.69307, acc 0.3125\n",
      "2016-09-11T09:23:07.118361: step 151, loss 4.58244, acc 0.421875\n",
      "2016-09-11T09:23:08.118947: step 152, loss 5.69813, acc 0.15625\n",
      "2016-09-11T09:23:09.096734: step 153, loss 5.14183, acc 0.328125\n",
      "2016-09-11T09:23:10.103457: step 154, loss 4.41157, acc 0.3125\n",
      "2016-09-11T09:23:11.103208: step 155, loss 5.54188, acc 0.328125\n",
      "2016-09-11T09:23:11.324763: step 156, loss 5.32278, acc 0.0714286\n",
      "2016-09-11T09:23:12.370976: step 157, loss 5.48227, acc 0.203125\n",
      "2016-09-11T09:23:13.306293: step 158, loss 4.88881, acc 0.265625\n",
      "2016-09-11T09:23:14.300602: step 159, loss 5.83176, acc 0.28125\n",
      "2016-09-11T09:23:15.321902: step 160, loss 4.9412, acc 0.265625\n",
      "2016-09-11T09:23:16.474660: step 161, loss 5.14755, acc 0.234375\n",
      "2016-09-11T09:23:17.506208: step 162, loss 5.2498, acc 0.359375\n",
      "2016-09-11T09:23:18.522230: step 163, loss 5.96102, acc 0.328125\n",
      "2016-09-11T09:23:19.675124: step 164, loss 6.12889, acc 0.359375\n",
      "2016-09-11T09:23:20.688759: step 165, loss 5.55245, acc 0.21875\n",
      "2016-09-11T09:23:21.697065: step 166, loss 5.975, acc 0.203125\n",
      "2016-09-11T09:23:22.714253: step 167, loss 5.16783, acc 0.25\n",
      "2016-09-11T09:23:23.690721: step 168, loss 5.20764, acc 0.296875\n",
      "2016-09-11T09:23:23.937418: step 169, loss 5.66151, acc 0.357143\n",
      "2016-09-11T09:23:25.082963: step 170, loss 5.73855, acc 0.28125\n",
      "2016-09-11T09:23:26.065154: step 171, loss 5.11076, acc 0.234375\n",
      "2016-09-11T09:23:27.059768: step 172, loss 5.9345, acc 0.359375\n",
      "2016-09-11T09:23:28.081584: step 173, loss 5.85622, acc 0.25\n",
      "2016-09-11T09:23:29.094104: step 174, loss 6.174, acc 0.28125\n",
      "2016-09-11T09:23:30.100239: step 175, loss 5.99618, acc 0.171875\n",
      "2016-09-11T09:23:31.100257: step 176, loss 5.47692, acc 0.3125\n",
      "2016-09-11T09:23:32.130215: step 177, loss 4.83554, acc 0.203125\n",
      "2016-09-11T09:23:33.094595: step 178, loss 6.0865, acc 0.296875\n",
      "2016-09-11T09:23:34.100616: step 179, loss 5.50049, acc 0.25\n",
      "2016-09-11T09:23:35.103983: step 180, loss 5.18272, acc 0.296875\n",
      "2016-09-11T09:23:36.118261: step 181, loss 7.28761, acc 0.203125\n",
      "2016-09-11T09:23:36.303594: step 182, loss 4.03501, acc 0.285714\n",
      "2016-09-11T09:23:37.468889: step 183, loss 6.04471, acc 0.265625\n",
      "2016-09-11T09:23:38.477227: step 184, loss 5.02228, acc 0.171875\n",
      "2016-09-11T09:23:39.499059: step 185, loss 6.4055, acc 0.25\n",
      "2016-09-11T09:23:40.508214: step 186, loss 6.1861, acc 0.140625\n",
      "2016-09-11T09:23:41.525647: step 187, loss 7.01847, acc 0.28125\n",
      "2016-09-11T09:23:42.508102: step 188, loss 5.35803, acc 0.296875\n",
      "2016-09-11T09:23:43.521676: step 189, loss 6.63787, acc 0.25\n",
      "2016-09-11T09:23:44.516680: step 190, loss 7.19183, acc 0.21875\n",
      "2016-09-11T09:23:45.666615: step 191, loss 7.3291, acc 0.296875\n",
      "2016-09-11T09:23:46.692487: step 192, loss 7.36669, acc 0.171875\n",
      "2016-09-11T09:23:47.679870: step 193, loss 6.83461, acc 0.265625\n",
      "2016-09-11T09:23:48.694729: step 194, loss 7.17334, acc 0.203125\n",
      "2016-09-11T09:23:48.911439: step 195, loss 4.67495, acc 0.214286\n",
      "2016-09-11T09:23:49.926732: step 196, loss 9.23957, acc 0.28125\n",
      "2016-09-11T09:23:50.906879: step 197, loss 5.41018, acc 0.25\n",
      "2016-09-11T09:23:51.931150: step 198, loss 6.18535, acc 0.234375\n",
      "2016-09-11T09:23:52.953683: step 199, loss 5.50496, acc 0.390625\n",
      "2016-09-11T09:23:54.060552: step 200, loss 5.65823, acc 0.203125\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:23:55.085490: step 200, loss 8.52447, acc 0.147959\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-200\n",
      "\n",
      "2016-09-11T09:23:57.057281: step 201, loss 6.89457, acc 0.203125\n",
      "2016-09-11T09:23:58.061824: step 202, loss 7.00187, acc 0.15625\n",
      "2016-09-11T09:23:59.075103: step 203, loss 6.29687, acc 0.359375\n",
      "2016-09-11T09:24:00.076845: step 204, loss 8.36743, acc 0.1875\n",
      "2016-09-11T09:24:01.094991: step 205, loss 7.29019, acc 0.234375\n",
      "2016-09-11T09:24:02.093350: step 206, loss 7.24807, acc 0.21875\n",
      "2016-09-11T09:24:03.139441: step 207, loss 9.25976, acc 0.203125\n",
      "2016-09-11T09:24:03.349299: step 208, loss 9.64823, acc 0.214286\n",
      "2016-09-11T09:24:04.463995: step 209, loss 5.86087, acc 0.21875\n",
      "2016-09-11T09:24:05.481731: step 210, loss 7.95073, acc 0.265625\n",
      "2016-09-11T09:24:06.492288: step 211, loss 7.26356, acc 0.328125\n",
      "2016-09-11T09:24:07.502159: step 212, loss 6.26318, acc 0.265625\n",
      "2016-09-11T09:24:08.530954: step 213, loss 9.71927, acc 0.25\n",
      "2016-09-11T09:24:09.669557: step 214, loss 6.60394, acc 0.203125\n",
      "2016-09-11T09:24:10.681302: step 215, loss 6.26939, acc 0.234375\n",
      "2016-09-11T09:24:11.707362: step 216, loss 8.33968, acc 0.21875\n",
      "2016-09-11T09:24:12.859234: step 217, loss 9.08308, acc 0.171875\n",
      "2016-09-11T09:24:13.855421: step 218, loss 7.75087, acc 0.25\n",
      "2016-09-11T09:24:14.727573: step 219, loss 7.62009, acc 0.21875\n",
      "2016-09-11T09:24:15.872522: step 220, loss 9.34257, acc 0.1875\n",
      "2016-09-11T09:24:16.073032: step 221, loss 12.3327, acc 0.142857\n",
      "2016-09-11T09:24:17.088515: step 222, loss 9.71952, acc 0.21875\n",
      "2016-09-11T09:24:18.088010: step 223, loss 7.75432, acc 0.265625\n",
      "2016-09-11T09:24:19.260695: step 224, loss 10.0381, acc 0.171875\n",
      "2016-09-11T09:24:20.277541: step 225, loss 8.07149, acc 0.359375\n",
      "2016-09-11T09:24:21.275380: step 226, loss 9.21915, acc 0.265625\n",
      "2016-09-11T09:24:22.310832: step 227, loss 8.2795, acc 0.265625\n",
      "2016-09-11T09:24:23.463595: step 228, loss 9.72363, acc 0.234375\n",
      "2016-09-11T09:24:24.488059: step 229, loss 9.6339, acc 0.1875\n",
      "2016-09-11T09:24:25.541585: step 230, loss 7.60968, acc 0.1875\n",
      "2016-09-11T09:24:26.552084: step 231, loss 9.47134, acc 0.234375\n",
      "2016-09-11T09:24:27.541449: step 232, loss 8.30633, acc 0.3125\n",
      "2016-09-11T09:24:28.648551: step 233, loss 6.94745, acc 0.21875\n",
      "2016-09-11T09:24:28.881234: step 234, loss 5.77127, acc 0.285714\n",
      "2016-09-11T09:24:29.906379: step 235, loss 9.67106, acc 0.28125\n",
      "2016-09-11T09:24:30.920487: step 236, loss 11.4625, acc 0.203125\n",
      "2016-09-11T09:24:31.942827: step 237, loss 12.3281, acc 0.234375\n",
      "2016-09-11T09:24:32.924099: step 238, loss 11.5328, acc 0.296875\n",
      "2016-09-11T09:24:33.998782: step 239, loss 11.8359, acc 0.140625\n",
      "2016-09-11T09:24:35.062381: step 240, loss 10.6075, acc 0.265625\n",
      "2016-09-11T09:24:36.066611: step 241, loss 11.6282, acc 0.234375\n",
      "2016-09-11T09:24:37.082132: step 242, loss 11.8329, acc 0.21875\n",
      "2016-09-11T09:24:38.313400: step 243, loss 11.5148, acc 0.21875\n",
      "2016-09-11T09:24:39.304332: step 244, loss 12.8663, acc 0.15625\n",
      "2016-09-11T09:24:40.283966: step 245, loss 11.1381, acc 0.203125\n",
      "2016-09-11T09:24:41.476879: step 246, loss 12.8765, acc 0.296875\n",
      "2016-09-11T09:24:41.869913: step 247, loss 5.27881, acc 0.285714\n",
      "2016-09-11T09:24:42.886841: step 248, loss 10.9019, acc 0.21875\n",
      "2016-09-11T09:24:43.891326: step 249, loss 12.0106, acc 0.1875\n",
      "2016-09-11T09:24:44.893529: step 250, loss 15.1032, acc 0.1875\n",
      "2016-09-11T09:24:45.890378: step 251, loss 11.9641, acc 0.15625\n",
      "2016-09-11T09:24:47.075693: step 252, loss 9.15978, acc 0.265625\n",
      "2016-09-11T09:24:48.089407: step 253, loss 16.966, acc 0.203125\n",
      "2016-09-11T09:24:49.274838: step 254, loss 20.4952, acc 0.125\n",
      "2016-09-11T09:24:50.286287: step 255, loss 13.6533, acc 0.1875\n",
      "2016-09-11T09:24:51.314961: step 256, loss 15.4941, acc 0.1875\n",
      "2016-09-11T09:24:52.526622: step 257, loss 13.2803, acc 0.140625\n",
      "2016-09-11T09:24:53.502372: step 258, loss 15.8726, acc 0.140625\n",
      "2016-09-11T09:24:54.511721: step 259, loss 13.1584, acc 0.1875\n",
      "2016-09-11T09:24:54.860692: step 260, loss 17.8458, acc 0.285714\n",
      "2016-09-11T09:24:55.883484: step 261, loss 20.7807, acc 0.171875\n",
      "2016-09-11T09:24:56.928976: step 262, loss 16.9276, acc 0.203125\n",
      "2016-09-11T09:24:57.904866: step 263, loss 21.1933, acc 0.25\n",
      "2016-09-11T09:24:58.915023: step 264, loss 21.4467, acc 0.15625\n",
      "2016-09-11T09:24:59.910937: step 265, loss 15.8927, acc 0.21875\n",
      "2016-09-11T09:25:00.902976: step 266, loss 17.4204, acc 0.203125\n",
      "2016-09-11T09:25:01.949930: step 267, loss 15.4379, acc 0.109375\n",
      "2016-09-11T09:25:02.898414: step 268, loss 22.1597, acc 0.1875\n",
      "2016-09-11T09:25:03.914320: step 269, loss 21.4438, acc 0.21875\n",
      "2016-09-11T09:25:05.110800: step 270, loss 16.834, acc 0.203125\n",
      "2016-09-11T09:25:06.142670: step 271, loss 19.8729, acc 0.171875\n",
      "2016-09-11T09:25:07.317104: step 272, loss 20.3371, acc 0.171875\n",
      "2016-09-11T09:25:07.520340: step 273, loss 20.5631, acc 0.357143\n",
      "2016-09-11T09:25:08.671970: step 274, loss 24.3363, acc 0.28125\n",
      "2016-09-11T09:25:09.689392: step 275, loss 27.7852, acc 0.265625\n",
      "2016-09-11T09:25:10.723671: step 276, loss 18.0963, acc 0.140625\n",
      "2016-09-11T09:25:11.864761: step 277, loss 26.3391, acc 0.203125\n",
      "2016-09-11T09:25:12.883508: step 278, loss 30.245, acc 0.140625\n",
      "2016-09-11T09:25:13.893614: step 279, loss 31.6621, acc 0.0625\n",
      "2016-09-11T09:25:14.898272: step 280, loss 22.835, acc 0.140625\n",
      "2016-09-11T09:25:15.917355: step 281, loss 22.6671, acc 0.125\n",
      "2016-09-11T09:25:16.916032: step 282, loss 30.012, acc 0.140625\n",
      "2016-09-11T09:25:17.916520: step 283, loss 23.0472, acc 0.1875\n",
      "2016-09-11T09:25:18.962198: step 284, loss 27.0013, acc 0.171875\n",
      "2016-09-11T09:25:19.898801: step 285, loss 33.2874, acc 0.09375\n",
      "2016-09-11T09:25:20.110577: step 286, loss 27.4524, acc 0\n",
      "2016-09-11T09:25:21.122614: step 287, loss 31.3256, acc 0.15625\n",
      "2016-09-11T09:25:22.114528: step 288, loss 28.1541, acc 0.109375\n",
      "2016-09-11T09:25:23.259213: step 289, loss 37.4386, acc 0.09375\n",
      "2016-09-11T09:25:24.302332: step 290, loss 29.4775, acc 0.1875\n",
      "2016-09-11T09:25:25.284459: step 291, loss 31.843, acc 0.09375\n",
      "2016-09-11T09:25:26.301716: step 292, loss 44.1377, acc 0.234375\n",
      "2016-09-11T09:25:27.287849: step 293, loss 39.893, acc 0.1875\n",
      "2016-09-11T09:25:28.307270: step 294, loss 39.4763, acc 0.15625\n",
      "2016-09-11T09:25:29.486632: step 295, loss 51.5132, acc 0.125\n",
      "2016-09-11T09:25:30.502356: step 296, loss 49.2364, acc 0.109375\n",
      "2016-09-11T09:25:31.492145: step 297, loss 36.341, acc 0.09375\n",
      "2016-09-11T09:25:32.500162: step 298, loss 35.9614, acc 0.15625\n",
      "2016-09-11T09:25:32.690043: step 299, loss 40.052, acc 0.214286\n",
      "2016-09-11T09:25:33.728303: step 300, loss 41.1394, acc 0.078125\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:25:34.694139: step 300, loss 62.1028, acc 0.117347\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-300\n",
      "\n",
      "2016-09-11T09:25:36.307400: step 301, loss 50.7455, acc 0.078125\n",
      "2016-09-11T09:25:37.263646: step 302, loss 45.7142, acc 0.15625\n",
      "2016-09-11T09:25:38.301373: step 303, loss 44.6232, acc 0.171875\n",
      "2016-09-11T09:25:39.313622: step 304, loss 67.9659, acc 0.140625\n",
      "2016-09-11T09:25:40.316437: step 305, loss 48.7891, acc 0.140625\n",
      "2016-09-11T09:25:41.455231: step 306, loss 67.8174, acc 0.078125\n",
      "2016-09-11T09:25:42.477225: step 307, loss 45.5364, acc 0.125\n",
      "2016-09-11T09:25:43.499918: step 308, loss 56.0344, acc 0.078125\n",
      "2016-09-11T09:25:44.484679: step 309, loss 59.7739, acc 0.125\n",
      "2016-09-11T09:25:45.496663: step 310, loss 82.7983, acc 0.109375\n",
      "2016-09-11T09:25:46.544728: step 311, loss 54.3835, acc 0.09375\n",
      "2016-09-11T09:25:46.891184: step 312, loss 51.1885, acc 0\n",
      "2016-09-11T09:25:47.949225: step 313, loss 57.7451, acc 0.09375\n",
      "2016-09-11T09:25:48.906416: step 314, loss 71.5091, acc 0.109375\n",
      "2016-09-11T09:25:50.080667: step 315, loss 59.032, acc 0.125\n",
      "2016-09-11T09:25:51.085829: step 316, loss 58.8046, acc 0.109375\n",
      "2016-09-11T09:25:52.126796: step 317, loss 97.3953, acc 0.125\n",
      "2016-09-11T09:25:53.323357: step 318, loss 81.5506, acc 0.109375\n",
      "2016-09-11T09:25:54.292300: step 319, loss 81.7972, acc 0.078125\n",
      "2016-09-11T09:25:55.320214: step 320, loss 69.7881, acc 0.125\n",
      "2016-09-11T09:25:56.324871: step 321, loss 78.9665, acc 0.09375\n",
      "2016-09-11T09:25:57.371848: step 322, loss 68.8952, acc 0.09375\n",
      "2016-09-11T09:25:58.313060: step 323, loss 90.719, acc 0.125\n",
      "2016-09-11T09:25:59.454081: step 324, loss 59.1556, acc 0.078125\n",
      "2016-09-11T09:25:59.686404: step 325, loss 128.389, acc 0\n",
      "2016-09-11T09:26:00.685866: step 326, loss 62.8246, acc 0.140625\n",
      "2016-09-11T09:26:01.695811: step 327, loss 75.6389, acc 0.203125\n",
      "2016-09-11T09:26:02.723704: step 328, loss 74.7351, acc 0.1875\n",
      "2016-09-11T09:26:03.865057: step 329, loss 79.1843, acc 0.1875\n",
      "2016-09-11T09:26:04.920886: step 330, loss 132.523, acc 0.09375\n",
      "2016-09-11T09:26:05.900243: step 331, loss 98.248, acc 0.109375\n",
      "2016-09-11T09:26:06.904369: step 332, loss 96.8136, acc 0.125\n",
      "2016-09-11T09:26:07.908156: step 333, loss 81.7596, acc 0.078125\n",
      "2016-09-11T09:26:08.925142: step 334, loss 98.8647, acc 0.046875\n",
      "2016-09-11T09:26:09.909485: step 335, loss 121.491, acc 0.078125\n",
      "2016-09-11T09:26:11.005404: step 336, loss 154.27, acc 0.078125\n",
      "2016-09-11T09:26:12.070051: step 337, loss 137.7, acc 0.078125\n",
      "2016-09-11T09:26:12.315960: step 338, loss 45.1282, acc 0.142857\n",
      "2016-09-11T09:26:13.287452: step 339, loss 88.8432, acc 0.125\n",
      "2016-09-11T09:26:14.269858: step 340, loss 110.856, acc 0.125\n",
      "2016-09-11T09:26:15.281394: step 341, loss 90.1407, acc 0.125\n",
      "2016-09-11T09:26:16.357833: step 342, loss 115.49, acc 0.109375\n",
      "2016-09-11T09:26:17.343120: step 343, loss 149.628, acc 0.0625\n",
      "2016-09-11T09:26:18.341021: step 344, loss 139.382, acc 0.0625\n",
      "2016-09-11T09:26:19.430082: step 345, loss 127.423, acc 0.078125\n",
      "2016-09-11T09:26:20.455323: step 346, loss 134.654, acc 0.0625\n",
      "2016-09-11T09:26:21.501873: step 347, loss 164.337, acc 0.046875\n",
      "2016-09-11T09:26:22.503036: step 348, loss 151.327, acc 0.046875\n",
      "2016-09-11T09:26:23.509151: step 349, loss 147.227, acc 0.203125\n",
      "2016-09-11T09:26:24.547370: step 350, loss 152.797, acc 0.125\n",
      "2016-09-11T09:26:24.754398: step 351, loss 125.955, acc 0.357143\n",
      "2016-09-11T09:26:25.744052: step 352, loss 139.488, acc 0.140625\n",
      "2016-09-11T09:26:26.925392: step 353, loss 125.724, acc 0.125\n",
      "2016-09-11T09:26:27.903513: step 354, loss 160.595, acc 0.109375\n",
      "2016-09-11T09:26:28.928205: step 355, loss 145.005, acc 0.078125\n",
      "2016-09-11T09:26:29.990394: step 356, loss 165.516, acc 0.140625\n",
      "2016-09-11T09:26:31.063812: step 357, loss 188.718, acc 0.078125\n",
      "2016-09-11T09:26:32.092533: step 358, loss 180.62, acc 0.09375\n",
      "2016-09-11T09:26:33.169231: step 359, loss 210.254, acc 0.078125\n",
      "2016-09-11T09:26:34.114480: step 360, loss 185.221, acc 0.078125\n",
      "2016-09-11T09:26:35.095369: step 361, loss 172.937, acc 0.09375\n",
      "2016-09-11T09:26:36.135876: step 362, loss 168.451, acc 0.0625\n",
      "2016-09-11T09:26:37.180690: step 363, loss 185.54, acc 0.046875\n",
      "2016-09-11T09:26:37.494618: step 364, loss 253.267, acc 0\n",
      "2016-09-11T09:26:38.465447: step 365, loss 215.614, acc 0.015625\n",
      "2016-09-11T09:26:39.461220: step 366, loss 186.732, acc 0.109375\n",
      "2016-09-11T09:26:40.484675: step 367, loss 182.274, acc 0.125\n",
      "2016-09-11T09:26:41.484940: step 368, loss 215.949, acc 0.09375\n",
      "2016-09-11T09:26:42.510729: step 369, loss 217.521, acc 0.09375\n",
      "2016-09-11T09:26:43.529546: step 370, loss 161.201, acc 0.234375\n",
      "2016-09-11T09:26:44.523169: step 371, loss 227.369, acc 0.09375\n",
      "2016-09-11T09:26:45.534017: step 372, loss 287.919, acc 0.09375\n",
      "2016-09-11T09:26:46.655108: step 373, loss 242.863, acc 0.078125\n",
      "2016-09-11T09:26:47.659284: step 374, loss 214.881, acc 0.078125\n",
      "2016-09-11T09:26:48.695582: step 375, loss 237.94, acc 0.078125\n",
      "2016-09-11T09:26:49.713551: step 376, loss 281.194, acc 0.09375\n",
      "2016-09-11T09:26:49.922176: step 377, loss 231.378, acc 0.142857\n",
      "2016-09-11T09:26:50.912340: step 378, loss 294.831, acc 0.125\n",
      "2016-09-11T09:26:51.919136: step 379, loss 258.848, acc 0.125\n",
      "2016-09-11T09:26:52.899217: step 380, loss 287.156, acc 0.03125\n",
      "2016-09-11T09:26:53.922955: step 381, loss 219.36, acc 0.046875\n",
      "2016-09-11T09:26:54.913990: step 382, loss 192.166, acc 0.078125\n",
      "2016-09-11T09:26:56.087892: step 383, loss 245.371, acc 0.125\n",
      "2016-09-11T09:26:57.099548: step 384, loss 330.283, acc 0.09375\n",
      "2016-09-11T09:26:58.136656: step 385, loss 254.255, acc 0.171875\n",
      "2016-09-11T09:26:59.103248: step 386, loss 312.16, acc 0.0625\n",
      "2016-09-11T09:27:00.128365: step 387, loss 274.362, acc 0.125\n",
      "2016-09-11T09:27:01.130291: step 388, loss 357.792, acc 0.046875\n",
      "2016-09-11T09:27:02.167479: step 389, loss 232.668, acc 0.140625\n",
      "2016-09-11T09:27:02.321740: step 390, loss 152.111, acc 0.0714286\n",
      "2016-09-11T09:27:03.366747: step 391, loss 241.055, acc 0.109375\n",
      "2016-09-11T09:27:04.308772: step 392, loss 320.939, acc 0.109375\n",
      "2016-09-11T09:27:05.469428: step 393, loss 371.364, acc 0.0625\n",
      "2016-09-11T09:27:06.509349: step 394, loss 391.695, acc 0.0625\n",
      "2016-09-11T09:27:07.487162: step 395, loss 246.801, acc 0.109375\n",
      "2016-09-11T09:27:08.504715: step 396, loss 265.224, acc 0.0625\n",
      "2016-09-11T09:27:09.529080: step 397, loss 398.283, acc 0.125\n",
      "2016-09-11T09:27:10.524541: step 398, loss 308.861, acc 0.109375\n",
      "2016-09-11T09:27:11.550089: step 399, loss 232.409, acc 0.125\n",
      "2016-09-11T09:27:12.523403: step 400, loss 284.079, acc 0.078125\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:27:13.504730: step 400, loss 501.373, acc 0.183673\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-400\n",
      "\n",
      "2016-09-11T09:27:15.524149: step 401, loss 366.323, acc 0.15625\n",
      "2016-09-11T09:27:16.567611: step 402, loss 276.124, acc 0.15625\n",
      "2016-09-11T09:27:16.866915: step 403, loss 595.102, acc 0.142857\n",
      "2016-09-11T09:27:17.890521: step 404, loss 262.843, acc 0.09375\n",
      "2016-09-11T09:27:18.973279: step 405, loss 309.876, acc 0.09375\n",
      "2016-09-11T09:27:20.077611: step 406, loss 245.583, acc 0.15625\n",
      "2016-09-11T09:27:21.120886: step 407, loss 266.037, acc 0.078125\n",
      "2016-09-11T09:27:22.355114: step 408, loss 275.545, acc 0.078125\n",
      "2016-09-11T09:27:23.463164: step 409, loss 358.923, acc 0.140625\n",
      "2016-09-11T09:27:24.559511: step 410, loss 432.738, acc 0.1875\n",
      "2016-09-11T09:27:25.634805: step 411, loss 414.506, acc 0.0625\n",
      "2016-09-11T09:27:27.200695: step 412, loss 404.266, acc 0.109375\n",
      "2016-09-11T09:27:28.573591: step 413, loss 544.247, acc 0.09375\n",
      "2016-09-11T09:27:29.904104: step 414, loss 354.848, acc 0.09375\n",
      "2016-09-11T09:27:31.168888: step 415, loss 333.983, acc 0.0625\n",
      "2016-09-11T09:27:31.577942: step 416, loss 253.788, acc 0.142857\n",
      "2016-09-11T09:27:33.009250: step 417, loss 341.192, acc 0.109375\n",
      "2016-09-11T09:27:34.089811: step 418, loss 283.897, acc 0.03125\n",
      "2016-09-11T09:27:35.142626: step 419, loss 343.603, acc 0.078125\n",
      "2016-09-11T09:27:36.148588: step 420, loss 393.876, acc 0.078125\n",
      "2016-09-11T09:27:37.190287: step 421, loss 383.458, acc 0.078125\n",
      "2016-09-11T09:27:38.204607: step 422, loss 342.38, acc 0.125\n",
      "2016-09-11T09:27:39.256767: step 423, loss 435.276, acc 0.125\n",
      "2016-09-11T09:27:40.269934: step 424, loss 463.082, acc 0.109375\n",
      "2016-09-11T09:27:41.261884: step 425, loss 437.313, acc 0.09375\n",
      "2016-09-11T09:27:42.260869: step 426, loss 452.615, acc 0.09375\n",
      "2016-09-11T09:27:43.263835: step 427, loss 450.402, acc 0.046875\n",
      "2016-09-11T09:27:44.302557: step 428, loss 414.196, acc 0.0625\n",
      "2016-09-11T09:27:44.606365: step 429, loss 471.791, acc 0.0714286\n",
      "2016-09-11T09:27:45.526660: step 430, loss 498.937, acc 0.078125\n",
      "2016-09-11T09:27:46.568960: step 431, loss 400.459, acc 0.09375\n",
      "2016-09-11T09:27:47.552211: step 432, loss 424.162, acc 0.03125\n",
      "2016-09-11T09:27:48.542581: step 433, loss 570.964, acc 0.0625\n",
      "2016-09-11T09:27:49.583847: step 434, loss 414.585, acc 0.140625\n",
      "2016-09-11T09:27:50.666616: step 435, loss 341.123, acc 0.21875\n",
      "2016-09-11T09:27:51.561737: step 436, loss 471.243, acc 0.09375\n",
      "2016-09-11T09:27:52.662569: step 437, loss 517.662, acc 0.125\n",
      "2016-09-11T09:27:53.676432: step 438, loss 346.615, acc 0.109375\n",
      "2016-09-11T09:27:54.730655: step 439, loss 420.581, acc 0.078125\n",
      "2016-09-11T09:27:55.697470: step 440, loss 405.317, acc 0.078125\n",
      "2016-09-11T09:27:56.773146: step 441, loss 464.616, acc 0.15625\n",
      "2016-09-11T09:27:56.991497: step 442, loss 371.005, acc 0\n",
      "2016-09-11T09:27:58.134632: step 443, loss 493.323, acc 0.0625\n",
      "2016-09-11T09:27:59.101776: step 444, loss 583.391, acc 0.125\n",
      "2016-09-11T09:28:00.145249: step 445, loss 459.579, acc 0.0625\n",
      "2016-09-11T09:28:01.134536: step 446, loss 443.737, acc 0.0625\n",
      "2016-09-11T09:28:02.454504: step 447, loss 613.592, acc 0.15625\n",
      "2016-09-11T09:28:03.363038: step 448, loss 437.317, acc 0.15625\n",
      "2016-09-11T09:28:04.336282: step 449, loss 573.635, acc 0.078125\n",
      "2016-09-11T09:28:05.508129: step 450, loss 457.888, acc 0.109375\n",
      "2016-09-11T09:28:06.491687: step 451, loss 755.457, acc 0.09375\n",
      "2016-09-11T09:28:07.513371: step 452, loss 579.518, acc 0.0625\n",
      "2016-09-11T09:28:08.505428: step 453, loss 498.074, acc 0.09375\n",
      "2016-09-11T09:28:09.547236: step 454, loss 515.77, acc 0.0625\n",
      "2016-09-11T09:28:09.874020: step 455, loss 320.803, acc 0.142857\n",
      "2016-09-11T09:28:10.873188: step 456, loss 594.406, acc 0.0625\n",
      "2016-09-11T09:28:11.874588: step 457, loss 592.869, acc 0.109375\n",
      "2016-09-11T09:28:12.870132: step 458, loss 601.627, acc 0.125\n",
      "2016-09-11T09:28:13.924693: step 459, loss 408.442, acc 0.125\n",
      "2016-09-11T09:28:14.943723: step 460, loss 556.396, acc 0.09375\n",
      "2016-09-11T09:28:15.977066: step 461, loss 760.124, acc 0.15625\n",
      "2016-09-11T09:28:17.073375: step 462, loss 645.457, acc 0.15625\n",
      "2016-09-11T09:28:18.090282: step 463, loss 525.236, acc 0.0625\n",
      "2016-09-11T09:28:19.090618: step 464, loss 597.726, acc 0.15625\n",
      "2016-09-11T09:28:20.082020: step 465, loss 650.89, acc 0.109375\n",
      "2016-09-11T09:28:21.077020: step 466, loss 713.297, acc 0.140625\n",
      "2016-09-11T09:28:22.123436: step 467, loss 588.774, acc 0.09375\n",
      "2016-09-11T09:28:22.374928: step 468, loss 650.111, acc 0\n",
      "2016-09-11T09:28:23.420847: step 469, loss 757.735, acc 0.078125\n",
      "2016-09-11T09:28:24.471829: step 470, loss 660.728, acc 0.078125\n",
      "2016-09-11T09:28:25.481266: step 471, loss 569.661, acc 0.078125\n",
      "2016-09-11T09:28:26.490329: step 472, loss 546.203, acc 0.109375\n",
      "2016-09-11T09:28:27.502722: step 473, loss 657.74, acc 0.1875\n",
      "2016-09-11T09:28:28.537933: step 474, loss 794.401, acc 0.046875\n",
      "2016-09-11T09:28:29.517985: step 475, loss 608.408, acc 0.09375\n",
      "2016-09-11T09:28:30.534016: step 476, loss 498.182, acc 0.15625\n",
      "2016-09-11T09:28:31.522436: step 477, loss 996.475, acc 0.0625\n",
      "2016-09-11T09:28:32.516589: step 478, loss 962.13, acc 0.109375\n",
      "2016-09-11T09:28:33.537427: step 479, loss 640.779, acc 0.09375\n",
      "2016-09-11T09:28:34.502094: step 480, loss 511.144, acc 0.109375\n",
      "2016-09-11T09:28:34.709964: step 481, loss 840.468, acc 0.0714286\n",
      "2016-09-11T09:28:35.726045: step 482, loss 748.732, acc 0.078125\n",
      "2016-09-11T09:28:36.817910: step 483, loss 746.587, acc 0.09375\n",
      "2016-09-11T09:28:37.717533: step 484, loss 696.193, acc 0.0625\n",
      "2016-09-11T09:28:38.721908: step 485, loss 724.046, acc 0.109375\n",
      "2016-09-11T09:28:39.740889: step 486, loss 650.872, acc 0.078125\n",
      "2016-09-11T09:28:40.736993: step 487, loss 774.611, acc 0.0625\n",
      "2016-09-11T09:28:41.710051: step 488, loss 886.951, acc 0.109375\n",
      "2016-09-11T09:28:42.739754: step 489, loss 906.465, acc 0.1875\n",
      "2016-09-11T09:28:43.790852: step 490, loss 638.196, acc 0.21875\n",
      "2016-09-11T09:28:44.794019: step 491, loss 796.197, acc 0.171875\n",
      "2016-09-11T09:28:45.993252: step 492, loss 869.742, acc 0.140625\n",
      "2016-09-11T09:28:47.368558: step 493, loss 723.843, acc 0.171875\n",
      "2016-09-11T09:28:48.137260: step 494, loss 578.75, acc 0.142857\n",
      "2016-09-11T09:28:49.619507: step 495, loss 609.062, acc 0.09375\n",
      "2016-09-11T09:28:50.520210: step 496, loss 817.389, acc 0.03125\n",
      "2016-09-11T09:28:51.515211: step 497, loss 741.497, acc 0.078125\n",
      "2016-09-11T09:28:52.668258: step 498, loss 867.095, acc 0.09375\n",
      "2016-09-11T09:28:53.771046: step 499, loss 803.744, acc 0.125\n",
      "2016-09-11T09:28:54.739109: step 500, loss 801.838, acc 0.109375\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:28:55.717927: step 500, loss 1746.14, acc 0.112245\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-500\n",
      "\n",
      "2016-09-11T09:28:57.954255: step 501, loss 810.736, acc 0.078125\n",
      "2016-09-11T09:28:58.998912: step 502, loss 939.702, acc 0.125\n",
      "2016-09-11T09:28:59.917128: step 503, loss 1161.1, acc 0.03125\n",
      "2016-09-11T09:29:01.081104: step 504, loss 872.004, acc 0.140625\n",
      "2016-09-11T09:29:02.060557: step 505, loss 977.516, acc 0.03125\n",
      "2016-09-11T09:29:03.064609: step 506, loss 946.691, acc 0.03125\n",
      "2016-09-11T09:29:03.289595: step 507, loss 401.687, acc 0.142857\n",
      "2016-09-11T09:29:04.290467: step 508, loss 649.584, acc 0.078125\n",
      "2016-09-11T09:29:05.285805: step 509, loss 826.195, acc 0.046875\n",
      "2016-09-11T09:29:06.295844: step 510, loss 989.607, acc 0.125\n",
      "2016-09-11T09:29:07.282951: step 511, loss 717.631, acc 0.109375\n",
      "2016-09-11T09:29:08.286158: step 512, loss 826.515, acc 0.0625\n",
      "2016-09-11T09:29:09.291409: step 513, loss 959.725, acc 0.078125\n",
      "2016-09-11T09:29:10.301200: step 514, loss 1042.44, acc 0.078125\n",
      "2016-09-11T09:29:11.485885: step 515, loss 780.165, acc 0.0625\n",
      "2016-09-11T09:29:12.490756: step 516, loss 956.345, acc 0.046875\n",
      "2016-09-11T09:29:13.498594: step 517, loss 897.788, acc 0.0625\n",
      "2016-09-11T09:29:14.491662: step 518, loss 973.493, acc 0.0625\n",
      "2016-09-11T09:29:15.549752: step 519, loss 1025.63, acc 0.078125\n",
      "2016-09-11T09:29:15.828624: step 520, loss 867.076, acc 0.214286\n",
      "2016-09-11T09:29:16.755260: step 521, loss 1079.22, acc 0.109375\n",
      "2016-09-11T09:29:17.739822: step 522, loss 860.202, acc 0.09375\n",
      "2016-09-11T09:29:18.876593: step 523, loss 1001.87, acc 0.078125\n",
      "2016-09-11T09:29:19.881262: step 524, loss 865.999, acc 0.0625\n",
      "2016-09-11T09:29:20.886545: step 525, loss 925.868, acc 0.078125\n",
      "2016-09-11T09:29:21.890057: step 526, loss 860.76, acc 0.0625\n",
      "2016-09-11T09:29:22.905744: step 527, loss 1174.52, acc 0.078125\n",
      "2016-09-11T09:29:23.933689: step 528, loss 704.769, acc 0.171875\n",
      "2016-09-11T09:29:24.892941: step 529, loss 819.544, acc 0.09375\n",
      "2016-09-11T09:29:25.905970: step 530, loss 672.97, acc 0.078125\n",
      "2016-09-11T09:29:27.076582: step 531, loss 815.162, acc 0.21875\n",
      "2016-09-11T09:29:28.074539: step 532, loss 899.781, acc 0.078125\n",
      "2016-09-11T09:29:28.280216: step 533, loss 1227.3, acc 0\n",
      "2016-09-11T09:29:29.280998: step 534, loss 703.577, acc 0.046875\n",
      "2016-09-11T09:29:30.300152: step 535, loss 1018.92, acc 0.078125\n",
      "2016-09-11T09:29:31.312931: step 536, loss 895.901, acc 0.09375\n",
      "2016-09-11T09:29:32.490549: step 537, loss 749.26, acc 0.125\n",
      "2016-09-11T09:29:33.470021: step 538, loss 936.615, acc 0.109375\n",
      "2016-09-11T09:29:34.483898: step 539, loss 1072.19, acc 0.078125\n",
      "2016-09-11T09:29:35.488889: step 540, loss 704.871, acc 0.09375\n",
      "2016-09-11T09:29:36.531343: step 541, loss 1143.08, acc 0.125\n",
      "2016-09-11T09:29:37.517995: step 542, loss 1114.27, acc 0.1875\n",
      "2016-09-11T09:29:38.504105: step 543, loss 1493.35, acc 0.0625\n",
      "2016-09-11T09:29:39.659757: step 544, loss 951.609, acc 0.125\n",
      "2016-09-11T09:29:40.689002: step 545, loss 797.07, acc 0.09375\n",
      "2016-09-11T09:29:40.885430: step 546, loss 984.865, acc 0.0714286\n",
      "2016-09-11T09:29:41.884813: step 547, loss 942.777, acc 0.046875\n",
      "2016-09-11T09:29:42.934586: step 548, loss 1117.6, acc 0.09375\n",
      "2016-09-11T09:29:43.914526: step 549, loss 786.86, acc 0.109375\n",
      "2016-09-11T09:29:44.902856: step 550, loss 1141.94, acc 0.0625\n",
      "2016-09-11T09:29:45.921538: step 551, loss 803.528, acc 0.109375\n",
      "2016-09-11T09:29:46.930580: step 552, loss 955.325, acc 0.078125\n",
      "2016-09-11T09:29:47.941108: step 553, loss 1103.19, acc 0.078125\n",
      "2016-09-11T09:29:48.925646: step 554, loss 1211.59, acc 0.109375\n",
      "2016-09-11T09:29:49.922630: step 555, loss 1261.12, acc 0.15625\n",
      "2016-09-11T09:29:51.110878: step 556, loss 1217.52, acc 0.09375\n",
      "2016-09-11T09:29:52.118820: step 557, loss 1275.29, acc 0.125\n",
      "2016-09-11T09:29:53.262013: step 558, loss 1011.22, acc 0.046875\n",
      "2016-09-11T09:29:53.493014: step 559, loss 1875.59, acc 0.214286\n",
      "2016-09-11T09:29:54.509915: step 560, loss 1347.65, acc 0.0625\n",
      "2016-09-11T09:29:55.533800: step 561, loss 859.071, acc 0.140625\n",
      "2016-09-11T09:29:56.502038: step 562, loss 927.757, acc 0.109375\n",
      "2016-09-11T09:29:57.494585: step 563, loss 1138.29, acc 0.046875\n",
      "2016-09-11T09:29:58.514580: step 564, loss 959.207, acc 0.09375\n",
      "2016-09-11T09:29:59.509317: step 565, loss 870.764, acc 0.140625\n",
      "2016-09-11T09:30:00.675560: step 566, loss 1080.3, acc 0.09375\n",
      "2016-09-11T09:30:01.663182: step 567, loss 984.283, acc 0.15625\n",
      "2016-09-11T09:30:02.726239: step 568, loss 1091.72, acc 0.109375\n",
      "2016-09-11T09:30:03.810284: step 569, loss 1283.69, acc 0.109375\n",
      "2016-09-11T09:30:04.899055: step 570, loss 700.779, acc 0.09375\n",
      "2016-09-11T09:30:05.910485: step 571, loss 1265.48, acc 0.0625\n",
      "2016-09-11T09:30:06.142694: step 572, loss 752.202, acc 0.214286\n",
      "2016-09-11T09:30:07.295354: step 573, loss 1121.66, acc 0.125\n",
      "2016-09-11T09:30:08.331365: step 574, loss 957.772, acc 0.09375\n",
      "2016-09-11T09:30:09.329414: step 575, loss 1623.39, acc 0.09375\n",
      "2016-09-11T09:30:10.436284: step 576, loss 870.94, acc 0.15625\n",
      "2016-09-11T09:30:11.489473: step 577, loss 1588.22, acc 0.0625\n",
      "2016-09-11T09:30:12.520102: step 578, loss 1089.5, acc 0.03125\n",
      "2016-09-11T09:30:13.507481: step 579, loss 1156.42, acc 0.09375\n",
      "2016-09-11T09:30:14.659138: step 580, loss 934.349, acc 0.078125\n",
      "2016-09-11T09:30:15.662346: step 581, loss 1191.2, acc 0.125\n",
      "2016-09-11T09:30:16.689305: step 582, loss 1233.28, acc 0.03125\n",
      "2016-09-11T09:30:17.743374: step 583, loss 1290.36, acc 0.15625\n",
      "2016-09-11T09:30:18.711234: step 584, loss 1364.58, acc 0.09375\n",
      "2016-09-11T09:30:18.918474: step 585, loss 1350.26, acc 0.0714286\n",
      "2016-09-11T09:30:19.917736: step 586, loss 1284.49, acc 0.078125\n",
      "2016-09-11T09:30:20.950807: step 587, loss 1507.65, acc 0.0625\n",
      "2016-09-11T09:30:22.062221: step 588, loss 1220.99, acc 0.09375\n",
      "2016-09-11T09:30:23.075986: step 589, loss 1595.57, acc 0.109375\n",
      "2016-09-11T09:30:24.119939: step 590, loss 1377.44, acc 0.125\n",
      "2016-09-11T09:30:25.181835: step 591, loss 1525.42, acc 0.046875\n",
      "2016-09-11T09:30:26.142802: step 592, loss 1063.26, acc 0.09375\n",
      "2016-09-11T09:30:27.113840: step 593, loss 1217.11, acc 0.0625\n",
      "2016-09-11T09:30:28.262396: step 594, loss 1067.2, acc 0.140625\n",
      "2016-09-11T09:30:29.299711: step 595, loss 1357.21, acc 0.109375\n",
      "2016-09-11T09:30:30.451327: step 596, loss 1180.19, acc 0.078125\n",
      "2016-09-11T09:30:31.491637: step 597, loss 1209.65, acc 0.078125\n",
      "2016-09-11T09:30:31.714404: step 598, loss 715.122, acc 0.214286\n",
      "2016-09-11T09:30:32.745494: step 599, loss 1014.05, acc 0.03125\n",
      "2016-09-11T09:30:33.808045: step 600, loss 1385.99, acc 0.03125\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:30:34.817451: step 600, loss 3521.1, acc 0.147959\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-600\n",
      "\n",
      "2016-09-11T09:30:37.087575: step 601, loss 1123.63, acc 0.109375\n",
      "2016-09-11T09:30:38.100093: step 602, loss 1465.36, acc 0.09375\n",
      "2016-09-11T09:30:39.271551: step 603, loss 1150.63, acc 0.109375\n",
      "2016-09-11T09:30:40.271003: step 604, loss 1479.06, acc 0.09375\n",
      "2016-09-11T09:30:41.267273: step 605, loss 1321.43, acc 0.078125\n",
      "2016-09-11T09:30:42.265962: step 606, loss 1660.25, acc 0.03125\n",
      "2016-09-11T09:30:43.297535: step 607, loss 1471.95, acc 0.078125\n",
      "2016-09-11T09:30:44.301553: step 608, loss 1607.72, acc 0.03125\n",
      "2016-09-11T09:30:45.340840: step 609, loss 1975.06, acc 0.125\n",
      "2016-09-11T09:30:46.465255: step 610, loss 1400.73, acc 0.09375\n",
      "2016-09-11T09:30:46.680390: step 611, loss 1997.74, acc 0.214286\n",
      "2016-09-11T09:30:47.699059: step 612, loss 1260.72, acc 0.078125\n",
      "2016-09-11T09:30:48.709911: step 613, loss 1513.41, acc 0.03125\n",
      "2016-09-11T09:30:49.723719: step 614, loss 1205.79, acc 0.09375\n",
      "2016-09-11T09:30:50.781391: step 615, loss 1520.24, acc 0.09375\n",
      "2016-09-11T09:30:51.865223: step 616, loss 1588.27, acc 0.0625\n",
      "2016-09-11T09:30:52.979703: step 617, loss 1399.89, acc 0.09375\n",
      "2016-09-11T09:30:54.149629: step 618, loss 1721.5, acc 0.0625\n",
      "2016-09-11T09:30:55.122871: step 619, loss 1385.64, acc 0.109375\n",
      "2016-09-11T09:30:56.102638: step 620, loss 1556.74, acc 0.0625\n",
      "2016-09-11T09:30:57.124862: step 621, loss 1977.43, acc 0.0625\n",
      "2016-09-11T09:30:58.171619: step 622, loss 1145.41, acc 0.109375\n",
      "2016-09-11T09:30:59.186688: step 623, loss 1482.54, acc 0.125\n",
      "2016-09-11T09:30:59.474184: step 624, loss 1745.44, acc 0.142857\n",
      "2016-09-11T09:31:00.580181: step 625, loss 1792.68, acc 0.078125\n",
      "2016-09-11T09:31:01.540542: step 626, loss 1479.57, acc 0.09375\n",
      "2016-09-11T09:31:02.666869: step 627, loss 1472.68, acc 0.09375\n",
      "2016-09-11T09:31:03.669186: step 628, loss 1388.07, acc 0.15625\n",
      "2016-09-11T09:31:04.722030: step 629, loss 1032.72, acc 0.171875\n",
      "2016-09-11T09:31:05.727608: step 630, loss 1851.23, acc 0.109375\n",
      "2016-09-11T09:31:06.884493: step 631, loss 1695.1, acc 0.09375\n",
      "2016-09-11T09:31:07.906459: step 632, loss 2258.99, acc 0.0625\n",
      "2016-09-11T09:31:09.089610: step 633, loss 1734.2, acc 0.140625\n",
      "2016-09-11T09:31:10.100088: step 634, loss 1773.66, acc 0.109375\n",
      "2016-09-11T09:31:11.127369: step 635, loss 1765.71, acc 0\n",
      "2016-09-11T09:31:12.282583: step 636, loss 1425.33, acc 0.09375\n",
      "2016-09-11T09:31:12.524715: step 637, loss 1241.54, acc 0.142857\n",
      "2016-09-11T09:31:13.549207: step 638, loss 1531.03, acc 0.0625\n",
      "2016-09-11T09:31:14.671004: step 639, loss 1303.46, acc 0.0625\n",
      "2016-09-11T09:31:15.722123: step 640, loss 1899.91, acc 0.0625\n",
      "2016-09-11T09:31:16.721593: step 641, loss 1888.57, acc 0.0625\n",
      "2016-09-11T09:31:17.874488: step 642, loss 1790.53, acc 0.109375\n",
      "2016-09-11T09:31:18.895977: step 643, loss 1795.17, acc 0.03125\n",
      "2016-09-11T09:31:19.988035: step 644, loss 1565.82, acc 0.109375\n",
      "2016-09-11T09:31:21.086984: step 645, loss 1279.33, acc 0.078125\n",
      "2016-09-11T09:31:22.168384: step 646, loss 1199.27, acc 0.125\n",
      "2016-09-11T09:31:23.159114: step 647, loss 1560.96, acc 0.109375\n",
      "2016-09-11T09:31:24.177982: step 648, loss 1565.72, acc 0.140625\n",
      "2016-09-11T09:31:25.312764: step 649, loss 1521.25, acc 0.078125\n",
      "2016-09-11T09:31:25.579860: step 650, loss 1774.37, acc 0.0714286\n",
      "2016-09-11T09:31:26.515166: step 651, loss 1887.74, acc 0.09375\n",
      "2016-09-11T09:31:27.581023: step 652, loss 1545.91, acc 0.0625\n",
      "2016-09-11T09:31:28.533558: step 653, loss 1996.09, acc 0.0625\n",
      "2016-09-11T09:31:29.533273: step 654, loss 1421.18, acc 0.15625\n",
      "2016-09-11T09:31:30.511277: step 655, loss 1846.84, acc 0.140625\n",
      "2016-09-11T09:31:31.513647: step 656, loss 1587.49, acc 0.0625\n",
      "2016-09-11T09:31:32.657258: step 657, loss 2037.38, acc 0.03125\n",
      "2016-09-11T09:31:33.518837: step 658, loss 1567.18, acc 0.0625\n",
      "2016-09-11T09:31:34.657226: step 659, loss 1763.76, acc 0.0625\n",
      "2016-09-11T09:31:35.675702: step 660, loss 1528.97, acc 0.109375\n",
      "2016-09-11T09:31:36.693118: step 661, loss 2005.55, acc 0.125\n",
      "2016-09-11T09:31:37.702050: step 662, loss 1852.06, acc 0.109375\n",
      "2016-09-11T09:31:37.907064: step 663, loss 1897.06, acc 0.0714286\n",
      "2016-09-11T09:31:38.964011: step 664, loss 1773.35, acc 0.125\n",
      "2016-09-11T09:31:39.928338: step 665, loss 2179.9, acc 0.03125\n",
      "2016-09-11T09:31:40.934976: step 666, loss 1992.26, acc 0.03125\n",
      "2016-09-11T09:31:41.987067: step 667, loss 1442.75, acc 0.03125\n",
      "2016-09-11T09:31:42.925376: step 668, loss 1884.31, acc 0.03125\n",
      "2016-09-11T09:31:44.078416: step 669, loss 1937.21, acc 0.046875\n",
      "2016-09-11T09:31:45.085436: step 670, loss 2148.08, acc 0.15625\n",
      "2016-09-11T09:31:46.073726: step 671, loss 1944.74, acc 0.109375\n",
      "2016-09-11T09:31:47.066625: step 672, loss 1652.95, acc 0.109375\n",
      "2016-09-11T09:31:48.068272: step 673, loss 1948.87, acc 0.125\n",
      "2016-09-11T09:31:49.070467: step 674, loss 2320.02, acc 0.078125\n",
      "2016-09-11T09:31:50.121733: step 675, loss 1504, acc 0.125\n",
      "2016-09-11T09:31:50.299121: step 676, loss 1661.01, acc 0.0714286\n",
      "2016-09-11T09:31:51.327810: step 677, loss 1506.06, acc 0.0625\n",
      "2016-09-11T09:31:52.322018: step 678, loss 1907.61, acc 0.125\n",
      "2016-09-11T09:31:53.469859: step 679, loss 1564.42, acc 0.078125\n",
      "2016-09-11T09:31:54.468957: step 680, loss 1476, acc 0.078125\n",
      "2016-09-11T09:31:55.509594: step 681, loss 1658.65, acc 0.125\n",
      "2016-09-11T09:31:56.489685: step 682, loss 2101.54, acc 0.078125\n",
      "2016-09-11T09:31:57.496086: step 683, loss 2318.84, acc 0.140625\n",
      "2016-09-11T09:31:58.487403: step 684, loss 2229.97, acc 0.046875\n",
      "2016-09-11T09:31:59.520684: step 685, loss 2513.39, acc 0.109375\n",
      "2016-09-11T09:32:00.526312: step 686, loss 1445.36, acc 0.09375\n",
      "2016-09-11T09:32:01.519694: step 687, loss 1484.25, acc 0.078125\n",
      "2016-09-11T09:32:02.511462: step 688, loss 2223.13, acc 0.03125\n",
      "2016-09-11T09:32:02.720858: step 689, loss 3578.9, acc 0.142857\n",
      "2016-09-11T09:32:03.709312: step 690, loss 2235.38, acc 0.0625\n",
      "2016-09-11T09:32:04.865200: step 691, loss 1597.89, acc 0.109375\n",
      "2016-09-11T09:32:05.873883: step 692, loss 1769.12, acc 0.125\n",
      "2016-09-11T09:32:06.900341: step 693, loss 1808.7, acc 0.09375\n",
      "2016-09-11T09:32:07.883212: step 694, loss 2045.34, acc 0.15625\n",
      "2016-09-11T09:32:08.876141: step 695, loss 1992.14, acc 0.125\n",
      "2016-09-11T09:32:09.912459: step 696, loss 2080.41, acc 0.125\n",
      "2016-09-11T09:32:10.907652: step 697, loss 1528.09, acc 0.15625\n",
      "2016-09-11T09:32:11.957073: step 698, loss 1683.83, acc 0.15625\n",
      "2016-09-11T09:32:12.922519: step 699, loss 2266.79, acc 0.015625\n",
      "2016-09-11T09:32:13.914022: step 700, loss 1350.52, acc 0.125\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:32:14.901687: step 700, loss 6136.84, acc 0.168367\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-700\n",
      "\n",
      "2016-09-11T09:32:16.881192: step 701, loss 1694.57, acc 0.109375\n",
      "2016-09-11T09:32:17.102622: step 702, loss 1400.35, acc 0\n",
      "2016-09-11T09:32:18.127980: step 703, loss 1358.34, acc 0.125\n",
      "2016-09-11T09:32:19.122614: step 704, loss 1866.88, acc 0.046875\n",
      "2016-09-11T09:32:20.261229: step 705, loss 1227.78, acc 0.09375\n",
      "2016-09-11T09:32:21.274438: step 706, loss 2145.39, acc 0.03125\n",
      "2016-09-11T09:32:22.296291: step 707, loss 2011.35, acc 0.078125\n",
      "2016-09-11T09:32:23.263457: step 708, loss 1555.56, acc 0.171875\n",
      "2016-09-11T09:32:24.269495: step 709, loss 1447.08, acc 0.203125\n",
      "2016-09-11T09:32:25.134348: step 710, loss 1852.92, acc 0.046875\n",
      "2016-09-11T09:32:26.099955: step 711, loss 2061.01, acc 0\n",
      "2016-09-11T09:32:27.288440: step 712, loss 1948.03, acc 0.0625\n",
      "2016-09-11T09:32:28.306903: step 713, loss 2638.08, acc 0.078125\n",
      "2016-09-11T09:32:29.309426: step 714, loss 2218.65, acc 0.125\n",
      "2016-09-11T09:32:29.544525: step 715, loss 1607.14, acc 0\n",
      "2016-09-11T09:32:30.549117: step 716, loss 1724.68, acc 0.125\n",
      "2016-09-11T09:32:31.552389: step 717, loss 1912.19, acc 0.078125\n",
      "2016-09-11T09:32:32.697408: step 718, loss 2355.9, acc 0.046875\n",
      "2016-09-11T09:32:33.663645: step 719, loss 1889.61, acc 0.15625\n",
      "2016-09-11T09:32:34.703123: step 720, loss 1509.89, acc 0.09375\n",
      "2016-09-11T09:32:35.668274: step 721, loss 2068.63, acc 0.109375\n",
      "2016-09-11T09:32:36.681256: step 722, loss 2191.58, acc 0.0625\n",
      "2016-09-11T09:32:37.693315: step 723, loss 2068.99, acc 0.03125\n",
      "2016-09-11T09:32:38.678955: step 724, loss 1605.04, acc 0.15625\n",
      "2016-09-11T09:32:39.676070: step 725, loss 1909.73, acc 0.09375\n",
      "2016-09-11T09:32:40.696253: step 726, loss 2338.76, acc 0.09375\n",
      "2016-09-11T09:32:41.740102: step 727, loss 1649.21, acc 0.109375\n",
      "2016-09-11T09:32:42.088263: step 728, loss 6032.61, acc 0\n",
      "2016-09-11T09:32:43.091446: step 729, loss 2081.99, acc 0.078125\n",
      "2016-09-11T09:32:44.111931: step 730, loss 1769, acc 0.109375\n",
      "2016-09-11T09:32:45.118720: step 731, loss 2022.86, acc 0.09375\n",
      "2016-09-11T09:32:46.133862: step 732, loss 2259.69, acc 0.140625\n",
      "2016-09-11T09:32:47.139205: step 733, loss 1535.55, acc 0.109375\n",
      "2016-09-11T09:32:48.265735: step 734, loss 2315.92, acc 0.03125\n",
      "2016-09-11T09:32:49.122769: step 735, loss 2479.3, acc 0.0625\n",
      "2016-09-11T09:32:50.258160: step 736, loss 2325.72, acc 0.0625\n",
      "2016-09-11T09:32:51.261253: step 737, loss 2125.52, acc 0.09375\n",
      "2016-09-11T09:32:52.262591: step 738, loss 2014.98, acc 0.0625\n",
      "2016-09-11T09:32:53.286977: step 739, loss 1813.06, acc 0.109375\n",
      "2016-09-11T09:32:54.324408: step 740, loss 2735.41, acc 0.09375\n",
      "2016-09-11T09:32:54.519924: step 741, loss 1574.82, acc 0.142857\n",
      "2016-09-11T09:32:55.659139: step 742, loss 2996.37, acc 0.125\n",
      "2016-09-11T09:32:56.660641: step 743, loss 2609.38, acc 0.09375\n",
      "2016-09-11T09:32:57.670948: step 744, loss 2194.71, acc 0.109375\n",
      "2016-09-11T09:32:58.685950: step 745, loss 2288.8, acc 0.109375\n",
      "2016-09-11T09:32:59.697144: step 746, loss 2454.65, acc 0.109375\n",
      "2016-09-11T09:33:00.710172: step 747, loss 2059.77, acc 0.0625\n",
      "2016-09-11T09:33:01.693239: step 748, loss 2007.92, acc 0.09375\n",
      "2016-09-11T09:33:02.735737: step 749, loss 2274.42, acc 0.09375\n",
      "2016-09-11T09:33:03.750819: step 750, loss 2053.16, acc 0.171875\n",
      "2016-09-11T09:33:04.752514: step 751, loss 2481.97, acc 0.0625\n",
      "2016-09-11T09:33:05.729588: step 752, loss 2068.59, acc 0.15625\n",
      "2016-09-11T09:33:06.756812: step 753, loss 1712.19, acc 0.109375\n",
      "2016-09-11T09:33:07.074306: step 754, loss 1648.55, acc 0.214286\n",
      "2016-09-11T09:33:08.081696: step 755, loss 1973.63, acc 0.0625\n",
      "2016-09-11T09:33:09.210301: step 756, loss 2563.58, acc 0.078125\n",
      "2016-09-11T09:33:10.395338: step 757, loss 1708.69, acc 0.125\n",
      "2016-09-11T09:33:11.350221: step 758, loss 2675.16, acc 0.140625\n",
      "2016-09-11T09:33:12.410939: step 759, loss 2743.21, acc 0.03125\n",
      "2016-09-11T09:33:13.527042: step 760, loss 1877.63, acc 0.078125\n",
      "2016-09-11T09:33:14.633553: step 761, loss 1722.85, acc 0.109375\n",
      "2016-09-11T09:33:15.645376: step 762, loss 2709.85, acc 0.078125\n",
      "2016-09-11T09:33:16.797544: step 763, loss 1884.45, acc 0.125\n",
      "2016-09-11T09:33:17.915113: step 764, loss 1800.35, acc 0.109375\n",
      "2016-09-11T09:33:19.051994: step 765, loss 1905.17, acc 0.046875\n",
      "2016-09-11T09:33:20.069494: step 766, loss 2658.94, acc 0.078125\n",
      "2016-09-11T09:33:20.381958: step 767, loss 2515.2, acc 0.0714286\n",
      "2016-09-11T09:33:21.347723: step 768, loss 1883.89, acc 0.078125\n",
      "2016-09-11T09:33:22.324162: step 769, loss 1708.96, acc 0.0625\n",
      "2016-09-11T09:33:23.334532: step 770, loss 2335.86, acc 0.125\n",
      "2016-09-11T09:33:24.386335: step 771, loss 3147.06, acc 0.046875\n",
      "2016-09-11T09:33:25.357972: step 772, loss 2483.68, acc 0.109375\n",
      "2016-09-11T09:33:26.334912: step 773, loss 2007.59, acc 0.078125\n",
      "2016-09-11T09:33:27.385163: step 774, loss 2329.06, acc 0.0625\n",
      "2016-09-11T09:33:28.341685: step 775, loss 2814.78, acc 0.109375\n",
      "2016-09-11T09:33:29.322340: step 776, loss 2524.36, acc 0.078125\n",
      "2016-09-11T09:33:30.563070: step 777, loss 1895.26, acc 0.046875\n",
      "2016-09-11T09:33:31.538875: step 778, loss 1958.89, acc 0.09375\n",
      "2016-09-11T09:33:32.502185: step 779, loss 2198.38, acc 0.0625\n",
      "2016-09-11T09:33:32.737191: step 780, loss 1904.23, acc 0.0714286\n",
      "2016-09-11T09:33:33.714939: step 781, loss 2773.47, acc 0.015625\n",
      "2016-09-11T09:33:34.865289: step 782, loss 2539.48, acc 0.125\n",
      "2016-09-11T09:33:35.877936: step 783, loss 2482.63, acc 0.046875\n",
      "2016-09-11T09:33:36.898500: step 784, loss 1726.14, acc 0.09375\n",
      "2016-09-11T09:33:37.907801: step 785, loss 2455.87, acc 0.109375\n",
      "2016-09-11T09:33:38.893118: step 786, loss 2220.52, acc 0.109375\n",
      "2016-09-11T09:33:39.901208: step 787, loss 1815.56, acc 0.109375\n",
      "2016-09-11T09:33:40.906979: step 788, loss 1790.05, acc 0.1875\n",
      "2016-09-11T09:33:41.929171: step 789, loss 2901.82, acc 0.125\n",
      "2016-09-11T09:33:42.950671: step 790, loss 2403.24, acc 0.140625\n",
      "2016-09-11T09:33:43.952397: step 791, loss 2176.29, acc 0.09375\n",
      "2016-09-11T09:33:45.089916: step 792, loss 4214.52, acc 0.0625\n",
      "2016-09-11T09:33:45.309808: step 793, loss 4385.95, acc 0.0714286\n",
      "2016-09-11T09:33:46.320471: step 794, loss 2885.94, acc 0.109375\n",
      "2016-09-11T09:33:47.386091: step 795, loss 2338.63, acc 0.046875\n",
      "2016-09-11T09:33:48.340811: step 796, loss 2695.01, acc 0.0625\n",
      "2016-09-11T09:33:49.493332: step 797, loss 2033.18, acc 0.09375\n",
      "2016-09-11T09:33:50.501225: step 798, loss 1926.79, acc 0.1875\n",
      "2016-09-11T09:33:51.523322: step 799, loss 1982.98, acc 0.109375\n",
      "2016-09-11T09:33:52.516812: step 800, loss 3217.01, acc 0.0625\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:33:53.488928: step 800, loss 9970.18, acc 0.147959\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-800\n",
      "\n",
      "2016-09-11T09:33:55.091301: step 801, loss 2734.82, acc 0.125\n",
      "2016-09-11T09:33:56.100899: step 802, loss 2304.42, acc 0.078125\n",
      "2016-09-11T09:33:57.099346: step 803, loss 2313.33, acc 0.078125\n",
      "2016-09-11T09:33:58.089723: step 804, loss 2836.5, acc 0.109375\n",
      "2016-09-11T09:33:59.090493: step 805, loss 2012.32, acc 0.03125\n",
      "2016-09-11T09:33:59.328443: step 806, loss 3283.27, acc 0\n",
      "2016-09-11T09:34:00.310410: step 807, loss 3163.73, acc 0.109375\n",
      "2016-09-11T09:34:01.301170: step 808, loss 1833.53, acc 0.140625\n",
      "2016-09-11T09:34:02.301519: step 809, loss 2492.49, acc 0.0625\n",
      "2016-09-11T09:34:03.487544: step 810, loss 3081.6, acc 0.140625\n",
      "2016-09-11T09:34:04.509818: step 811, loss 1682.54, acc 0.078125\n",
      "2016-09-11T09:34:05.671674: step 812, loss 1919.03, acc 0.140625\n",
      "2016-09-11T09:34:06.691234: step 813, loss 3534.76, acc 0.09375\n",
      "2016-09-11T09:34:07.688344: step 814, loss 2112.69, acc 0.078125\n",
      "2016-09-11T09:34:08.711780: step 815, loss 2301.96, acc 0.09375\n",
      "2016-09-11T09:34:09.700149: step 816, loss 2280.51, acc 0.109375\n",
      "2016-09-11T09:34:10.879115: step 817, loss 2101.18, acc 0.078125\n",
      "2016-09-11T09:34:11.891206: step 818, loss 2279.97, acc 0.140625\n",
      "2016-09-11T09:34:12.112636: step 819, loss 2217.46, acc 0.214286\n",
      "2016-09-11T09:34:13.263851: step 820, loss 3216.19, acc 0.109375\n",
      "2016-09-11T09:34:14.267335: step 821, loss 2720.85, acc 0.078125\n",
      "2016-09-11T09:34:15.257952: step 822, loss 2224.96, acc 0.15625\n",
      "2016-09-11T09:34:16.273213: step 823, loss 2467.95, acc 0.109375\n",
      "2016-09-11T09:34:17.307911: step 824, loss 2794.94, acc 0.03125\n",
      "2016-09-11T09:34:18.278796: step 825, loss 1787.9, acc 0.0625\n",
      "2016-09-11T09:34:19.296347: step 826, loss 2220.87, acc 0.09375\n",
      "2016-09-11T09:34:20.302996: step 827, loss 1525.86, acc 0.171875\n",
      "2016-09-11T09:34:21.314055: step 828, loss 2896.49, acc 0.09375\n",
      "2016-09-11T09:34:22.305992: step 829, loss 2854.19, acc 0.078125\n",
      "2016-09-11T09:34:23.294647: step 830, loss 2382.99, acc 0.109375\n",
      "2016-09-11T09:34:24.296379: step 831, loss 2298.74, acc 0.140625\n",
      "2016-09-11T09:34:24.659377: step 832, loss 2975.46, acc 0.142857\n",
      "2016-09-11T09:34:25.505956: step 833, loss 2159.76, acc 0.0625\n",
      "2016-09-11T09:34:26.663892: step 834, loss 2997.38, acc 0.03125\n",
      "2016-09-11T09:34:27.695795: step 835, loss 2749.01, acc 0.125\n",
      "2016-09-11T09:34:28.704558: step 836, loss 2705.39, acc 0.078125\n",
      "2016-09-11T09:34:29.872376: step 837, loss 2750.31, acc 0.09375\n",
      "2016-09-11T09:34:30.887640: step 838, loss 2060.08, acc 0.09375\n",
      "2016-09-11T09:34:31.894310: step 839, loss 2671.56, acc 0.0625\n",
      "2016-09-11T09:34:32.912068: step 840, loss 3156.78, acc 0.09375\n",
      "2016-09-11T09:34:34.063361: step 841, loss 2978.82, acc 0.0625\n",
      "2016-09-11T09:34:35.072100: step 842, loss 2457.31, acc 0.09375\n",
      "2016-09-11T09:34:36.089603: step 843, loss 3497.6, acc 0.0625\n",
      "2016-09-11T09:34:37.081471: step 844, loss 2135.94, acc 0.03125\n",
      "2016-09-11T09:34:37.292206: step 845, loss 4863.75, acc 0.0714286\n",
      "2016-09-11T09:34:38.297926: step 846, loss 2636.76, acc 0.09375\n",
      "2016-09-11T09:34:39.299677: step 847, loss 2579.27, acc 0.09375\n",
      "2016-09-11T09:34:40.379331: step 848, loss 2392.2, acc 0.09375\n",
      "2016-09-11T09:34:41.303757: step 849, loss 2152.29, acc 0.09375\n",
      "2016-09-11T09:34:42.309371: step 850, loss 2828.95, acc 0.125\n",
      "2016-09-11T09:34:43.302285: step 851, loss 3826.8, acc 0.09375\n",
      "2016-09-11T09:34:44.323348: step 852, loss 2600.97, acc 0.109375\n",
      "2016-09-11T09:34:45.494794: step 853, loss 3110, acc 0.046875\n",
      "2016-09-11T09:34:46.484785: step 854, loss 2782.54, acc 0.125\n",
      "2016-09-11T09:34:47.511734: step 855, loss 2784.85, acc 0.15625\n",
      "2016-09-11T09:34:48.502839: step 856, loss 2430.7, acc 0.109375\n",
      "2016-09-11T09:34:49.533829: step 857, loss 2070.48, acc 0.15625\n",
      "2016-09-11T09:34:49.867163: step 858, loss 2330.74, acc 0.0714286\n",
      "2016-09-11T09:34:50.882392: step 859, loss 3948.11, acc 0.09375\n",
      "2016-09-11T09:34:52.062013: step 860, loss 2059.28, acc 0.03125\n",
      "2016-09-11T09:34:53.090151: step 861, loss 2182.2, acc 0.078125\n",
      "2016-09-11T09:34:54.276298: step 862, loss 2897.54, acc 0.078125\n",
      "2016-09-11T09:34:55.273753: step 863, loss 3749.51, acc 0.09375\n",
      "2016-09-11T09:34:56.283619: step 864, loss 3304.62, acc 0.140625\n",
      "2016-09-11T09:34:57.307846: step 865, loss 3348.85, acc 0.078125\n",
      "2016-09-11T09:34:58.293927: step 866, loss 2411.74, acc 0.140625\n",
      "2016-09-11T09:34:59.288414: step 867, loss 3120.74, acc 0.125\n",
      "2016-09-11T09:35:00.284716: step 868, loss 2631.55, acc 0.078125\n",
      "2016-09-11T09:35:01.286385: step 869, loss 2737.48, acc 0.078125\n",
      "2016-09-11T09:35:02.297318: step 870, loss 3958.94, acc 0.0625\n",
      "2016-09-11T09:35:02.504091: step 871, loss 1585.07, acc 0.0714286\n",
      "2016-09-11T09:35:03.494970: step 872, loss 3405.33, acc 0.109375\n",
      "2016-09-11T09:35:04.502942: step 873, loss 3317.14, acc 0.03125\n",
      "2016-09-11T09:35:05.510697: step 874, loss 3091.06, acc 0.078125\n",
      "2016-09-11T09:35:06.863045: step 875, loss 3103.24, acc 0.09375\n",
      "2016-09-11T09:35:07.895293: step 876, loss 3665.08, acc 0.109375\n",
      "2016-09-11T09:35:08.914931: step 877, loss 2721.63, acc 0.078125\n",
      "2016-09-11T09:35:09.911735: step 878, loss 3786.38, acc 0.015625\n",
      "2016-09-11T09:35:11.080209: step 879, loss 2305.99, acc 0.046875\n",
      "2016-09-11T09:35:12.073457: step 880, loss 3828.38, acc 0.03125\n",
      "2016-09-11T09:35:13.085234: step 881, loss 3475.31, acc 0.09375\n",
      "2016-09-11T09:35:14.075892: step 882, loss 2244.92, acc 0.046875\n",
      "2016-09-11T09:35:15.098436: step 883, loss 4097.18, acc 0.125\n",
      "2016-09-11T09:35:15.315810: step 884, loss 2740.65, acc 0.142857\n",
      "2016-09-11T09:35:16.455843: step 885, loss 2959.08, acc 0.140625\n",
      "2016-09-11T09:35:17.484205: step 886, loss 2407.87, acc 0.09375\n",
      "2016-09-11T09:35:18.457369: step 887, loss 3444.14, acc 0.0625\n",
      "2016-09-11T09:35:19.351302: step 888, loss 2683.97, acc 0.09375\n",
      "2016-09-11T09:35:20.324145: step 889, loss 3574.57, acc 0.03125\n",
      "2016-09-11T09:35:21.307837: step 890, loss 2434, acc 0.109375\n",
      "2016-09-11T09:35:22.297117: step 891, loss 2907.4, acc 0.0625\n",
      "2016-09-11T09:35:23.301562: step 892, loss 2613.61, acc 0.046875\n",
      "2016-09-11T09:35:24.308874: step 893, loss 2521.64, acc 0.21875\n",
      "2016-09-11T09:35:25.304510: step 894, loss 4412.61, acc 0.078125\n",
      "2016-09-11T09:35:26.297191: step 895, loss 3918.72, acc 0.09375\n",
      "2016-09-11T09:35:27.293776: step 896, loss 3169.47, acc 0.0625\n",
      "2016-09-11T09:35:27.657597: step 897, loss 3240.01, acc 0.0714286\n",
      "2016-09-11T09:35:28.674269: step 898, loss 2753.78, acc 0.09375\n",
      "2016-09-11T09:35:29.671578: step 899, loss 2263.91, acc 0.0625\n",
      "2016-09-11T09:35:30.688118: step 900, loss 3772.17, acc 0.0625\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:35:31.688282: step 900, loss 14879.5, acc 0.168367\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-900\n",
      "\n",
      "2016-09-11T09:35:33.535936: step 901, loss 2909.95, acc 0.078125\n",
      "2016-09-11T09:35:34.492006: step 902, loss 3036.51, acc 0.0625\n",
      "2016-09-11T09:35:35.523409: step 903, loss 2693.59, acc 0.046875\n",
      "2016-09-11T09:35:36.474933: step 904, loss 4255.43, acc 0.109375\n",
      "2016-09-11T09:35:37.089481: step 905, loss 3457.13, acc 0.09375\n",
      "2016-09-11T09:35:37.879879: step 906, loss 3830.26, acc 0.171875\n",
      "2016-09-11T09:35:38.514174: step 907, loss 2976.3, acc 0.140625\n",
      "2016-09-11T09:35:39.307922: step 908, loss 3133.06, acc 0.109375\n",
      "2016-09-11T09:35:40.290876: step 909, loss 3449.08, acc 0.046875\n",
      "2016-09-11T09:35:40.515761: step 910, loss 2422.61, acc 0.0714286\n",
      "2016-09-11T09:35:41.685655: step 911, loss 3146.59, acc 0.03125\n",
      "2016-09-11T09:35:42.685420: step 912, loss 2172.3, acc 0.109375\n",
      "2016-09-11T09:35:43.703904: step 913, loss 3757.54, acc 0.109375\n",
      "2016-09-11T09:35:44.690539: step 914, loss 4255.64, acc 0.125\n",
      "2016-09-11T09:35:45.690833: step 915, loss 3783.58, acc 0.078125\n",
      "2016-09-11T09:35:46.711691: step 916, loss 3334.59, acc 0.09375\n",
      "2016-09-11T09:35:47.899638: step 917, loss 2876.63, acc 0.0625\n",
      "2016-09-11T09:35:48.894404: step 918, loss 2572.22, acc 0.15625\n",
      "2016-09-11T09:35:50.059307: step 919, loss 3252.81, acc 0.046875\n",
      "2016-09-11T09:35:51.057480: step 920, loss 2292.27, acc 0.09375\n",
      "2016-09-11T09:35:52.058573: step 921, loss 2584.74, acc 0.125\n",
      "2016-09-11T09:35:53.075813: step 922, loss 3612.57, acc 0.109375\n",
      "2016-09-11T09:35:53.276084: step 923, loss 1692.17, acc 0.0714286\n",
      "2016-09-11T09:35:54.281654: step 924, loss 3971.64, acc 0.015625\n",
      "2016-09-11T09:35:55.057900: step 925, loss 4352.65, acc 0.078125\n",
      "2016-09-11T09:35:55.691299: step 926, loss 3242.04, acc 0.0625\n",
      "2016-09-11T09:35:56.327276: step 927, loss 4690.15, acc 0.09375\n",
      "2016-09-11T09:35:57.105856: step 928, loss 4197.1, acc 0.125\n",
      "2016-09-11T09:35:57.921301: step 929, loss 4282.49, acc 0.09375\n",
      "2016-09-11T09:35:58.969278: step 930, loss 3462.72, acc 0.09375\n",
      "2016-09-11T09:35:59.923060: step 931, loss 2356.51, acc 0.109375\n",
      "2016-09-11T09:36:01.054115: step 932, loss 3442.4, acc 0.0625\n",
      "2016-09-11T09:36:01.963574: step 933, loss 3044.05, acc 0.140625\n",
      "2016-09-11T09:36:03.066861: step 934, loss 2627.43, acc 0.109375\n",
      "2016-09-11T09:36:04.064979: step 935, loss 3407.44, acc 0.03125\n",
      "2016-09-11T09:36:04.275751: step 936, loss 1788.8, acc 0.0714286\n",
      "2016-09-11T09:36:05.304629: step 937, loss 3616.97, acc 0.078125\n",
      "2016-09-11T09:36:06.333329: step 938, loss 2582.26, acc 0.09375\n",
      "2016-09-11T09:36:07.314282: step 939, loss 2766.48, acc 0.1875\n",
      "2016-09-11T09:36:08.306393: step 940, loss 3707.96, acc 0.109375\n",
      "2016-09-11T09:36:09.331969: step 941, loss 4765.11, acc 0.125\n",
      "2016-09-11T09:36:10.352547: step 942, loss 4339.02, acc 0.09375\n",
      "2016-09-11T09:36:11.305158: step 943, loss 3527.66, acc 0.171875\n",
      "2016-09-11T09:36:12.454279: step 944, loss 3251.84, acc 0.140625\n",
      "2016-09-11T09:36:13.313691: step 945, loss 2584.33, acc 0.171875\n",
      "2016-09-11T09:36:14.315333: step 946, loss 3619.68, acc 0.078125\n",
      "2016-09-11T09:36:15.299111: step 947, loss 3839.06, acc 0.078125\n",
      "2016-09-11T09:36:16.312289: step 948, loss 3743.47, acc 0.125\n",
      "2016-09-11T09:36:16.675063: step 949, loss 6512.23, acc 0.142857\n",
      "2016-09-11T09:36:17.688788: step 950, loss 3281.17, acc 0.140625\n",
      "2016-09-11T09:36:18.704416: step 951, loss 3290.27, acc 0.140625\n",
      "2016-09-11T09:36:19.703426: step 952, loss 4073.62, acc 0.109375\n",
      "2016-09-11T09:36:20.704228: step 953, loss 4187.99, acc 0.125\n",
      "2016-09-11T09:36:21.918331: step 954, loss 4777.73, acc 0.0625\n",
      "2016-09-11T09:36:23.083871: step 955, loss 4184.17, acc 0.078125\n",
      "2016-09-11T09:36:24.086748: step 956, loss 3707.12, acc 0.0625\n",
      "2016-09-11T09:36:25.094079: step 957, loss 3092.99, acc 0.09375\n",
      "2016-09-11T09:36:26.087053: step 958, loss 3226.89, acc 0.09375\n",
      "2016-09-11T09:36:27.111378: step 959, loss 3628.5, acc 0.09375\n",
      "2016-09-11T09:36:28.108370: step 960, loss 3281.15, acc 0.03125\n",
      "2016-09-11T09:36:29.256236: step 961, loss 4290.72, acc 0.140625\n",
      "2016-09-11T09:36:29.474987: step 962, loss 2995.27, acc 0.285714\n",
      "2016-09-11T09:36:30.478982: step 963, loss 2899.61, acc 0.125\n",
      "2016-09-11T09:36:31.500391: step 964, loss 2485.9, acc 0.0625\n",
      "2016-09-11T09:36:32.537215: step 965, loss 2336.35, acc 0.0625\n",
      "2016-09-11T09:36:33.502133: step 966, loss 4872.58, acc 0.15625\n",
      "2016-09-11T09:36:34.507706: step 967, loss 3092.8, acc 0.03125\n",
      "2016-09-11T09:36:35.517674: step 968, loss 3489.42, acc 0.125\n",
      "2016-09-11T09:36:36.516795: step 969, loss 3361.69, acc 0.171875\n",
      "2016-09-11T09:36:37.511038: step 970, loss 3611.58, acc 0.09375\n",
      "2016-09-11T09:36:38.531039: step 971, loss 4167.84, acc 0.109375\n",
      "2016-09-11T09:36:39.535557: step 972, loss 3103.38, acc 0.109375\n",
      "2016-09-11T09:36:40.512200: step 973, loss 4515.07, acc 0.0625\n",
      "2016-09-11T09:36:41.536341: step 974, loss 3228.2, acc 0.0625\n",
      "2016-09-11T09:36:41.778964: step 975, loss 4913.74, acc 0.142857\n",
      "2016-09-11T09:36:42.730250: step 976, loss 3495.26, acc 0.109375\n",
      "2016-09-11T09:36:43.736052: step 977, loss 4966.28, acc 0.09375\n",
      "2016-09-11T09:36:44.723350: step 978, loss 3653.64, acc 0.078125\n",
      "2016-09-11T09:36:45.747325: step 979, loss 4122.66, acc 0.140625\n",
      "2016-09-11T09:36:46.716819: step 980, loss 3416.1, acc 0.109375\n",
      "2016-09-11T09:36:47.720536: step 981, loss 4051.02, acc 0.078125\n",
      "2016-09-11T09:36:48.732062: step 982, loss 4042.23, acc 0.078125\n",
      "2016-09-11T09:36:49.736401: step 983, loss 2622.78, acc 0.125\n",
      "2016-09-11T09:36:50.711998: step 984, loss 3923.09, acc 0.140625\n",
      "2016-09-11T09:36:51.717441: step 985, loss 3084.26, acc 0.125\n",
      "2016-09-11T09:36:52.706855: step 986, loss 3989.42, acc 0.109375\n",
      "2016-09-11T09:36:53.876539: step 987, loss 5154, acc 0.0625\n",
      "2016-09-11T09:36:54.080853: step 988, loss 3327.96, acc 0.0714286\n",
      "2016-09-11T09:36:55.257249: step 989, loss 4273.94, acc 0.125\n",
      "2016-09-11T09:36:56.101499: step 990, loss 3258.24, acc 0.109375\n",
      "2016-09-11T09:36:57.112371: step 991, loss 3051.65, acc 0.15625\n",
      "2016-09-11T09:36:58.119467: step 992, loss 6017.35, acc 0.09375\n",
      "2016-09-11T09:36:59.302982: step 993, loss 4487.5, acc 0.109375\n",
      "2016-09-11T09:37:00.307418: step 994, loss 3781.67, acc 0.125\n",
      "2016-09-11T09:37:01.358299: step 995, loss 4791.21, acc 0.09375\n",
      "2016-09-11T09:37:02.316500: step 996, loss 3077.1, acc 0.09375\n",
      "2016-09-11T09:37:03.317799: step 997, loss 3602.55, acc 0.09375\n",
      "2016-09-11T09:37:04.469640: step 998, loss 3495.67, acc 0.109375\n",
      "2016-09-11T09:37:05.498288: step 999, loss 3829.79, acc 0.125\n",
      "2016-09-11T09:37:06.537883: step 1000, loss 4614.67, acc 0.0625\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:37:07.655975: step 1000, loss 20093.4, acc 0.168367\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-1000\n",
      "\n",
      "2016-09-11T09:37:08.688444: step 1001, loss 3839.48, acc 0.142857\n",
      "2016-09-11T09:37:09.769727: step 1002, loss 3677.95, acc 0.09375\n",
      "2016-09-11T09:37:10.719341: step 1003, loss 3990.58, acc 0.140625\n",
      "2016-09-11T09:37:11.795960: step 1004, loss 3385.8, acc 0.046875\n",
      "2016-09-11T09:37:12.867927: step 1005, loss 3893.08, acc 0.03125\n",
      "2016-09-11T09:37:13.881624: step 1006, loss 3125.99, acc 0.140625\n",
      "2016-09-11T09:37:14.731871: step 1007, loss 3926.73, acc 0.078125\n",
      "2016-09-11T09:37:15.717247: step 1008, loss 3121.09, acc 0.140625\n",
      "2016-09-11T09:37:16.710040: step 1009, loss 4996.29, acc 0.0625\n",
      "2016-09-11T09:37:17.743977: step 1010, loss 3819.58, acc 0.125\n",
      "2016-09-11T09:37:18.729224: step 1011, loss 4228.08, acc 0.09375\n",
      "2016-09-11T09:37:19.865560: step 1012, loss 2719.86, acc 0.140625\n",
      "2016-09-11T09:37:20.887392: step 1013, loss 3821.28, acc 0.078125\n",
      "2016-09-11T09:37:21.093855: step 1014, loss 4232.17, acc 0.0714286\n",
      "2016-09-11T09:37:22.105300: step 1015, loss 3826.47, acc 0.125\n",
      "2016-09-11T09:37:23.120817: step 1016, loss 3725.61, acc 0.0625\n",
      "2016-09-11T09:37:24.253364: step 1017, loss 4228.29, acc 0.09375\n",
      "2016-09-11T09:37:25.259881: step 1018, loss 4951.42, acc 0.03125\n",
      "2016-09-11T09:37:26.277049: step 1019, loss 4126.25, acc 0.078125\n",
      "2016-09-11T09:37:27.315467: step 1020, loss 5737.56, acc 0.046875\n",
      "2016-09-11T09:37:28.310973: step 1021, loss 3792.33, acc 0.1875\n",
      "2016-09-11T09:37:29.313631: step 1022, loss 4684.13, acc 0.09375\n",
      "2016-09-11T09:37:30.303929: step 1023, loss 3751.54, acc 0.203125\n",
      "2016-09-11T09:37:31.280881: step 1024, loss 4217.16, acc 0.09375\n",
      "2016-09-11T09:37:32.299215: step 1025, loss 4414.63, acc 0.078125\n",
      "2016-09-11T09:37:33.285776: step 1026, loss 4933.41, acc 0.046875\n",
      "2016-09-11T09:37:33.500797: step 1027, loss 3211.99, acc 0.0714286\n",
      "2016-09-11T09:37:34.522850: step 1028, loss 4958.47, acc 0.0625\n",
      "2016-09-11T09:37:35.485642: step 1029, loss 4012.4, acc 0.15625\n",
      "2016-09-11T09:37:36.496056: step 1030, loss 5196.1, acc 0.078125\n",
      "2016-09-11T09:37:37.491620: step 1031, loss 3121.83, acc 0.09375\n",
      "2016-09-11T09:37:38.510635: step 1032, loss 4035.5, acc 0.0625\n",
      "2016-09-11T09:37:39.653386: step 1033, loss 3080.76, acc 0.03125\n",
      "2016-09-11T09:37:40.527672: step 1034, loss 3747.83, acc 0.125\n",
      "2016-09-11T09:37:41.505822: step 1035, loss 4264.3, acc 0.015625\n",
      "2016-09-11T09:37:42.657368: step 1036, loss 5015.06, acc 0.09375\n",
      "2016-09-11T09:37:43.681679: step 1037, loss 3412.38, acc 0.0625\n",
      "2016-09-11T09:37:44.695544: step 1038, loss 2437.49, acc 0.140625\n",
      "2016-09-11T09:37:45.706327: step 1039, loss 3456.72, acc 0.15625\n",
      "2016-09-11T09:37:46.067129: step 1040, loss 3316.83, acc 0.214286\n",
      "2016-09-11T09:37:47.064510: step 1041, loss 3288.58, acc 0.15625\n",
      "2016-09-11T09:37:48.061823: step 1042, loss 2881.72, acc 0.1875\n",
      "2016-09-11T09:37:49.281927: step 1043, loss 5102.92, acc 0.0625\n",
      "2016-09-11T09:37:50.288430: step 1044, loss 2690.63, acc 0.140625\n",
      "2016-09-11T09:37:51.285577: step 1045, loss 3161.98, acc 0.046875\n",
      "2016-09-11T09:37:52.294526: step 1046, loss 2770.09, acc 0.140625\n",
      "2016-09-11T09:37:53.296723: step 1047, loss 3472.96, acc 0.09375\n",
      "2016-09-11T09:37:54.297792: step 1048, loss 5790.97, acc 0.109375\n",
      "2016-09-11T09:37:55.309855: step 1049, loss 4791.07, acc 0.09375\n",
      "2016-09-11T09:37:56.280824: step 1050, loss 5041.86, acc 0.03125\n",
      "2016-09-11T09:37:57.282092: step 1051, loss 3413.5, acc 0.0625\n",
      "2016-09-11T09:37:58.281767: step 1052, loss 3611.89, acc 0.125\n",
      "2016-09-11T09:37:58.482166: step 1053, loss 5222.16, acc 0.142857\n",
      "2016-09-11T09:37:59.489046: step 1054, loss 3438.72, acc 0.015625\n",
      "2016-09-11T09:38:00.493906: step 1055, loss 4194.2, acc 0.0625\n",
      "2016-09-11T09:38:01.507677: step 1056, loss 3836.05, acc 0.125\n",
      "2016-09-11T09:38:02.695762: step 1057, loss 3000.83, acc 0.046875\n",
      "2016-09-11T09:38:03.701673: step 1058, loss 4775.92, acc 0.109375\n",
      "2016-09-11T09:38:04.694666: step 1059, loss 5211.15, acc 0.109375\n",
      "2016-09-11T09:38:05.899687: step 1060, loss 6609.93, acc 0.046875\n",
      "2016-09-11T09:38:06.696035: step 1061, loss 4146.93, acc 0.15625\n",
      "2016-09-11T09:38:07.306497: step 1062, loss 2826.02, acc 0.109375\n",
      "2016-09-11T09:38:08.102955: step 1063, loss 5024.23, acc 0.046875\n",
      "2016-09-11T09:38:08.907430: step 1064, loss 3995.75, acc 0.125\n",
      "2016-09-11T09:38:09.906357: step 1065, loss 5255.46, acc 0.15625\n",
      "2016-09-11T09:38:10.255752: step 1066, loss 3513.23, acc 0\n",
      "2016-09-11T09:38:11.262308: step 1067, loss 4608.96, acc 0.15625\n",
      "2016-09-11T09:38:12.275355: step 1068, loss 5464.34, acc 0.09375\n",
      "2016-09-11T09:38:13.280045: step 1069, loss 4532.25, acc 0.078125\n",
      "2016-09-11T09:38:14.274354: step 1070, loss 4547.83, acc 0.078125\n",
      "2016-09-11T09:38:15.115180: step 1071, loss 4704.4, acc 0.125\n",
      "2016-09-11T09:38:16.101926: step 1072, loss 4507.08, acc 0.078125\n",
      "2016-09-11T09:38:17.095429: step 1073, loss 3935.36, acc 0.171875\n",
      "2016-09-11T09:38:18.098072: step 1074, loss 5749.05, acc 0.0625\n",
      "2016-09-11T09:38:19.090232: step 1075, loss 3107.7, acc 0.0625\n",
      "2016-09-11T09:38:20.079558: step 1076, loss 4187.71, acc 0.09375\n",
      "2016-09-11T09:38:21.102490: step 1077, loss 3603.78, acc 0.140625\n",
      "2016-09-11T09:38:22.100798: step 1078, loss 4117.12, acc 0.078125\n",
      "2016-09-11T09:38:22.459330: step 1079, loss 3564.02, acc 0.0714286\n",
      "2016-09-11T09:38:23.506944: step 1080, loss 4860.88, acc 0.15625\n",
      "2016-09-11T09:38:24.511765: step 1081, loss 5843.79, acc 0.0625\n",
      "2016-09-11T09:38:25.480753: step 1082, loss 4423.72, acc 0.0625\n",
      "2016-09-11T09:38:26.123427: step 1083, loss 4216.01, acc 0.03125\n",
      "2016-09-11T09:38:26.901338: step 1084, loss 6348.34, acc 0.078125\n",
      "2016-09-11T09:38:27.528126: step 1085, loss 3754.61, acc 0.09375\n",
      "2016-09-11T09:38:28.296503: step 1086, loss 3766.05, acc 0.09375\n",
      "2016-09-11T09:38:29.294414: step 1087, loss 4740.9, acc 0.09375\n",
      "2016-09-11T09:38:30.306699: step 1088, loss 3820.39, acc 0.09375\n",
      "2016-09-11T09:38:31.267642: step 1089, loss 2988.61, acc 0.046875\n",
      "2016-09-11T09:38:32.280008: step 1090, loss 3459.13, acc 0.09375\n",
      "2016-09-11T09:38:33.283415: step 1091, loss 4105.06, acc 0.078125\n",
      "2016-09-11T09:38:33.488708: step 1092, loss 2120.33, acc 0.0714286\n",
      "2016-09-11T09:38:34.481271: step 1093, loss 3045.71, acc 0.078125\n",
      "2016-09-11T09:38:35.490997: step 1094, loss 4473.04, acc 0.078125\n",
      "2016-09-11T09:38:36.492612: step 1095, loss 4144.09, acc 0.078125\n",
      "2016-09-11T09:38:37.515716: step 1096, loss 5232.34, acc 0.140625\n",
      "2016-09-11T09:38:38.674946: step 1097, loss 4779.94, acc 0.140625\n",
      "2016-09-11T09:38:39.674200: step 1098, loss 3262.54, acc 0.109375\n",
      "2016-09-11T09:38:40.718229: step 1099, loss 3699.52, acc 0.140625\n",
      "2016-09-11T09:38:41.699569: step 1100, loss 4265.99, acc 0.046875\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:38:42.703069: step 1100, loss 26298.8, acc 0.0918367\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-1100\n",
      "\n",
      "2016-09-11T09:38:44.662070: step 1101, loss 4757.83, acc 0.015625\n",
      "2016-09-11T09:38:45.674239: step 1102, loss 5214.94, acc 0.0625\n",
      "2016-09-11T09:38:46.690197: step 1103, loss 4946.89, acc 0.1875\n",
      "2016-09-11T09:38:47.692748: step 1104, loss 4796.08, acc 0.171875\n",
      "2016-09-11T09:38:47.892788: step 1105, loss 8775.13, acc 0\n",
      "2016-09-11T09:38:48.903707: step 1106, loss 5123.49, acc 0.09375\n",
      "2016-09-11T09:38:50.061362: step 1107, loss 5407.35, acc 0.09375\n",
      "2016-09-11T09:38:51.102055: step 1108, loss 4651.84, acc 0.078125\n",
      "2016-09-11T09:38:52.089298: step 1109, loss 4005.48, acc 0.125\n",
      "2016-09-11T09:38:53.102430: step 1110, loss 5655.46, acc 0.109375\n",
      "2016-09-11T09:38:54.117516: step 1111, loss 3452.8, acc 0.09375\n",
      "2016-09-11T09:38:55.104948: step 1112, loss 4847.3, acc 0.046875\n",
      "2016-09-11T09:38:56.261855: step 1113, loss 6213.99, acc 0.125\n",
      "2016-09-11T09:38:57.267987: step 1114, loss 4737.37, acc 0.03125\n",
      "2016-09-11T09:38:58.261763: step 1115, loss 4181.19, acc 0.0625\n",
      "2016-09-11T09:38:59.271948: step 1116, loss 3962.64, acc 0.140625\n",
      "2016-09-11T09:39:00.319767: step 1117, loss 4124.8, acc 0.0625\n",
      "2016-09-11T09:39:00.494494: step 1118, loss 7443.29, acc 0.214286\n",
      "2016-09-11T09:39:01.503192: step 1119, loss 4776.41, acc 0.09375\n",
      "2016-09-11T09:39:02.513017: step 1120, loss 3959.46, acc 0.015625\n",
      "2016-09-11T09:39:03.496836: step 1121, loss 4109.18, acc 0.15625\n",
      "2016-09-11T09:39:04.523786: step 1122, loss 4140.48, acc 0.03125\n",
      "2016-09-11T09:39:05.691024: step 1123, loss 4811.7, acc 0.09375\n",
      "2016-09-11T09:39:06.704510: step 1124, loss 2652.45, acc 0.0625\n",
      "2016-09-11T09:39:07.874613: step 1125, loss 5605.98, acc 0.09375\n",
      "2016-09-11T09:39:08.913430: step 1126, loss 4395.63, acc 0.1875\n",
      "2016-09-11T09:39:10.307850: step 1127, loss 3611.8, acc 0.09375\n",
      "2016-09-11T09:39:11.301263: step 1128, loss 4531.67, acc 0.0625\n",
      "2016-09-11T09:39:12.307954: step 1129, loss 4161.45, acc 0.0625\n",
      "2016-09-11T09:39:13.318188: step 1130, loss 6361.16, acc 0.0625\n",
      "2016-09-11T09:39:13.661643: step 1131, loss 2297.43, acc 0.0714286\n",
      "2016-09-11T09:39:14.678176: step 1132, loss 5522.02, acc 0.0625\n",
      "2016-09-11T09:39:15.671689: step 1133, loss 5338.07, acc 0.078125\n",
      "2016-09-11T09:39:16.675046: step 1134, loss 4814.06, acc 0.078125\n",
      "2016-09-11T09:39:17.660868: step 1135, loss 4885.11, acc 0.109375\n",
      "2016-09-11T09:39:18.674299: step 1136, loss 4779.2, acc 0.109375\n",
      "2016-09-11T09:39:19.699206: step 1137, loss 4244.53, acc 0.09375\n",
      "2016-09-11T09:39:20.699480: step 1138, loss 3864.89, acc 0.078125\n",
      "2016-09-11T09:39:21.691750: step 1139, loss 5083.93, acc 0.21875\n",
      "2016-09-11T09:39:22.696948: step 1140, loss 5075.52, acc 0.078125\n",
      "2016-09-11T09:39:23.690665: step 1141, loss 4050.82, acc 0.09375\n",
      "2016-09-11T09:39:24.728732: step 1142, loss 4234.33, acc 0.09375\n",
      "2016-09-11T09:39:25.765665: step 1143, loss 3078.41, acc 0.078125\n",
      "2016-09-11T09:39:26.084147: step 1144, loss 2772.43, acc 0.142857\n",
      "2016-09-11T09:39:27.077930: step 1145, loss 4929.65, acc 0.078125\n",
      "2016-09-11T09:39:28.098600: step 1146, loss 3693.86, acc 0.0625\n",
      "2016-09-11T09:39:29.109079: step 1147, loss 3203.47, acc 0.125\n",
      "2016-09-11T09:39:30.168216: step 1148, loss 3691.97, acc 0.078125\n",
      "2016-09-11T09:39:31.094536: step 1149, loss 4662.15, acc 0.125\n",
      "2016-09-11T09:39:32.114692: step 1150, loss 5075.18, acc 0.109375\n",
      "2016-09-11T09:39:33.172068: step 1151, loss 4957.76, acc 0.171875\n",
      "2016-09-11T09:39:34.174727: step 1152, loss 4720.67, acc 0.046875\n",
      "2016-09-11T09:39:35.126632: step 1153, loss 6691.04, acc 0.078125\n",
      "2016-09-11T09:39:36.155167: step 1154, loss 5945.47, acc 0.125\n",
      "2016-09-11T09:39:37.162961: step 1155, loss 4193.87, acc 0.078125\n",
      "2016-09-11T09:39:38.147472: step 1156, loss 5117.02, acc 0.03125\n",
      "2016-09-11T09:39:38.501720: step 1157, loss 6082.43, acc 0.0714286\n",
      "2016-09-11T09:39:39.482546: step 1158, loss 4188.06, acc 0.046875\n",
      "2016-09-11T09:39:40.484356: step 1159, loss 3523.85, acc 0.046875\n",
      "2016-09-11T09:39:41.485530: step 1160, loss 5202.24, acc 0.09375\n",
      "2016-09-11T09:39:42.538489: step 1161, loss 5446.99, acc 0.171875\n",
      "2016-09-11T09:39:43.508696: step 1162, loss 5013.31, acc 0.09375\n",
      "2016-09-11T09:39:44.523447: step 1163, loss 5498.59, acc 0.109375\n",
      "2016-09-11T09:39:45.523996: step 1164, loss 4766.36, acc 0.125\n",
      "2016-09-11T09:39:46.665206: step 1165, loss 4778.73, acc 0.078125\n",
      "2016-09-11T09:39:47.677979: step 1166, loss 4086.54, acc 0.046875\n",
      "2016-09-11T09:39:48.698894: step 1167, loss 3322.44, acc 0.078125\n",
      "2016-09-11T09:39:49.711285: step 1168, loss 5625.41, acc 0.078125\n",
      "2016-09-11T09:39:50.719801: step 1169, loss 4388.2, acc 0.125\n",
      "2016-09-11T09:39:50.910480: step 1170, loss 2323.69, acc 0.142857\n",
      "2016-09-11T09:39:51.963624: step 1171, loss 5324.8, acc 0.046875\n",
      "2016-09-11T09:39:52.937255: step 1172, loss 4929.97, acc 0.078125\n",
      "2016-09-11T09:39:53.990724: step 1173, loss 7671.83, acc 0.109375\n",
      "2016-09-11T09:39:54.972566: step 1174, loss 5251.99, acc 0.09375\n",
      "2016-09-11T09:39:56.072422: step 1175, loss 4361.24, acc 0.125\n",
      "2016-09-11T09:39:57.124699: step 1176, loss 5626.13, acc 0.109375\n",
      "2016-09-11T09:39:58.131887: step 1177, loss 4114.82, acc 0.078125\n",
      "2016-09-11T09:39:59.137403: step 1178, loss 4111.99, acc 0.046875\n",
      "2016-09-11T09:40:00.123999: step 1179, loss 4049.39, acc 0.03125\n",
      "2016-09-11T09:40:01.138263: step 1180, loss 5155.36, acc 0.109375\n",
      "2016-09-11T09:40:02.332243: step 1181, loss 6780.63, acc 0.078125\n",
      "2016-09-11T09:40:03.406338: step 1182, loss 4915.64, acc 0.15625\n",
      "2016-09-11T09:40:03.690599: step 1183, loss 6387.42, acc 0.357143\n",
      "2016-09-11T09:40:04.729101: step 1184, loss 4528.44, acc 0.0625\n",
      "2016-09-11T09:40:05.724612: step 1185, loss 6187.38, acc 0.09375\n",
      "2016-09-11T09:40:06.745085: step 1186, loss 3664.91, acc 0.0625\n",
      "2016-09-11T09:40:07.726575: step 1187, loss 4178.99, acc 0.078125\n",
      "2016-09-11T09:40:08.786196: step 1188, loss 5491.35, acc 0.078125\n",
      "2016-09-11T09:40:09.752178: step 1189, loss 4496.39, acc 0.140625\n",
      "2016-09-11T09:40:10.909676: step 1190, loss 4823.2, acc 0.046875\n",
      "2016-09-11T09:40:11.971290: step 1191, loss 5443.45, acc 0.109375\n",
      "2016-09-11T09:40:13.098077: step 1192, loss 5411.24, acc 0.015625\n",
      "2016-09-11T09:40:14.126987: step 1193, loss 4042.97, acc 0.046875\n",
      "2016-09-11T09:40:15.118203: step 1194, loss 4988.53, acc 0.078125\n",
      "2016-09-11T09:40:16.169980: step 1195, loss 4687.35, acc 0.125\n",
      "2016-09-11T09:40:16.450766: step 1196, loss 7140.09, acc 0\n",
      "2016-09-11T09:40:17.561525: step 1197, loss 4059.29, acc 0.140625\n",
      "2016-09-11T09:40:18.708403: step 1198, loss 5583.95, acc 0.078125\n",
      "2016-09-11T09:40:19.783401: step 1199, loss 5520.7, acc 0.0625\n",
      "2016-09-11T09:40:20.712638: step 1200, loss 5252.49, acc 0.0625\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:40:21.704956: step 1200, loss 34264, acc 0.163265\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-1200\n",
      "\n",
      "2016-09-11T09:40:23.766235: step 1201, loss 5237.6, acc 0.09375\n",
      "2016-09-11T09:40:24.871249: step 1202, loss 6094.27, acc 0.0625\n",
      "2016-09-11T09:40:25.867415: step 1203, loss 5926.58, acc 0.0625\n",
      "2016-09-11T09:40:26.883056: step 1204, loss 4362.88, acc 0.015625\n",
      "2016-09-11T09:40:27.936280: step 1205, loss 4155.89, acc 0.09375\n",
      "2016-09-11T09:40:29.925353: step 1206, loss 3766.59, acc 0.109375\n",
      "2016-09-11T09:40:31.891167: step 1207, loss 5454.34, acc 0.078125\n",
      "2016-09-11T09:40:33.889873: step 1208, loss 4022.81, acc 0.140625\n",
      "2016-09-11T09:40:34.453959: step 1209, loss 4397.91, acc 0.142857\n",
      "2016-09-11T09:40:36.318832: step 1210, loss 5814.52, acc 0.03125\n",
      "2016-09-11T09:40:38.496373: step 1211, loss 5705.72, acc 0.109375\n",
      "2016-09-11T09:40:40.503127: step 1212, loss 4058.54, acc 0.09375\n",
      "2016-09-11T09:40:42.661967: step 1213, loss 4124.4, acc 0.09375\n",
      "2016-09-11T09:40:44.696623: step 1214, loss 3706.13, acc 0.078125\n",
      "2016-09-11T09:40:46.733963: step 1215, loss 4594.46, acc 0.0625\n",
      "2016-09-11T09:40:48.718525: step 1216, loss 4069.95, acc 0.125\n",
      "2016-09-11T09:40:50.716193: step 1217, loss 3669.41, acc 0.09375\n",
      "2016-09-11T09:40:52.887991: step 1218, loss 6496.22, acc 0.015625\n",
      "2016-09-11T09:40:54.896519: step 1219, loss 4020.13, acc 0.09375\n",
      "2016-09-11T09:40:56.907013: step 1220, loss 4615.69, acc 0.109375\n",
      "2016-09-11T09:40:59.089549: step 1221, loss 5589.08, acc 0.046875\n",
      "2016-09-11T09:40:59.583233: step 1222, loss 3146.62, acc 0.142857\n",
      "2016-09-11T09:41:01.696419: step 1223, loss 5410.39, acc 0.140625\n",
      "2016-09-11T09:41:03.696072: step 1224, loss 5511.64, acc 0.109375\n",
      "2016-09-11T09:41:05.725128: step 1225, loss 4742.61, acc 0.0625\n",
      "2016-09-11T09:41:07.881494: step 1226, loss 6850.02, acc 0.0625\n",
      "2016-09-11T09:41:09.746383: step 1227, loss 5603.09, acc 0.03125\n",
      "2016-09-11T09:41:11.891298: step 1228, loss 3839.47, acc 0.078125\n",
      "2016-09-11T09:41:13.945262: step 1229, loss 6603.19, acc 0.0625\n",
      "2016-09-11T09:41:15.901035: step 1230, loss 4715.46, acc 0.109375\n",
      "2016-09-11T09:41:17.907082: step 1231, loss 5292.51, acc 0.15625\n",
      "2016-09-11T09:41:19.911245: step 1232, loss 3717.49, acc 0.09375\n",
      "2016-09-11T09:41:22.070175: step 1233, loss 7421.16, acc 0.109375\n",
      "2016-09-11T09:41:24.084414: step 1234, loss 5268.41, acc 0.0625\n",
      "2016-09-11T09:41:24.477721: step 1235, loss 3544.53, acc 0.0714286\n",
      "2016-09-11T09:41:26.507195: step 1236, loss 7928.08, acc 0.140625\n",
      "2016-09-11T09:41:28.487572: step 1237, loss 5989.23, acc 0.0625\n",
      "2016-09-11T09:41:30.513251: step 1238, loss 5354.59, acc 0.09375\n",
      "2016-09-11T09:41:32.580391: step 1239, loss 5189.59, acc 0.171875\n",
      "2016-09-11T09:41:34.520479: step 1240, loss 7020.26, acc 0.078125\n",
      "2016-09-11T09:41:36.680107: step 1241, loss 4583.52, acc 0.109375\n",
      "2016-09-11T09:41:38.680023: step 1242, loss 5670, acc 0.09375\n",
      "2016-09-11T09:41:40.687288: step 1243, loss 7185.91, acc 0.0625\n",
      "2016-09-11T09:41:42.697908: step 1244, loss 7575.95, acc 0.140625\n",
      "2016-09-11T09:41:44.697225: step 1245, loss 4419.21, acc 0.125\n",
      "2016-09-11T09:41:46.751372: step 1246, loss 4308.65, acc 0.09375\n",
      "2016-09-11T09:41:48.902514: step 1247, loss 7459.81, acc 0.109375\n",
      "2016-09-11T09:41:49.300409: step 1248, loss 4974.37, acc 0.0714286\n",
      "2016-09-11T09:41:51.337773: step 1249, loss 4844.32, acc 0.0625\n",
      "2016-09-11T09:41:53.280056: step 1250, loss 5993.06, acc 0.125\n",
      "2016-09-11T09:41:55.276728: step 1251, loss 6022.65, acc 0.078125\n",
      "2016-09-11T09:41:57.301627: step 1252, loss 4473.93, acc 0.0625\n",
      "2016-09-11T09:41:59.293273: step 1253, loss 6548.63, acc 0.09375\n",
      "2016-09-11T09:42:01.327573: step 1254, loss 5822.13, acc 0.046875\n",
      "2016-09-11T09:42:03.332022: step 1255, loss 7068.64, acc 0.046875\n",
      "2016-09-11T09:42:05.358917: step 1256, loss 5492.79, acc 0.171875\n",
      "2016-09-11T09:42:07.300725: step 1257, loss 5723.19, acc 0.09375\n",
      "2016-09-11T09:42:09.295591: step 1258, loss 7310.18, acc 0.125\n",
      "2016-09-11T09:42:11.288968: step 1259, loss 4812.93, acc 0.15625\n",
      "2016-09-11T09:42:13.294157: step 1260, loss 6731.87, acc 0.046875\n",
      "2016-09-11T09:42:13.856035: step 1261, loss 4773.56, acc 0\n",
      "2016-09-11T09:42:15.952028: step 1262, loss 4887.34, acc 0.078125\n",
      "2016-09-11T09:42:18.087869: step 1263, loss 5266.21, acc 0.09375\n",
      "2016-09-11T09:42:20.356851: step 1264, loss 5447.14, acc 0.078125\n",
      "2016-09-11T09:42:22.516867: step 1265, loss 5852.6, acc 0.0625\n",
      "2016-09-11T09:42:24.563826: step 1266, loss 4868.78, acc 0.125\n",
      "2016-09-11T09:42:26.690930: step 1267, loss 5106.31, acc 0.109375\n",
      "2016-09-11T09:42:28.773258: step 1268, loss 7391.66, acc 0.09375\n",
      "2016-09-11T09:42:30.899626: step 1269, loss 7580.06, acc 0.09375\n",
      "2016-09-11T09:42:32.933210: step 1270, loss 6758.51, acc 0.09375\n",
      "2016-09-11T09:42:34.925950: step 1271, loss 4352.61, acc 0.109375\n",
      "2016-09-11T09:42:36.889742: step 1272, loss 7954.08, acc 0.015625\n",
      "2016-09-11T09:42:38.945226: step 1273, loss 4983.9, acc 0.109375\n",
      "2016-09-11T09:42:39.492830: step 1274, loss 9476.65, acc 0.142857\n",
      "2016-09-11T09:42:41.499222: step 1275, loss 8612.77, acc 0.09375\n",
      "2016-09-11T09:42:43.493269: step 1276, loss 6163.95, acc 0.203125\n",
      "2016-09-11T09:42:45.661364: step 1277, loss 6157.06, acc 0.015625\n",
      "2016-09-11T09:42:47.681397: step 1278, loss 6074.13, acc 0.125\n",
      "2016-09-11T09:42:49.679771: step 1279, loss 6909.87, acc 0.109375\n",
      "2016-09-11T09:42:51.702420: step 1280, loss 7587.5, acc 0.078125\n",
      "2016-09-11T09:42:53.695469: step 1281, loss 6916.11, acc 0.078125\n",
      "2016-09-11T09:42:55.867314: step 1282, loss 4507.67, acc 0.140625\n",
      "2016-09-11T09:42:57.883874: step 1283, loss 6175.72, acc 0.125\n",
      "2016-09-11T09:42:59.902020: step 1284, loss 6111.28, acc 0.109375\n",
      "2016-09-11T09:43:02.111605: step 1285, loss 5777.07, acc 0.078125\n",
      "2016-09-11T09:43:04.081615: step 1286, loss 6110.05, acc 0.109375\n",
      "2016-09-11T09:43:04.680032: step 1287, loss 6156.66, acc 0.142857\n",
      "2016-09-11T09:43:06.700087: step 1288, loss 4479.53, acc 0.09375\n",
      "2016-09-11T09:43:08.905410: step 1289, loss 6271.94, acc 0.078125\n",
      "2016-09-11T09:43:11.101662: step 1290, loss 4584.23, acc 0.140625\n",
      "2016-09-11T09:43:13.106739: step 1291, loss 6310.88, acc 0.0625\n",
      "2016-09-11T09:43:15.274705: step 1292, loss 7487.66, acc 0.140625\n",
      "2016-09-11T09:43:17.292841: step 1293, loss 6362.3, acc 0.03125\n",
      "2016-09-11T09:43:19.316718: step 1294, loss 5720.44, acc 0.0625\n",
      "2016-09-11T09:43:21.295959: step 1295, loss 5112.45, acc 0.109375\n",
      "2016-09-11T09:43:23.378882: step 1296, loss 7061.01, acc 0.125\n",
      "2016-09-11T09:43:25.487002: step 1297, loss 7351.37, acc 0.125\n",
      "2016-09-11T09:43:27.495899: step 1298, loss 5619.61, acc 0.09375\n",
      "2016-09-11T09:43:29.511033: step 1299, loss 5075.16, acc 0.140625\n",
      "2016-09-11T09:43:30.103400: step 1300, loss 3294.19, acc 0.142857\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:43:31.939137: step 1300, loss 43498.1, acc 0.168367\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-1300\n",
      "\n",
      "2016-09-11T09:43:35.537039: step 1301, loss 6683.11, acc 0.046875\n",
      "2016-09-11T09:43:37.509641: step 1302, loss 6993.27, acc 0.046875\n",
      "2016-09-11T09:43:39.522959: step 1303, loss 6769.21, acc 0.015625\n",
      "2016-09-11T09:43:41.665868: step 1304, loss 5473.98, acc 0.078125\n",
      "2016-09-11T09:43:43.668011: step 1305, loss 6349.5, acc 0.0625\n",
      "2016-09-11T09:43:45.672404: step 1306, loss 7938.66, acc 0.09375\n",
      "2016-09-11T09:43:47.510940: step 1307, loss 7554.16, acc 0.03125\n",
      "2016-09-11T09:43:49.529540: step 1308, loss 5685.01, acc 0.109375\n",
      "2016-09-11T09:43:51.657760: step 1309, loss 5230.08, acc 0.046875\n",
      "2016-09-11T09:43:53.674910: step 1310, loss 6361.13, acc 0.109375\n",
      "2016-09-11T09:43:55.703690: step 1311, loss 5613.26, acc 0.125\n",
      "2016-09-11T09:43:57.726023: step 1312, loss 5646.93, acc 0.078125\n",
      "2016-09-11T09:43:58.300586: step 1313, loss 8303.91, acc 0.0714286\n",
      "2016-09-11T09:44:00.277368: step 1314, loss 5120.26, acc 0.15625\n",
      "2016-09-11T09:44:02.266193: step 1315, loss 5275.68, acc 0.1875\n",
      "2016-09-11T09:44:04.281870: step 1316, loss 5534.8, acc 0.078125\n",
      "2016-09-11T09:44:06.262512: step 1317, loss 5101.38, acc 0.15625\n",
      "2016-09-11T09:44:08.100464: step 1318, loss 4325.18, acc 0.171875\n",
      "2016-09-11T09:44:10.267081: step 1319, loss 8149.56, acc 0.140625\n",
      "2016-09-11T09:44:12.262089: step 1320, loss 5621.93, acc 0.09375\n",
      "2016-09-11T09:44:14.287714: step 1321, loss 6233.21, acc 0.09375\n",
      "2016-09-11T09:44:16.359447: step 1322, loss 6305.69, acc 0.046875\n",
      "2016-09-11T09:44:18.318625: step 1323, loss 4520.19, acc 0.0625\n",
      "2016-09-11T09:44:20.509318: step 1324, loss 5665.75, acc 0.0625\n",
      "2016-09-11T09:44:22.502624: step 1325, loss 7814.86, acc 0.0625\n",
      "2016-09-11T09:44:23.116353: step 1326, loss 7768.54, acc 0.0714286\n",
      "2016-09-11T09:44:25.089081: step 1327, loss 5261.97, acc 0.078125\n",
      "2016-09-11T09:44:27.110343: step 1328, loss 5633.26, acc 0.046875\n",
      "2016-09-11T09:44:29.267017: step 1329, loss 6396.58, acc 0.109375\n",
      "2016-09-11T09:44:31.289439: step 1330, loss 5925.13, acc 0.1875\n",
      "2016-09-11T09:44:33.303878: step 1331, loss 6199.74, acc 0.109375\n",
      "2016-09-11T09:44:35.341823: step 1332, loss 6553.33, acc 0.0625\n",
      "2016-09-11T09:44:37.486251: step 1333, loss 4797.25, acc 0.03125\n",
      "2016-09-11T09:44:39.564043: step 1334, loss 7495.04, acc 0.09375\n",
      "2016-09-11T09:44:41.669217: step 1335, loss 4433.02, acc 0.109375\n",
      "2016-09-11T09:44:43.755123: step 1336, loss 6645.46, acc 0.109375\n",
      "2016-09-11T09:44:45.713953: step 1337, loss 6854.58, acc 0.125\n",
      "2016-09-11T09:44:47.742194: step 1338, loss 5194.79, acc 0.125\n",
      "2016-09-11T09:44:48.291907: step 1339, loss 7209.91, acc 0.0714286\n",
      "2016-09-11T09:44:50.489500: step 1340, loss 7475.27, acc 0.09375\n",
      "2016-09-11T09:44:52.480767: step 1341, loss 7982.3, acc 0.09375\n",
      "2016-09-11T09:44:54.548034: step 1342, loss 5912.93, acc 0.109375\n",
      "2016-09-11T09:44:56.504501: step 1343, loss 5352.88, acc 0.171875\n",
      "2016-09-11T09:44:58.489077: step 1344, loss 7582.14, acc 0.078125\n",
      "2016-09-11T09:45:00.667998: step 1345, loss 6006.5, acc 0.078125\n",
      "2016-09-11T09:45:03.101922: step 1346, loss 6253.51, acc 0.109375\n",
      "2016-09-11T09:45:05.130769: step 1347, loss 6578.85, acc 0.046875\n",
      "2016-09-11T09:45:07.668033: step 1348, loss 5560.28, acc 0.140625\n",
      "2016-09-11T09:45:09.686041: step 1349, loss 4834.89, acc 0.078125\n",
      "2016-09-11T09:45:11.727428: step 1350, loss 6707.63, acc 0.078125\n",
      "2016-09-11T09:45:13.715272: step 1351, loss 5995.35, acc 0.140625\n",
      "2016-09-11T09:45:14.275139: step 1352, loss 5559.21, acc 0\n",
      "2016-09-11T09:45:16.316573: step 1353, loss 8005.42, acc 0.015625\n",
      "2016-09-11T09:45:18.321486: step 1354, loss 6049.14, acc 0.0625\n",
      "2016-09-11T09:45:20.464218: step 1355, loss 7512.45, acc 0.0625\n",
      "2016-09-11T09:45:22.472049: step 1356, loss 5974.6, acc 0.09375\n",
      "2016-09-11T09:45:24.478491: step 1357, loss 5072.64, acc 0.109375\n",
      "2016-09-11T09:45:26.487642: step 1358, loss 7125.73, acc 0.078125\n",
      "2016-09-11T09:45:28.502271: step 1359, loss 4842.49, acc 0.109375\n",
      "2016-09-11T09:45:30.522469: step 1360, loss 7264.99, acc 0.109375\n",
      "2016-09-11T09:45:32.657965: step 1361, loss 7222.13, acc 0.0625\n",
      "2016-09-11T09:45:34.680459: step 1362, loss 4860.21, acc 0.15625\n",
      "2016-09-11T09:45:36.661993: step 1363, loss 5753.55, acc 0.109375\n",
      "2016-09-11T09:45:38.513193: step 1364, loss 6007.8, acc 0.15625\n",
      "2016-09-11T09:45:39.089601: step 1365, loss 6338.96, acc 0.0714286\n",
      "2016-09-11T09:45:41.087111: step 1366, loss 7238.77, acc 0.03125\n",
      "2016-09-11T09:45:43.098893: step 1367, loss 7486.26, acc 0.0625\n",
      "2016-09-11T09:45:45.120463: step 1368, loss 4165.19, acc 0.140625\n",
      "2016-09-11T09:45:47.270844: step 1369, loss 6636.56, acc 0.171875\n",
      "2016-09-11T09:45:49.268772: step 1370, loss 6228.67, acc 0.078125\n",
      "2016-09-11T09:45:51.337733: step 1371, loss 6500.53, acc 0.046875\n",
      "2016-09-11T09:45:53.468814: step 1372, loss 5365.83, acc 0.125\n",
      "2016-09-11T09:45:55.483320: step 1373, loss 6913.52, acc 0.09375\n",
      "2016-09-11T09:45:57.684677: step 1374, loss 7962.14, acc 0.171875\n",
      "2016-09-11T09:45:59.703764: step 1375, loss 6209.51, acc 0.125\n",
      "2016-09-11T09:46:01.704134: step 1376, loss 6615.47, acc 0.125\n",
      "2016-09-11T09:46:03.678051: step 1377, loss 8879.34, acc 0.09375\n",
      "2016-09-11T09:46:04.255591: step 1378, loss 7926.41, acc 0.142857\n",
      "2016-09-11T09:46:06.481154: step 1379, loss 10092.6, acc 0.109375\n",
      "2016-09-11T09:46:08.484426: step 1380, loss 6190.93, acc 0.15625\n",
      "2016-09-11T09:46:10.666066: step 1381, loss 8753.38, acc 0.078125\n",
      "2016-09-11T09:46:12.697984: step 1382, loss 7461.99, acc 0.140625\n",
      "2016-09-11T09:46:14.700351: step 1383, loss 5854.94, acc 0.09375\n",
      "2016-09-11T09:46:16.888592: step 1384, loss 8264.6, acc 0.03125\n",
      "2016-09-11T09:46:18.956377: step 1385, loss 5481.81, acc 0.0625\n",
      "2016-09-11T09:46:20.908669: step 1386, loss 7397.46, acc 0.03125\n",
      "2016-09-11T09:46:23.079965: step 1387, loss 6713.42, acc 0.0625\n",
      "2016-09-11T09:46:25.282452: step 1388, loss 7716.69, acc 0.078125\n",
      "2016-09-11T09:46:27.291477: step 1389, loss 7253.19, acc 0.015625\n",
      "2016-09-11T09:46:29.467150: step 1390, loss 6560.05, acc 0.078125\n",
      "2016-09-11T09:46:29.873699: step 1391, loss 6259.54, acc 0.142857\n",
      "2016-09-11T09:46:31.895237: step 1392, loss 5697.56, acc 0.15625\n",
      "2016-09-11T09:46:34.261792: step 1393, loss 7313.9, acc 0.109375\n",
      "2016-09-11T09:46:36.481107: step 1394, loss 6885.43, acc 0.109375\n",
      "2016-09-11T09:46:38.470606: step 1395, loss 5507.09, acc 0.109375\n",
      "2016-09-11T09:46:40.470593: step 1396, loss 5839.66, acc 0.015625\n",
      "2016-09-11T09:46:42.497355: step 1397, loss 4093, acc 0.0625\n",
      "2016-09-11T09:46:44.500319: step 1398, loss 7160.14, acc 0.09375\n",
      "2016-09-11T09:46:46.683856: step 1399, loss 6391.33, acc 0.0625\n",
      "2016-09-11T09:46:48.897961: step 1400, loss 8030.6, acc 0.09375\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:46:50.870722: step 1400, loss 52515.8, acc 0.168367\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-1400\n",
      "\n",
      "2016-09-11T09:46:54.122981: step 1401, loss 8174.27, acc 0.046875\n",
      "2016-09-11T09:46:56.123295: step 1402, loss 6700.38, acc 0.09375\n",
      "2016-09-11T09:46:58.083299: step 1403, loss 7488.51, acc 0.125\n",
      "2016-09-11T09:46:58.683337: step 1404, loss 7187.27, acc 0.214286\n",
      "2016-09-11T09:47:00.684098: step 1405, loss 5225.3, acc 0.046875\n",
      "2016-09-11T09:47:02.692974: step 1406, loss 5196.3, acc 0.046875\n",
      "2016-09-11T09:47:05.059312: step 1407, loss 8734.88, acc 0.09375\n",
      "2016-09-11T09:47:07.081419: step 1408, loss 10103.8, acc 0.109375\n",
      "2016-09-11T09:47:09.270112: step 1409, loss 6965.12, acc 0.109375\n",
      "2016-09-11T09:47:11.368754: step 1410, loss 8711.2, acc 0.078125\n",
      "2016-09-11T09:47:13.312558: step 1411, loss 7178.72, acc 0.046875\n",
      "2016-09-11T09:47:15.293988: step 1412, loss 5205.75, acc 0.09375\n",
      "2016-09-11T09:47:17.316828: step 1413, loss 7717.22, acc 0.078125\n",
      "2016-09-11T09:47:19.483174: step 1414, loss 5629.89, acc 0.15625\n",
      "2016-09-11T09:47:21.516575: step 1415, loss 6688.8, acc 0.015625\n",
      "2016-09-11T09:47:23.503621: step 1416, loss 6543.84, acc 0.09375\n",
      "2016-09-11T09:47:24.112076: step 1417, loss 3247.93, acc 0\n",
      "2016-09-11T09:47:26.099743: step 1418, loss 4832.98, acc 0.09375\n",
      "2016-09-11T09:47:28.270989: step 1419, loss 7967.25, acc 0.078125\n",
      "2016-09-11T09:47:30.275724: step 1420, loss 6735.81, acc 0.15625\n",
      "2016-09-11T09:47:32.285743: step 1421, loss 5946.22, acc 0.125\n",
      "2016-09-11T09:47:34.349336: step 1422, loss 5386.84, acc 0.171875\n",
      "2016-09-11T09:47:36.497634: step 1423, loss 6134.03, acc 0.09375\n",
      "2016-09-11T09:47:38.681050: step 1424, loss 8291.27, acc 0.078125\n",
      "2016-09-11T09:47:40.710630: step 1425, loss 4679.84, acc 0.0625\n",
      "2016-09-11T09:47:42.705523: step 1426, loss 5440.62, acc 0.046875\n",
      "2016-09-11T09:47:44.703015: step 1427, loss 6562.18, acc 0.140625\n",
      "2016-09-11T09:47:46.721911: step 1428, loss 5530.76, acc 0.09375\n",
      "2016-09-11T09:47:48.708112: step 1429, loss 9757.93, acc 0.03125\n",
      "2016-09-11T09:47:49.293205: step 1430, loss 4037.29, acc 0.214286\n",
      "2016-09-11T09:47:51.301935: step 1431, loss 7852.92, acc 0.15625\n",
      "2016-09-11T09:47:53.280862: step 1432, loss 6086.42, acc 0.109375\n",
      "2016-09-11T09:47:55.464248: step 1433, loss 5470.04, acc 0.078125\n",
      "2016-09-11T09:47:57.463208: step 1434, loss 8687.39, acc 0.125\n",
      "2016-09-11T09:47:59.474637: step 1435, loss 6454.66, acc 0.09375\n",
      "2016-09-11T09:48:01.490609: step 1436, loss 7276.15, acc 0.09375\n",
      "2016-09-11T09:48:03.501959: step 1437, loss 6383.81, acc 0.078125\n",
      "2016-09-11T09:48:05.675611: step 1438, loss 7428.64, acc 0.09375\n",
      "2016-09-11T09:48:07.681914: step 1439, loss 6305.56, acc 0.125\n",
      "2016-09-11T09:48:09.673520: step 1440, loss 4654.56, acc 0.0625\n",
      "2016-09-11T09:48:11.688878: step 1441, loss 6202.21, acc 0.09375\n",
      "2016-09-11T09:48:13.687544: step 1442, loss 6219.33, acc 0.125\n",
      "2016-09-11T09:48:14.265100: step 1443, loss 4726.9, acc 0.0714286\n",
      "2016-09-11T09:48:16.267098: step 1444, loss 6009.86, acc 0.125\n",
      "2016-09-11T09:48:18.287293: step 1445, loss 7299.65, acc 0.0625\n",
      "2016-09-11T09:48:20.287315: step 1446, loss 7130.7, acc 0.125\n",
      "2016-09-11T09:48:22.454681: step 1447, loss 9044.64, acc 0.078125\n",
      "2016-09-11T09:48:24.307602: step 1448, loss 7890.02, acc 0.125\n",
      "2016-09-11T09:48:26.486590: step 1449, loss 6057.69, acc 0.0625\n",
      "2016-09-11T09:48:28.462634: step 1450, loss 6882.96, acc 0.078125\n",
      "2016-09-11T09:48:30.494647: step 1451, loss 5035.4, acc 0.109375\n",
      "2016-09-11T09:48:32.485747: step 1452, loss 7131.81, acc 0.109375\n",
      "2016-09-11T09:48:34.488842: step 1453, loss 7078.7, acc 0.09375\n",
      "2016-09-11T09:48:36.493926: step 1454, loss 8955.35, acc 0.046875\n",
      "2016-09-11T09:48:38.664134: step 1455, loss 6595.7, acc 0.140625\n",
      "2016-09-11T09:48:39.081057: step 1456, loss 8923.81, acc 0\n",
      "2016-09-11T09:48:41.265416: step 1457, loss 6936.26, acc 0.046875\n",
      "2016-09-11T09:48:43.284739: step 1458, loss 5621.35, acc 0.0625\n",
      "2016-09-11T09:48:45.265918: step 1459, loss 7230.73, acc 0.125\n",
      "2016-09-11T09:48:47.295797: step 1460, loss 7495.07, acc 0.140625\n",
      "2016-09-11T09:48:49.298323: step 1461, loss 6138.39, acc 0.09375\n",
      "2016-09-11T09:48:51.482681: step 1462, loss 11492.5, acc 0.125\n",
      "2016-09-11T09:48:53.482029: step 1463, loss 7433.46, acc 0.140625\n",
      "2016-09-11T09:48:55.509463: step 1464, loss 7967.05, acc 0.0625\n",
      "2016-09-11T09:48:57.493263: step 1465, loss 5209.18, acc 0.140625\n",
      "2016-09-11T09:48:59.665783: step 1466, loss 5247.62, acc 0.125\n",
      "2016-09-11T09:49:01.481945: step 1467, loss 5300.84, acc 0.078125\n",
      "2016-09-11T09:49:03.502872: step 1468, loss 6687.41, acc 0.125\n",
      "2016-09-11T09:49:04.075981: step 1469, loss 10604, acc 0.0714286\n",
      "2016-09-11T09:49:06.084741: step 1470, loss 5173.87, acc 0.09375\n",
      "2016-09-11T09:49:08.098392: step 1471, loss 6397.75, acc 0.0625\n",
      "2016-09-11T09:49:10.103248: step 1472, loss 5457.68, acc 0.0625\n",
      "2016-09-11T09:49:12.259960: step 1473, loss 6210.13, acc 0.15625\n",
      "2016-09-11T09:49:14.271402: step 1474, loss 8246.99, acc 0.046875\n",
      "2016-09-11T09:49:16.287709: step 1475, loss 8345.73, acc 0.109375\n",
      "2016-09-11T09:49:18.297904: step 1476, loss 7157.24, acc 0.09375\n",
      "2016-09-11T09:49:20.469988: step 1477, loss 10501.4, acc 0.125\n",
      "2016-09-11T09:49:22.458739: step 1478, loss 7715.91, acc 0.09375\n",
      "2016-09-11T09:49:24.463985: step 1479, loss 5390.52, acc 0.046875\n",
      "2016-09-11T09:49:26.480481: step 1480, loss 9436.5, acc 0.015625\n",
      "2016-09-11T09:49:28.567015: step 1481, loss 6967.07, acc 0.0625\n",
      "2016-09-11T09:49:29.091688: step 1482, loss 6058.36, acc 0.142857\n",
      "2016-09-11T09:49:31.118281: step 1483, loss 8886.32, acc 0.0625\n",
      "2016-09-11T09:49:33.267505: step 1484, loss 5874.41, acc 0.09375\n",
      "2016-09-11T09:49:35.260649: step 1485, loss 6462, acc 0.03125\n",
      "2016-09-11T09:49:37.274703: step 1486, loss 9798.06, acc 0.015625\n",
      "2016-09-11T09:49:39.280532: step 1487, loss 9731.68, acc 0.09375\n",
      "2016-09-11T09:49:41.311753: step 1488, loss 5856.12, acc 0.125\n",
      "2016-09-11T09:49:43.285928: step 1489, loss 7022.43, acc 0.0625\n",
      "2016-09-11T09:49:45.280014: step 1490, loss 7635.29, acc 0.046875\n",
      "2016-09-11T09:49:47.287198: step 1491, loss 7697.15, acc 0.109375\n",
      "2016-09-11T09:49:49.282474: step 1492, loss 8880.68, acc 0.046875\n",
      "2016-09-11T09:49:51.294484: step 1493, loss 10453.7, acc 0.109375\n",
      "2016-09-11T09:49:53.490899: step 1494, loss 5998.94, acc 0.0625\n",
      "2016-09-11T09:49:54.057234: step 1495, loss 4441.26, acc 0\n",
      "2016-09-11T09:49:55.897406: step 1496, loss 7388.13, acc 0.0625\n",
      "2016-09-11T09:49:58.077932: step 1497, loss 9111.01, acc 0.125\n",
      "2016-09-11T09:50:00.061765: step 1498, loss 7723.01, acc 0.15625\n",
      "2016-09-11T09:50:02.406786: step 1499, loss 9547.64, acc 0.125\n",
      "2016-09-11T09:50:04.369933: step 1500, loss 7736.64, acc 0.125\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:50:06.616576: step 1500, loss 63968.2, acc 0.0867347\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-1500\n",
      "\n",
      "2016-09-11T09:50:10.506493: step 1501, loss 6344.22, acc 0.109375\n",
      "2016-09-11T09:50:12.614648: step 1502, loss 7495, acc 0.15625\n",
      "2016-09-11T09:50:14.839382: step 1503, loss 7835.36, acc 0.03125\n",
      "2016-09-11T09:50:16.985266: step 1504, loss 7112.68, acc 0.09375\n",
      "2016-09-11T09:50:19.120063: step 1505, loss 6724.67, acc 0.0625\n",
      "2016-09-11T09:50:21.294531: step 1506, loss 8519.19, acc 0.09375\n",
      "2016-09-11T09:50:23.285518: step 1507, loss 8591.93, acc 0.109375\n",
      "2016-09-11T09:50:23.856872: step 1508, loss 5520.62, acc 0.214286\n",
      "2016-09-11T09:50:25.877182: step 1509, loss 6398.12, acc 0.109375\n",
      "2016-09-11T09:50:27.932989: step 1510, loss 6865.43, acc 0.0625\n",
      "2016-09-11T09:50:30.292623: step 1511, loss 9594.02, acc 0.078125\n",
      "2016-09-11T09:50:32.985638: step 1512, loss 10832.9, acc 0.09375\n",
      "2016-09-11T09:50:34.978646: step 1513, loss 5796.32, acc 0.140625\n",
      "2016-09-11T09:50:37.170535: step 1514, loss 8438.71, acc 0.046875\n",
      "2016-09-11T09:50:39.198973: step 1515, loss 8530.94, acc 0.125\n",
      "2016-09-11T09:50:41.304490: step 1516, loss 4874.39, acc 0.03125\n",
      "2016-09-11T09:50:43.507420: step 1517, loss 7045.47, acc 0.0625\n",
      "2016-09-11T09:50:45.504719: step 1518, loss 7295.81, acc 0.078125\n",
      "2016-09-11T09:50:47.565396: step 1519, loss 8123.02, acc 0.109375\n",
      "2016-09-11T09:50:49.539739: step 1520, loss 6436.03, acc 0.078125\n",
      "2016-09-11T09:50:50.105816: step 1521, loss 4773.39, acc 0\n",
      "2016-09-11T09:50:52.125799: step 1522, loss 6462.39, acc 0.125\n",
      "2016-09-11T09:50:54.141458: step 1523, loss 8012.51, acc 0.140625\n",
      "2016-09-11T09:50:56.280587: step 1524, loss 6535.36, acc 0.140625\n",
      "2016-09-11T09:50:58.269516: step 1525, loss 5786.62, acc 0.09375\n",
      "2016-09-11T09:51:00.286584: step 1526, loss 7253.79, acc 0.046875\n",
      "2016-09-11T09:51:02.289114: step 1527, loss 5838.9, acc 0.109375\n",
      "2016-09-11T09:51:04.281066: step 1528, loss 10139.7, acc 0.03125\n",
      "2016-09-11T09:51:06.277392: step 1529, loss 7242.82, acc 0.09375\n",
      "2016-09-11T09:51:08.291225: step 1530, loss 8439.39, acc 0.109375\n",
      "2016-09-11T09:51:10.308813: step 1531, loss 7385.62, acc 0.125\n",
      "2016-09-11T09:51:12.296057: step 1532, loss 8400.62, acc 0.015625\n",
      "2016-09-11T09:51:14.471998: step 1533, loss 9772.22, acc 0.078125\n",
      "2016-09-11T09:51:14.888782: step 1534, loss 6771.09, acc 0\n",
      "2016-09-11T09:51:17.076312: step 1535, loss 9125.25, acc 0.078125\n",
      "2016-09-11T09:51:19.098489: step 1536, loss 13810.9, acc 0.109375\n",
      "2016-09-11T09:51:21.091579: step 1537, loss 7514.29, acc 0.078125\n",
      "2016-09-11T09:51:23.265866: step 1538, loss 7726.37, acc 0.09375\n",
      "2016-09-11T09:51:25.102365: step 1539, loss 7817.54, acc 0.0625\n",
      "2016-09-11T09:51:27.265516: step 1540, loss 8920.77, acc 0.0625\n",
      "2016-09-11T09:51:29.316799: step 1541, loss 8075.81, acc 0.09375\n",
      "2016-09-11T09:51:31.288131: step 1542, loss 7083.54, acc 0.046875\n",
      "2016-09-11T09:51:33.317287: step 1543, loss 6827.61, acc 0.09375\n",
      "2016-09-11T09:51:35.337969: step 1544, loss 8680.23, acc 0.109375\n",
      "2016-09-11T09:51:37.545599: step 1545, loss 8040.04, acc 0.03125\n",
      "2016-09-11T09:51:39.476590: step 1546, loss 4578.16, acc 0.140625\n",
      "2016-09-11T09:51:39.896577: step 1547, loss 7271.45, acc 0.214286\n",
      "2016-09-11T09:51:41.915077: step 1548, loss 8328.18, acc 0.09375\n",
      "2016-09-11T09:51:43.939510: step 1549, loss 8311.64, acc 0.140625\n",
      "2016-09-11T09:51:45.911505: step 1550, loss 7746.58, acc 0.171875\n",
      "2016-09-11T09:51:47.903879: step 1551, loss 6288.14, acc 0.109375\n",
      "2016-09-11T09:51:49.958669: step 1552, loss 9174.35, acc 0.078125\n",
      "2016-09-11T09:51:51.901382: step 1553, loss 8482.79, acc 0.125\n",
      "2016-09-11T09:51:53.943317: step 1554, loss 7163.62, acc 0.109375\n",
      "2016-09-11T09:51:56.099111: step 1555, loss 7448.62, acc 0.09375\n",
      "2016-09-11T09:51:58.140104: step 1556, loss 6712.01, acc 0.09375\n",
      "2016-09-11T09:52:00.164212: step 1557, loss 9728.85, acc 0.09375\n",
      "2016-09-11T09:52:02.311819: step 1558, loss 8100.69, acc 0.078125\n",
      "2016-09-11T09:52:04.312245: step 1559, loss 4801.88, acc 0.125\n",
      "2016-09-11T09:52:04.909175: step 1560, loss 9391.23, acc 0\n",
      "2016-09-11T09:52:06.905652: step 1561, loss 7842.65, acc 0.078125\n",
      "2016-09-11T09:52:08.893630: step 1562, loss 6795.58, acc 0.078125\n",
      "2016-09-11T09:52:10.958973: step 1563, loss 7442.35, acc 0.09375\n",
      "2016-09-11T09:52:12.990694: step 1564, loss 7061.68, acc 0.078125\n",
      "2016-09-11T09:52:14.899951: step 1565, loss 9137.27, acc 0.0625\n",
      "2016-09-11T09:52:17.069776: step 1566, loss 8684.99, acc 0.0625\n",
      "2016-09-11T09:52:19.064597: step 1567, loss 8499.19, acc 0.109375\n",
      "2016-09-11T09:52:21.120478: step 1568, loss 10921.6, acc 0.109375\n",
      "2016-09-11T09:52:23.111329: step 1569, loss 7517.63, acc 0.109375\n",
      "2016-09-11T09:52:25.262620: step 1570, loss 8128.84, acc 0.03125\n",
      "2016-09-11T09:52:27.288152: step 1571, loss 9172.62, acc 0.0625\n",
      "2016-09-11T09:52:29.290260: step 1572, loss 4953.04, acc 0.1875\n",
      "2016-09-11T09:52:29.882975: step 1573, loss 9509.82, acc 0.142857\n",
      "2016-09-11T09:52:31.910033: step 1574, loss 10390.2, acc 0.078125\n",
      "2016-09-11T09:52:33.905567: step 1575, loss 7140.76, acc 0.109375\n",
      "2016-09-11T09:52:36.073903: step 1576, loss 8488.02, acc 0.125\n",
      "2016-09-11T09:52:38.068656: step 1577, loss 7666.6, acc 0.015625\n",
      "2016-09-11T09:52:40.089259: step 1578, loss 9484.64, acc 0.046875\n",
      "2016-09-11T09:52:42.095457: step 1579, loss 7861.63, acc 0.0625\n",
      "2016-09-11T09:52:44.260897: step 1580, loss 9487.13, acc 0.09375\n",
      "2016-09-11T09:52:46.268845: step 1581, loss 8155.99, acc 0.09375\n",
      "2016-09-11T09:52:48.320116: step 1582, loss 10456, acc 0.078125\n",
      "2016-09-11T09:52:50.262464: step 1583, loss 8834.3, acc 0.0625\n",
      "2016-09-11T09:52:52.313333: step 1584, loss 8424.04, acc 0.09375\n",
      "2016-09-11T09:52:54.280329: step 1585, loss 10427.2, acc 0.0625\n",
      "2016-09-11T09:52:54.682498: step 1586, loss 6741.96, acc 0.214286\n",
      "2016-09-11T09:52:56.709317: step 1587, loss 9182.85, acc 0.078125\n",
      "2016-09-11T09:52:58.860575: step 1588, loss 8213.84, acc 0.140625\n",
      "2016-09-11T09:53:00.869291: step 1589, loss 9465.55, acc 0.03125\n",
      "2016-09-11T09:53:02.909106: step 1590, loss 9605.15, acc 0.03125\n",
      "2016-09-11T09:53:04.895789: step 1591, loss 11722.7, acc 0.0625\n",
      "2016-09-11T09:53:07.061563: step 1592, loss 8587.49, acc 0.046875\n",
      "2016-09-11T09:53:09.083563: step 1593, loss 8055.73, acc 0.046875\n",
      "2016-09-11T09:53:11.097250: step 1594, loss 6236.41, acc 0.125\n",
      "2016-09-11T09:53:13.168433: step 1595, loss 6468.83, acc 0.171875\n",
      "2016-09-11T09:53:15.403182: step 1596, loss 8681.63, acc 0.03125\n",
      "2016-09-11T09:53:17.488728: step 1597, loss 8664.68, acc 0.09375\n",
      "2016-09-11T09:53:19.564828: step 1598, loss 12133.7, acc 0.015625\n",
      "2016-09-11T09:53:20.176585: step 1599, loss 4721.8, acc 0.142857\n",
      "2016-09-11T09:53:22.277722: step 1600, loss 7590.46, acc 0.078125\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:53:24.238614: step 1600, loss 77846.3, acc 0.168367\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-1600\n",
      "\n",
      "2016-09-11T09:53:28.362010: step 1601, loss 8856.63, acc 0.0625\n",
      "2016-09-11T09:53:30.327148: step 1602, loss 7777.46, acc 0.09375\n",
      "2016-09-11T09:53:32.458379: step 1603, loss 9252.86, acc 0.125\n",
      "2016-09-11T09:53:34.477730: step 1604, loss 8340.76, acc 0.125\n",
      "2016-09-11T09:53:36.476897: step 1605, loss 6740, acc 0.125\n",
      "2016-09-11T09:53:38.481778: step 1606, loss 7784.09, acc 0.171875\n",
      "2016-09-11T09:53:40.489290: step 1607, loss 8703.34, acc 0.15625\n",
      "2016-09-11T09:53:42.661221: step 1608, loss 9699.58, acc 0.078125\n",
      "2016-09-11T09:53:44.685436: step 1609, loss 10540.7, acc 0.109375\n",
      "2016-09-11T09:53:46.692680: step 1610, loss 7042.28, acc 0.078125\n",
      "2016-09-11T09:53:48.672621: step 1611, loss 9404.95, acc 0.09375\n",
      "2016-09-11T09:53:49.097102: step 1612, loss 14088.6, acc 0.142857\n",
      "2016-09-11T09:53:51.123620: step 1613, loss 8318.56, acc 0.078125\n",
      "2016-09-11T09:53:53.122472: step 1614, loss 12466.3, acc 0.09375\n",
      "2016-09-11T09:53:55.124613: step 1615, loss 9380.18, acc 0.03125\n",
      "2016-09-11T09:53:57.112769: step 1616, loss 8618.36, acc 0.09375\n",
      "2016-09-11T09:53:59.098670: step 1617, loss 11889.5, acc 0.046875\n",
      "2016-09-11T09:54:01.110478: step 1618, loss 7593.75, acc 0.109375\n",
      "2016-09-11T09:54:03.101200: step 1619, loss 5609.68, acc 0.078125\n",
      "2016-09-11T09:54:05.094712: step 1620, loss 9442.07, acc 0.0625\n",
      "2016-09-11T09:54:07.118907: step 1621, loss 9320.29, acc 0.125\n",
      "2016-09-11T09:54:09.123702: step 1622, loss 8067.8, acc 0.03125\n",
      "2016-09-11T09:54:11.269160: step 1623, loss 7606.8, acc 0.0625\n",
      "2016-09-11T09:54:13.271726: step 1624, loss 7425.93, acc 0.09375\n",
      "2016-09-11T09:54:13.697897: step 1625, loss 10097.8, acc 0.142857\n",
      "2016-09-11T09:54:15.719148: step 1626, loss 9089.39, acc 0.078125\n",
      "2016-09-11T09:54:17.696073: step 1627, loss 7943.11, acc 0.03125\n",
      "2016-09-11T09:54:19.885102: step 1628, loss 7878.62, acc 0.078125\n",
      "2016-09-11T09:54:21.888454: step 1629, loss 7868.02, acc 0.09375\n",
      "2016-09-11T09:54:24.058731: step 1630, loss 9590.24, acc 0.0625\n",
      "2016-09-11T09:54:26.084734: step 1631, loss 9986.48, acc 0.125\n",
      "2016-09-11T09:54:28.073066: step 1632, loss 10442.1, acc 0.171875\n",
      "2016-09-11T09:54:30.084554: step 1633, loss 9791.49, acc 0.046875\n",
      "2016-09-11T09:54:32.101780: step 1634, loss 9648.81, acc 0.125\n",
      "2016-09-11T09:54:34.147199: step 1635, loss 9308.44, acc 0.09375\n",
      "2016-09-11T09:54:36.114401: step 1636, loss 12892.3, acc 0.09375\n",
      "2016-09-11T09:54:38.261839: step 1637, loss 10217.4, acc 0.109375\n",
      "2016-09-11T09:54:38.702344: step 1638, loss 4201.09, acc 0.0714286\n",
      "2016-09-11T09:54:40.714232: step 1639, loss 14503.5, acc 0.140625\n",
      "2016-09-11T09:54:42.705815: step 1640, loss 8404.42, acc 0.0625\n",
      "2016-09-11T09:54:44.870658: step 1641, loss 10420.9, acc 0.046875\n",
      "2016-09-11T09:54:47.078476: step 1642, loss 10749.2, acc 0.03125\n",
      "2016-09-11T09:54:49.076539: step 1643, loss 8878.04, acc 0.09375\n",
      "2016-09-11T09:54:51.100083: step 1644, loss 10038, acc 0.140625\n",
      "2016-09-11T09:54:53.459015: step 1645, loss 10771.8, acc 0.140625\n",
      "2016-09-11T09:54:55.472896: step 1646, loss 12317.7, acc 0.125\n",
      "2016-09-11T09:54:57.480154: step 1647, loss 8425.68, acc 0.125\n",
      "2016-09-11T09:54:59.522517: step 1648, loss 10514.2, acc 0.078125\n",
      "2016-09-11T09:55:01.506166: step 1649, loss 11732.7, acc 0.0625\n",
      "2016-09-11T09:55:03.517962: step 1650, loss 8257, acc 0.03125\n",
      "2016-09-11T09:55:04.088974: step 1651, loss 6175.2, acc 0.0714286\n",
      "2016-09-11T09:55:06.100278: step 1652, loss 6675.4, acc 0.1875\n",
      "2016-09-11T09:55:08.355031: step 1653, loss 9247.82, acc 0.0625\n",
      "2016-09-11T09:55:10.311263: step 1654, loss 10754.3, acc 0.078125\n",
      "2016-09-11T09:55:12.465268: step 1655, loss 7374.2, acc 0.125\n",
      "2016-09-11T09:55:14.473778: step 1656, loss 12486.4, acc 0.078125\n",
      "2016-09-11T09:55:16.499591: step 1657, loss 10483, acc 0.03125\n",
      "2016-09-11T09:55:18.701466: step 1658, loss 9095.05, acc 0.125\n",
      "2016-09-11T09:55:20.725119: step 1659, loss 7833.63, acc 0.03125\n",
      "2016-09-11T09:55:22.855422: step 1660, loss 11686.5, acc 0.03125\n",
      "2016-09-11T09:55:24.721859: step 1661, loss 10161.6, acc 0.046875\n",
      "2016-09-11T09:55:26.869886: step 1662, loss 8693.92, acc 0.03125\n",
      "2016-09-11T09:55:28.888046: step 1663, loss 9658.98, acc 0.0625\n",
      "2016-09-11T09:55:29.307374: step 1664, loss 6535.61, acc 0.0714286\n",
      "2016-09-11T09:55:31.282207: step 1665, loss 7309.41, acc 0.078125\n",
      "2016-09-11T09:55:33.285314: step 1666, loss 9153.46, acc 0.171875\n",
      "2016-09-11T09:55:35.285516: step 1667, loss 8094.74, acc 0.15625\n",
      "2016-09-11T09:55:37.279258: step 1668, loss 6876.37, acc 0.171875\n",
      "2016-09-11T09:55:39.489988: step 1669, loss 10813.6, acc 0.0625\n",
      "2016-09-11T09:55:41.479183: step 1670, loss 8056.3, acc 0.109375\n",
      "2016-09-11T09:55:43.518579: step 1671, loss 11004.4, acc 0.078125\n",
      "2016-09-11T09:55:45.494214: step 1672, loss 8527.82, acc 0.078125\n",
      "2016-09-11T09:55:47.530255: step 1673, loss 8785.09, acc 0.046875\n",
      "2016-09-11T09:55:49.658577: step 1674, loss 7798.18, acc 0.140625\n",
      "2016-09-11T09:55:51.688972: step 1675, loss 7232.59, acc 0.09375\n",
      "2016-09-11T09:55:53.680055: step 1676, loss 13328.8, acc 0.109375\n",
      "2016-09-11T09:55:54.091067: step 1677, loss 5243.29, acc 0\n",
      "2016-09-11T09:55:56.286638: step 1678, loss 6199.66, acc 0.0625\n",
      "2016-09-11T09:55:58.294996: step 1679, loss 9322.91, acc 0.125\n",
      "2016-09-11T09:56:00.282345: step 1680, loss 7088.23, acc 0.140625\n",
      "2016-09-11T09:56:02.292006: step 1681, loss 11217, acc 0.140625\n",
      "2016-09-11T09:56:04.461931: step 1682, loss 10679.9, acc 0.078125\n",
      "2016-09-11T09:56:06.475356: step 1683, loss 7427.98, acc 0.0625\n",
      "2016-09-11T09:56:08.486744: step 1684, loss 7839.13, acc 0.09375\n",
      "2016-09-11T09:56:10.471332: step 1685, loss 7649.95, acc 0.046875\n",
      "2016-09-11T09:56:12.497186: step 1686, loss 7614.94, acc 0.140625\n",
      "2016-09-11T09:56:14.511379: step 1687, loss 10378, acc 0.078125\n",
      "2016-09-11T09:56:16.509246: step 1688, loss 8607.37, acc 0.09375\n",
      "2016-09-11T09:56:18.494937: step 1689, loss 9680.08, acc 0.125\n",
      "2016-09-11T09:56:19.068816: step 1690, loss 11855.5, acc 0.142857\n",
      "2016-09-11T09:56:21.116688: step 1691, loss 10267.6, acc 0.109375\n",
      "2016-09-11T09:56:23.297196: step 1692, loss 9883.16, acc 0\n",
      "2016-09-11T09:56:25.479713: step 1693, loss 9113.58, acc 0.109375\n",
      "2016-09-11T09:56:27.482110: step 1694, loss 8515.14, acc 0.109375\n",
      "2016-09-11T09:56:29.543752: step 1695, loss 8960.94, acc 0.046875\n",
      "2016-09-11T09:56:31.492676: step 1696, loss 7601.77, acc 0.0625\n",
      "2016-09-11T09:56:33.501048: step 1697, loss 10101.9, acc 0.0625\n",
      "2016-09-11T09:56:35.508169: step 1698, loss 9767.29, acc 0.109375\n",
      "2016-09-11T09:56:37.474135: step 1699, loss 9911.58, acc 0.03125\n",
      "2016-09-11T09:56:39.479403: step 1700, loss 8431.74, acc 0.046875\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:56:41.457947: step 1700, loss 91170.1, acc 0.168367\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-1700\n",
      "\n",
      "2016-09-11T09:56:44.675817: step 1701, loss 11145.6, acc 0.09375\n",
      "2016-09-11T09:56:46.871514: step 1702, loss 7684.32, acc 0.125\n",
      "2016-09-11T09:56:47.275081: step 1703, loss 6624.48, acc 0.0714286\n",
      "2016-09-11T09:56:49.291941: step 1704, loss 14111.2, acc 0.046875\n",
      "2016-09-11T09:56:51.495064: step 1705, loss 9787.23, acc 0.109375\n",
      "2016-09-11T09:56:53.679012: step 1706, loss 8673.76, acc 0.078125\n",
      "2016-09-11T09:56:55.691117: step 1707, loss 7016.07, acc 0.078125\n",
      "2016-09-11T09:56:57.865196: step 1708, loss 7592.26, acc 0.046875\n",
      "2016-09-11T09:56:59.869677: step 1709, loss 7828.02, acc 0.078125\n",
      "2016-09-11T09:57:01.916530: step 1710, loss 6719.94, acc 0.0625\n",
      "2016-09-11T09:57:04.056072: step 1711, loss 7492.08, acc 0.09375\n",
      "2016-09-11T09:57:06.097895: step 1712, loss 10524.2, acc 0.109375\n",
      "2016-09-11T09:57:08.082072: step 1713, loss 8211.89, acc 0.046875\n",
      "2016-09-11T09:57:10.118691: step 1714, loss 6157.02, acc 0.125\n",
      "2016-09-11T09:57:12.290243: step 1715, loss 10256.8, acc 0.09375\n",
      "2016-09-11T09:57:12.700053: step 1716, loss 6357.93, acc 0\n",
      "2016-09-11T09:57:14.877931: step 1717, loss 10100.9, acc 0.09375\n",
      "2016-09-11T09:57:16.893025: step 1718, loss 8604.62, acc 0.0625\n",
      "2016-09-11T09:57:19.056897: step 1719, loss 12121.5, acc 0.171875\n",
      "2016-09-11T09:57:21.096424: step 1720, loss 15019.5, acc 0.0625\n",
      "2016-09-11T09:57:23.135400: step 1721, loss 7948.45, acc 0.0625\n",
      "2016-09-11T09:57:25.260447: step 1722, loss 8640.55, acc 0.109375\n",
      "2016-09-11T09:57:27.305084: step 1723, loss 8519.64, acc 0.078125\n",
      "2016-09-11T09:57:29.331184: step 1724, loss 6661.17, acc 0.078125\n",
      "2016-09-11T09:57:31.296643: step 1725, loss 8969.22, acc 0.0625\n",
      "2016-09-11T09:57:33.325641: step 1726, loss 9181.77, acc 0.046875\n",
      "2016-09-11T09:57:35.335163: step 1727, loss 7412.81, acc 0.046875\n",
      "2016-09-11T09:57:37.508885: step 1728, loss 6247.92, acc 0.078125\n",
      "2016-09-11T09:57:38.062946: step 1729, loss 17208.7, acc 0\n",
      "2016-09-11T09:57:39.909822: step 1730, loss 6199.07, acc 0.140625\n",
      "2016-09-11T09:57:42.083452: step 1731, loss 8752.44, acc 0.0625\n",
      "2016-09-11T09:57:44.068467: step 1732, loss 10276.4, acc 0.046875\n",
      "2016-09-11T09:57:46.105049: step 1733, loss 11235.6, acc 0.078125\n",
      "2016-09-11T09:57:48.117835: step 1734, loss 8711.09, acc 0.078125\n",
      "2016-09-11T09:57:50.366326: step 1735, loss 13820.3, acc 0.109375\n",
      "2016-09-11T09:57:52.470150: step 1736, loss 8325.62, acc 0.046875\n",
      "2016-09-11T09:57:54.462396: step 1737, loss 10100.6, acc 0.09375\n",
      "2016-09-11T09:57:56.526462: step 1738, loss 7272.51, acc 0.09375\n",
      "2016-09-11T09:57:58.505973: step 1739, loss 11171.7, acc 0.0625\n",
      "2016-09-11T09:58:00.694997: step 1740, loss 10604.7, acc 0.109375\n",
      "2016-09-11T09:58:02.700761: step 1741, loss 7874.79, acc 0.125\n",
      "2016-09-11T09:58:03.265629: step 1742, loss 7353.43, acc 0.214286\n",
      "2016-09-11T09:58:05.322502: step 1743, loss 9800.51, acc 0.21875\n",
      "2016-09-11T09:58:07.367340: step 1744, loss 8686.03, acc 0.109375\n",
      "2016-09-11T09:58:09.498787: step 1745, loss 9214.63, acc 0.015625\n",
      "2016-09-11T09:58:11.504557: step 1746, loss 9627.15, acc 0.09375\n",
      "2016-09-11T09:58:13.706139: step 1747, loss 8508.04, acc 0.171875\n",
      "2016-09-11T09:58:15.696219: step 1748, loss 14399.3, acc 0.15625\n",
      "2016-09-11T09:58:17.703454: step 1749, loss 9776.94, acc 0.109375\n",
      "2016-09-11T09:58:19.899681: step 1750, loss 8113.93, acc 0.125\n",
      "2016-09-11T09:58:22.101066: step 1751, loss 9560.64, acc 0.09375\n",
      "2016-09-11T09:58:24.298321: step 1752, loss 10941.4, acc 0.15625\n",
      "2016-09-11T09:58:26.308566: step 1753, loss 10384.3, acc 0.078125\n",
      "2016-09-11T09:58:28.329668: step 1754, loss 12029, acc 0.03125\n",
      "2016-09-11T09:58:28.915551: step 1755, loss 8314.93, acc 0.142857\n",
      "2016-09-11T09:58:31.112625: step 1756, loss 9602.59, acc 0.09375\n",
      "2016-09-11T09:58:33.102086: step 1757, loss 12627.1, acc 0.15625\n",
      "2016-09-11T09:58:35.274624: step 1758, loss 8714.77, acc 0.125\n",
      "2016-09-11T09:58:37.337944: step 1759, loss 7626.46, acc 0.09375\n",
      "2016-09-11T09:58:39.466531: step 1760, loss 10317, acc 0.09375\n",
      "2016-09-11T09:58:41.460331: step 1761, loss 10893.8, acc 0.078125\n",
      "2016-09-11T09:58:43.484041: step 1762, loss 9664.02, acc 0.046875\n",
      "2016-09-11T09:58:45.501107: step 1763, loss 8331.75, acc 0.21875\n",
      "2016-09-11T09:58:47.525187: step 1764, loss 11132.4, acc 0.140625\n",
      "2016-09-11T09:58:49.697351: step 1765, loss 14024.3, acc 0.078125\n",
      "2016-09-11T09:58:51.734428: step 1766, loss 9446.25, acc 0\n",
      "2016-09-11T09:58:53.699178: step 1767, loss 9340.39, acc 0.109375\n",
      "2016-09-11T09:58:54.269051: step 1768, loss 6957.3, acc 0\n",
      "2016-09-11T09:58:56.294321: step 1769, loss 9649.77, acc 0.109375\n",
      "2016-09-11T09:58:58.291637: step 1770, loss 8240.79, acc 0.09375\n",
      "2016-09-11T09:59:00.317957: step 1771, loss 8982.11, acc 0.078125\n",
      "2016-09-11T09:59:02.322524: step 1772, loss 9374.19, acc 0.140625\n",
      "2016-09-11T09:59:04.515074: step 1773, loss 7751.55, acc 0.109375\n",
      "2016-09-11T09:59:06.669533: step 1774, loss 8888.52, acc 0.0625\n",
      "2016-09-11T09:59:08.773819: step 1775, loss 8481.42, acc 0.09375\n",
      "2016-09-11T09:59:10.733119: step 1776, loss 12360.4, acc 0.03125\n",
      "2016-09-11T09:59:12.832626: step 1777, loss 8851.33, acc 0.125\n",
      "2016-09-11T09:59:14.869759: step 1778, loss 7013.5, acc 0.078125\n",
      "2016-09-11T09:59:16.909253: step 1779, loss 11833.9, acc 0.09375\n",
      "2016-09-11T09:59:18.977908: step 1780, loss 9713.56, acc 0.109375\n",
      "2016-09-11T09:59:19.509217: step 1781, loss 10954.8, acc 0.142857\n",
      "2016-09-11T09:59:21.569405: step 1782, loss 6977.66, acc 0.078125\n",
      "2016-09-11T09:59:23.654638: step 1783, loss 6625.1, acc 0.09375\n",
      "2016-09-11T09:59:25.687638: step 1784, loss 10073.9, acc 0.078125\n",
      "2016-09-11T09:59:27.741439: step 1785, loss 8824.65, acc 0.0625\n",
      "2016-09-11T09:59:29.743639: step 1786, loss 8680.57, acc 0.078125\n",
      "2016-09-11T09:59:31.785892: step 1787, loss 8455.24, acc 0.078125\n",
      "2016-09-11T09:59:33.943679: step 1788, loss 9799.38, acc 0.078125\n",
      "2016-09-11T09:59:35.922585: step 1789, loss 8301.65, acc 0.03125\n",
      "2016-09-11T09:59:37.936495: step 1790, loss 13109.4, acc 0.0625\n",
      "2016-09-11T09:59:40.056739: step 1791, loss 11986.8, acc 0.0625\n",
      "2016-09-11T09:59:42.059577: step 1792, loss 8347.81, acc 0.046875\n",
      "2016-09-11T09:59:44.068439: step 1793, loss 10083.6, acc 0.125\n",
      "2016-09-11T09:59:44.481001: step 1794, loss 8018.45, acc 0.0714286\n",
      "2016-09-11T09:59:46.486162: step 1795, loss 10667.8, acc 0.046875\n",
      "2016-09-11T09:59:48.510855: step 1796, loss 11343.1, acc 0.03125\n",
      "2016-09-11T09:59:50.499185: step 1797, loss 7969.48, acc 0.046875\n",
      "2016-09-11T09:59:52.655733: step 1798, loss 9830.84, acc 0.09375\n",
      "2016-09-11T09:59:54.669974: step 1799, loss 8613.05, acc 0.125\n",
      "2016-09-11T09:59:56.859190: step 1800, loss 12643.8, acc 0.09375\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T09:59:58.741644: step 1800, loss 106591, acc 0.112245\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-1800\n",
      "\n",
      "2016-09-11T10:00:02.310965: step 1801, loss 12384.5, acc 0.140625\n",
      "2016-09-11T10:00:04.493852: step 1802, loss 8417.32, acc 0.15625\n",
      "2016-09-11T10:00:06.477235: step 1803, loss 11217, acc 0.03125\n",
      "2016-09-11T10:00:08.524048: step 1804, loss 10287.4, acc 0.140625\n",
      "2016-09-11T10:00:10.674782: step 1805, loss 11873.6, acc 0.046875\n",
      "2016-09-11T10:00:12.753649: step 1806, loss 10584.3, acc 0.109375\n",
      "2016-09-11T10:00:13.473913: step 1807, loss 4821.43, acc 0.142857\n",
      "2016-09-11T10:00:15.932813: step 1808, loss 10519.3, acc 0\n",
      "2016-09-11T10:00:18.070561: step 1809, loss 12977.1, acc 0.0625\n",
      "2016-09-11T10:00:20.087282: step 1810, loss 7716.11, acc 0.046875\n",
      "2016-09-11T10:00:22.091617: step 1811, loss 11730.9, acc 0.171875\n",
      "2016-09-11T10:00:24.102553: step 1812, loss 10969.2, acc 0.203125\n",
      "2016-09-11T10:00:26.169816: step 1813, loss 10440.8, acc 0.078125\n",
      "2016-09-11T10:00:28.111567: step 1814, loss 10533.3, acc 0.109375\n",
      "2016-09-11T10:00:30.094597: step 1815, loss 12225.3, acc 0.125\n",
      "2016-09-11T10:00:32.132271: step 1816, loss 6570.74, acc 0.046875\n",
      "2016-09-11T10:00:34.102600: step 1817, loss 7911.5, acc 0.109375\n",
      "2016-09-11T10:00:36.110626: step 1818, loss 10444.2, acc 0.078125\n",
      "2016-09-11T10:00:38.287834: step 1819, loss 8984.88, acc 0.0625\n",
      "2016-09-11T10:00:38.698601: step 1820, loss 18922, acc 0\n",
      "2016-09-11T10:00:40.878343: step 1821, loss 9813.04, acc 0.09375\n",
      "2016-09-11T10:00:42.897285: step 1822, loss 7617.27, acc 0.171875\n",
      "2016-09-11T10:00:44.967057: step 1823, loss 8289.36, acc 0.140625\n",
      "2016-09-11T10:00:47.092320: step 1824, loss 11122.3, acc 0.125\n",
      "2016-09-11T10:00:49.294869: step 1825, loss 8749.22, acc 0.109375\n",
      "2016-09-11T10:00:51.471836: step 1826, loss 12177.9, acc 0.09375\n",
      "2016-09-11T10:00:53.664298: step 1827, loss 12070, acc 0.046875\n",
      "2016-09-11T10:00:55.714115: step 1828, loss 6643.6, acc 0.078125\n",
      "2016-09-11T10:00:57.865157: step 1829, loss 9434.72, acc 0.125\n",
      "2016-09-11T10:00:59.884100: step 1830, loss 10368.2, acc 0.0625\n",
      "2016-09-11T10:01:02.056236: step 1831, loss 10379.5, acc 0.0625\n",
      "2016-09-11T10:01:04.093319: step 1832, loss 12912.7, acc 0.046875\n",
      "2016-09-11T10:01:04.862086: step 1833, loss 4327.79, acc 0\n",
      "2016-09-11T10:01:06.909398: step 1834, loss 11317.2, acc 0.078125\n",
      "2016-09-11T10:01:09.092120: step 1835, loss 12018.7, acc 0.0625\n",
      "2016-09-11T10:01:11.273349: step 1836, loss 7729.96, acc 0.109375\n",
      "2016-09-11T10:01:13.455122: step 1837, loss 8502.45, acc 0.09375\n",
      "2016-09-11T10:01:15.484088: step 1838, loss 7367.96, acc 0.109375\n",
      "2016-09-11T10:01:17.671442: step 1839, loss 10002.6, acc 0.078125\n",
      "2016-09-11T10:01:19.706856: step 1840, loss 12737.2, acc 0.15625\n",
      "2016-09-11T10:01:21.692870: step 1841, loss 12226.6, acc 0.109375\n",
      "2016-09-11T10:01:23.875475: step 1842, loss 12298.1, acc 0.046875\n",
      "2016-09-11T10:01:25.886711: step 1843, loss 10733.7, acc 0.0625\n",
      "2016-09-11T10:01:28.062628: step 1844, loss 11034.1, acc 0.078125\n",
      "2016-09-11T10:01:30.096815: step 1845, loss 13996.2, acc 0.046875\n",
      "2016-09-11T10:01:30.676637: step 1846, loss 12376.8, acc 0\n",
      "2016-09-11T10:01:32.684836: step 1847, loss 11720.9, acc 0.078125\n",
      "2016-09-11T10:01:34.711185: step 1848, loss 11471, acc 0.125\n",
      "2016-09-11T10:01:36.879010: step 1849, loss 9292.97, acc 0.09375\n",
      "2016-09-11T10:01:39.056239: step 1850, loss 12616.8, acc 0.0625\n",
      "2016-09-11T10:01:41.096456: step 1851, loss 12686.1, acc 0.046875\n",
      "2016-09-11T10:01:43.262972: step 1852, loss 7840.81, acc 0.046875\n",
      "2016-09-11T10:01:45.477583: step 1853, loss 11283.7, acc 0.140625\n",
      "2016-09-11T10:01:47.486381: step 1854, loss 10438.6, acc 0.0625\n",
      "2016-09-11T10:01:49.493099: step 1855, loss 10042.9, acc 0.078125\n",
      "2016-09-11T10:01:51.497663: step 1856, loss 14724.9, acc 0.078125\n",
      "2016-09-11T10:01:53.501042: step 1857, loss 11034.1, acc 0.09375\n",
      "2016-09-11T10:01:55.657701: step 1858, loss 7485, acc 0.1875\n",
      "2016-09-11T10:01:56.087170: step 1859, loss 11297.9, acc 0.0714286\n",
      "2016-09-11T10:01:58.101506: step 1860, loss 13820.7, acc 0.125\n",
      "2016-09-11T10:02:00.285018: step 1861, loss 11550.9, acc 0.078125\n",
      "2016-09-11T10:02:02.275111: step 1862, loss 11261.7, acc 0.046875\n",
      "2016-09-11T10:02:04.294877: step 1863, loss 6599.39, acc 0.140625\n",
      "2016-09-11T10:02:06.475508: step 1864, loss 9289.47, acc 0.171875\n",
      "2016-09-11T10:02:08.486458: step 1865, loss 11002.8, acc 0.15625\n",
      "2016-09-11T10:02:10.867527: step 1866, loss 13061.5, acc 0.140625\n",
      "2016-09-11T10:02:12.925329: step 1867, loss 9238.31, acc 0.109375\n",
      "2016-09-11T10:02:15.057850: step 1868, loss 9468.65, acc 0.125\n",
      "2016-09-11T10:02:17.099507: step 1869, loss 13503.2, acc 0.078125\n",
      "2016-09-11T10:02:19.094709: step 1870, loss 9111.77, acc 0.171875\n",
      "2016-09-11T10:02:21.087633: step 1871, loss 8831.82, acc 0.046875\n",
      "2016-09-11T10:02:21.676317: step 1872, loss 5910.5, acc 0.0714286\n",
      "2016-09-11T10:02:23.685429: step 1873, loss 9817.37, acc 0.09375\n",
      "2016-09-11T10:02:25.860766: step 1874, loss 11000.3, acc 0.21875\n",
      "2016-09-11T10:02:27.877771: step 1875, loss 9998.32, acc 0.15625\n",
      "2016-09-11T10:02:29.884542: step 1876, loss 12195.6, acc 0.0625\n",
      "2016-09-11T10:02:31.883961: step 1877, loss 14124.1, acc 0.03125\n",
      "2016-09-11T10:02:34.071164: step 1878, loss 9011.04, acc 0.09375\n",
      "2016-09-11T10:02:36.094812: step 1879, loss 7904.8, acc 0.046875\n",
      "2016-09-11T10:02:38.083770: step 1880, loss 9798.42, acc 0.09375\n",
      "2016-09-11T10:02:40.084085: step 1881, loss 12925.7, acc 0.0625\n",
      "2016-09-11T10:02:42.264530: step 1882, loss 10834.6, acc 0.03125\n",
      "2016-09-11T10:02:44.260103: step 1883, loss 9113.19, acc 0.09375\n",
      "2016-09-11T10:02:46.265976: step 1884, loss 10988.2, acc 0.15625\n",
      "2016-09-11T10:02:46.861876: step 1885, loss 6713.36, acc 0\n",
      "2016-09-11T10:02:48.690423: step 1886, loss 13201.7, acc 0.03125\n",
      "2016-09-11T10:02:50.856591: step 1887, loss 9836.63, acc 0.078125\n",
      "2016-09-11T10:02:52.862589: step 1888, loss 7859.88, acc 0.078125\n",
      "2016-09-11T10:02:54.657551: step 1889, loss 9745.18, acc 0.0625\n",
      "2016-09-11T10:02:56.060408: step 1890, loss 10112.9, acc 0.171875\n",
      "2016-09-11T10:02:57.289945: step 1891, loss 9320.33, acc 0.125\n",
      "2016-09-11T10:02:59.111550: step 1892, loss 13950.3, acc 0.046875\n",
      "2016-09-11T10:03:01.111176: step 1893, loss 10855.6, acc 0.09375\n",
      "2016-09-11T10:03:03.115443: step 1894, loss 8790.99, acc 0.0625\n",
      "2016-09-11T10:03:05.293098: step 1895, loss 9445.93, acc 0.078125\n",
      "2016-09-11T10:03:07.277140: step 1896, loss 11640.6, acc 0.078125\n",
      "2016-09-11T10:03:09.461793: step 1897, loss 10322.8, acc 0.03125\n",
      "2016-09-11T10:03:09.914807: step 1898, loss 9822.86, acc 0.142857\n",
      "2016-09-11T10:03:12.059180: step 1899, loss 10736.8, acc 0.0625\n",
      "2016-09-11T10:03:14.056089: step 1900, loss 7203.06, acc 0.078125\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T10:03:15.900646: step 1900, loss 121409, acc 0.168367\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-1900\n",
      "\n",
      "2016-09-11T10:03:18.887682: step 1901, loss 11413.9, acc 0.078125\n",
      "2016-09-11T10:03:20.910225: step 1902, loss 10612.5, acc 0.046875\n",
      "2016-09-11T10:03:23.070658: step 1903, loss 9721.66, acc 0.078125\n",
      "2016-09-11T10:03:25.071864: step 1904, loss 12186.6, acc 0.125\n",
      "2016-09-11T10:03:27.061353: step 1905, loss 8101.47, acc 0.078125\n",
      "2016-09-11T10:03:28.886933: step 1906, loss 10958.3, acc 0.09375\n",
      "2016-09-11T10:03:31.066540: step 1907, loss 10740.1, acc 0.140625\n",
      "2016-09-11T10:03:33.080399: step 1908, loss 9869.47, acc 0.140625\n",
      "2016-09-11T10:03:35.086659: step 1909, loss 12785.4, acc 0.09375\n",
      "2016-09-11T10:03:37.097244: step 1910, loss 12490.3, acc 0.15625\n",
      "2016-09-11T10:03:37.672891: step 1911, loss 20503, acc 0.142857\n",
      "2016-09-11T10:03:39.680259: step 1912, loss 10135.9, acc 0.0625\n",
      "2016-09-11T10:03:41.683543: step 1913, loss 9249.12, acc 0.09375\n",
      "2016-09-11T10:03:43.858437: step 1914, loss 13314.8, acc 0.03125\n",
      "2016-09-11T10:03:45.685216: step 1915, loss 10943.5, acc 0.078125\n",
      "2016-09-11T10:03:47.687849: step 1916, loss 12567.8, acc 0.078125\n",
      "2016-09-11T10:03:49.684461: step 1917, loss 10564.7, acc 0.140625\n",
      "2016-09-11T10:03:51.704585: step 1918, loss 17353.2, acc 0.078125\n",
      "2016-09-11T10:03:53.710422: step 1919, loss 9355.07, acc 0.078125\n",
      "2016-09-11T10:03:55.700580: step 1920, loss 9811.48, acc 0.0625\n",
      "2016-09-11T10:03:57.703163: step 1921, loss 9348.49, acc 0.125\n",
      "2016-09-11T10:03:59.705315: step 1922, loss 9449.52, acc 0.078125\n",
      "2016-09-11T10:04:01.702686: step 1923, loss 10540.7, acc 0.0625\n",
      "2016-09-11T10:04:02.274812: step 1924, loss 6114.16, acc 0\n",
      "2016-09-11T10:04:04.479760: step 1925, loss 9197.91, acc 0.09375\n",
      "2016-09-11T10:04:06.537375: step 1926, loss 11619.2, acc 0.078125\n",
      "2016-09-11T10:04:08.692375: step 1927, loss 9242.86, acc 0.125\n",
      "2016-09-11T10:04:10.691220: step 1928, loss 13094.6, acc 0.109375\n",
      "2016-09-11T10:04:12.714260: step 1929, loss 10260.6, acc 0.125\n",
      "2016-09-11T10:04:14.687332: step 1930, loss 13207.5, acc 0.140625\n",
      "2016-09-11T10:04:16.686704: step 1931, loss 10059.8, acc 0.09375\n",
      "2016-09-11T10:04:18.728308: step 1932, loss 9652.99, acc 0.15625\n",
      "2016-09-11T10:04:20.714799: step 1933, loss 10093.9, acc 0.125\n",
      "2016-09-11T10:04:22.917285: step 1934, loss 7587.69, acc 0.09375\n",
      "2016-09-11T10:04:24.889215: step 1935, loss 13364.1, acc 0.109375\n",
      "2016-09-11T10:04:26.906612: step 1936, loss 14012.6, acc 0.015625\n",
      "2016-09-11T10:04:27.476868: step 1937, loss 10748.7, acc 0\n",
      "2016-09-11T10:04:29.533526: step 1938, loss 13749.7, acc 0.125\n",
      "2016-09-11T10:04:31.556775: step 1939, loss 10639.1, acc 0.109375\n",
      "2016-09-11T10:04:33.784154: step 1940, loss 10757, acc 0.0625\n",
      "2016-09-11T10:04:35.745738: step 1941, loss 11668.5, acc 0.0625\n",
      "2016-09-11T10:04:37.724602: step 1942, loss 12761.8, acc 0.109375\n",
      "2016-09-11T10:04:39.877813: step 1943, loss 12019.8, acc 0\n",
      "2016-09-11T10:04:41.914542: step 1944, loss 10586.5, acc 0.109375\n",
      "2016-09-11T10:04:44.098491: step 1945, loss 11359.8, acc 0.125\n",
      "2016-09-11T10:04:46.283864: step 1946, loss 9781.32, acc 0.09375\n",
      "2016-09-11T10:04:48.315724: step 1947, loss 8831.8, acc 0.125\n",
      "2016-09-11T10:04:50.466888: step 1948, loss 13229.1, acc 0.09375\n",
      "2016-09-11T10:04:52.715532: step 1949, loss 15243.3, acc 0.125\n",
      "2016-09-11T10:04:53.259548: step 1950, loss 19743.8, acc 0.0714286\n",
      "2016-09-11T10:04:55.305220: step 1951, loss 9065.47, acc 0.09375\n",
      "2016-09-11T10:04:57.337913: step 1952, loss 17090.6, acc 0.03125\n",
      "2016-09-11T10:04:59.478859: step 1953, loss 8914.16, acc 0.078125\n",
      "2016-09-11T10:05:01.495619: step 1954, loss 12041.4, acc 0.0625\n",
      "2016-09-11T10:05:03.589698: step 1955, loss 12770.4, acc 0.125\n",
      "2016-09-11T10:05:05.522920: step 1956, loss 10608.9, acc 0.0625\n",
      "2016-09-11T10:05:07.509319: step 1957, loss 9034.94, acc 0.09375\n",
      "2016-09-11T10:05:09.478741: step 1958, loss 9356.88, acc 0.09375\n",
      "2016-09-11T10:05:11.492170: step 1959, loss 10578.2, acc 0.046875\n",
      "2016-09-11T10:05:13.727516: step 1960, loss 11262.3, acc 0.046875\n",
      "2016-09-11T10:05:15.720500: step 1961, loss 10795.7, acc 0.078125\n",
      "2016-09-11T10:05:17.857549: step 1962, loss 8293.03, acc 0.1875\n",
      "2016-09-11T10:05:18.300953: step 1963, loss 15271.7, acc 0.214286\n",
      "2016-09-11T10:05:20.302728: step 1964, loss 10922.8, acc 0.125\n",
      "2016-09-11T10:05:22.319348: step 1965, loss 11302.6, acc 0.09375\n",
      "2016-09-11T10:05:24.471516: step 1966, loss 9552.23, acc 0.0625\n",
      "2016-09-11T10:05:26.496930: step 1967, loss 11056.7, acc 0.078125\n",
      "2016-09-11T10:05:28.504645: step 1968, loss 12790.6, acc 0.109375\n",
      "2016-09-11T10:05:30.512314: step 1969, loss 9159.32, acc 0.03125\n",
      "2016-09-11T10:05:32.500770: step 1970, loss 12898, acc 0.09375\n",
      "2016-09-11T10:05:34.499830: step 1971, loss 9412.82, acc 0.09375\n",
      "2016-09-11T10:05:36.479540: step 1972, loss 13252, acc 0.09375\n",
      "2016-09-11T10:05:38.491196: step 1973, loss 12515.4, acc 0.046875\n",
      "2016-09-11T10:05:40.501435: step 1974, loss 11770, acc 0.09375\n",
      "2016-09-11T10:05:42.682920: step 1975, loss 9052.36, acc 0.109375\n",
      "2016-09-11T10:05:43.273604: step 1976, loss 12242.4, acc 0.0714286\n",
      "2016-09-11T10:05:45.458098: step 1977, loss 12117.5, acc 0.140625\n",
      "2016-09-11T10:05:47.487079: step 1978, loss 14396.4, acc 0.046875\n",
      "2016-09-11T10:05:49.480573: step 1979, loss 9143.45, acc 0.09375\n",
      "2016-09-11T10:05:51.480471: step 1980, loss 16545.8, acc 0.0625\n",
      "2016-09-11T10:05:53.486932: step 1981, loss 14416.6, acc 0.109375\n",
      "2016-09-11T10:05:55.489350: step 1982, loss 10686.2, acc 0.046875\n",
      "2016-09-11T10:05:57.660479: step 1983, loss 9773.7, acc 0.09375\n",
      "2016-09-11T10:05:59.677801: step 1984, loss 9451.72, acc 0.140625\n",
      "2016-09-11T10:06:01.689908: step 1985, loss 11839.8, acc 0.125\n",
      "2016-09-11T10:06:03.687287: step 1986, loss 7609.88, acc 0.078125\n",
      "2016-09-11T10:06:05.710940: step 1987, loss 12970.1, acc 0.046875\n",
      "2016-09-11T10:06:07.727525: step 1988, loss 13516.6, acc 0.03125\n",
      "2016-09-11T10:06:08.276155: step 1989, loss 8793.68, acc 0\n",
      "2016-09-11T10:06:10.090689: step 1990, loss 10913.6, acc 0.109375\n",
      "2016-09-11T10:06:12.260855: step 1991, loss 12328.5, acc 0.0625\n",
      "2016-09-11T10:06:14.278356: step 1992, loss 12307.7, acc 0.15625\n",
      "2016-09-11T10:06:16.307139: step 1993, loss 14962.2, acc 0.125\n",
      "2016-09-11T10:06:18.278695: step 1994, loss 9691.85, acc 0.078125\n",
      "2016-09-11T10:06:20.307494: step 1995, loss 16466, acc 0.03125\n",
      "2016-09-11T10:06:22.310137: step 1996, loss 14240.2, acc 0.078125\n",
      "2016-09-11T10:06:24.280415: step 1997, loss 12031.4, acc 0.125\n",
      "2016-09-11T10:06:26.296069: step 1998, loss 8366.09, acc 0.125\n",
      "2016-09-11T10:06:28.334384: step 1999, loss 10865.9, acc 0.09375\n",
      "2016-09-11T10:06:30.297369: step 2000, loss 11054.5, acc 0.0625\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T10:06:32.297987: step 2000, loss 139340, acc 0.0816327\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-2000\n",
      "\n",
      "2016-09-11T10:06:35.697154: step 2001, loss 12031.6, acc 0.0625\n",
      "2016-09-11T10:06:36.304841: step 2002, loss 10460.5, acc 0.0714286\n",
      "2016-09-11T10:06:38.701392: step 2003, loss 11825.8, acc 0\n",
      "2016-09-11T10:06:40.881196: step 2004, loss 12264.4, acc 0.078125\n",
      "2016-09-11T10:06:42.903734: step 2005, loss 10221.9, acc 0.140625\n",
      "2016-09-11T10:06:45.066755: step 2006, loss 10582.4, acc 0.03125\n",
      "2016-09-11T10:06:47.080235: step 2007, loss 11915, acc 0.078125\n",
      "2016-09-11T10:06:49.090976: step 2008, loss 15273.9, acc 0.046875\n",
      "2016-09-11T10:06:51.265234: step 2009, loss 11409, acc 0.109375\n",
      "2016-09-11T10:06:53.295296: step 2010, loss 16246.7, acc 0.0625\n",
      "2016-09-11T10:06:55.291653: step 2011, loss 17234.7, acc 0.09375\n",
      "2016-09-11T10:06:57.507876: step 2012, loss 14131.9, acc 0.078125\n",
      "2016-09-11T10:06:59.695586: step 2013, loss 15434.3, acc 0.015625\n",
      "2016-09-11T10:07:01.696992: step 2014, loss 10174.6, acc 0.078125\n",
      "2016-09-11T10:07:02.282731: step 2015, loss 16110, acc 0.214286\n",
      "2016-09-11T10:07:04.455817: step 2016, loss 15967.7, acc 0.09375\n",
      "2016-09-11T10:07:06.516048: step 2017, loss 10204, acc 0.15625\n",
      "2016-09-11T10:07:08.680015: step 2018, loss 15354.5, acc 0.03125\n",
      "2016-09-11T10:07:10.719991: step 2019, loss 10014.1, acc 0.046875\n",
      "2016-09-11T10:07:12.712423: step 2020, loss 15376.9, acc 0.046875\n",
      "2016-09-11T10:07:14.864682: step 2021, loss 9994.69, acc 0.125\n",
      "2016-09-11T10:07:17.091819: step 2022, loss 22384.4, acc 0.109375\n",
      "2016-09-11T10:07:19.143917: step 2023, loss 10106.4, acc 0.09375\n",
      "2016-09-11T10:07:21.089018: step 2024, loss 11895.1, acc 0.171875\n",
      "2016-09-11T10:07:23.113533: step 2025, loss 9190.36, acc 0.0625\n",
      "2016-09-11T10:07:25.125722: step 2026, loss 13484, acc 0.09375\n",
      "2016-09-11T10:07:27.260126: step 2027, loss 13080.4, acc 0.078125\n",
      "2016-09-11T10:07:27.723788: step 2028, loss 10731.9, acc 0.142857\n",
      "2016-09-11T10:07:29.858401: step 2029, loss 12990.5, acc 0.046875\n",
      "2016-09-11T10:07:31.881241: step 2030, loss 13146.9, acc 0.046875\n",
      "2016-09-11T10:07:33.888874: step 2031, loss 12317.9, acc 0.09375\n",
      "2016-09-11T10:07:35.720534: step 2032, loss 10679.1, acc 0.03125\n",
      "2016-09-11T10:07:37.872434: step 2033, loss 14305.4, acc 0.109375\n",
      "2016-09-11T10:07:39.899690: step 2034, loss 9648.38, acc 0.140625\n",
      "2016-09-11T10:07:41.870894: step 2035, loss 16446.3, acc 0.125\n",
      "2016-09-11T10:07:43.877788: step 2036, loss 11543, acc 0.046875\n",
      "2016-09-11T10:07:45.878070: step 2037, loss 7958.15, acc 0.09375\n",
      "2016-09-11T10:07:47.881097: step 2038, loss 12681.7, acc 0.140625\n",
      "2016-09-11T10:07:49.905783: step 2039, loss 9713.55, acc 0.0625\n",
      "2016-09-11T10:07:51.946444: step 2040, loss 11415.5, acc 0.09375\n",
      "2016-09-11T10:07:52.530023: step 2041, loss 12025, acc 0\n",
      "2016-09-11T10:07:54.513109: step 2042, loss 13622.7, acc 0.078125\n",
      "2016-09-11T10:07:56.657896: step 2043, loss 8526.52, acc 0.0625\n",
      "2016-09-11T10:07:58.694962: step 2044, loss 7568.73, acc 0.171875\n",
      "2016-09-11T10:08:00.779950: step 2045, loss 13056.3, acc 0.140625\n",
      "2016-09-11T10:08:02.867393: step 2046, loss 13835.8, acc 0.046875\n",
      "2016-09-11T10:08:04.732480: step 2047, loss 13825.8, acc 0.046875\n",
      "2016-09-11T10:08:06.692604: step 2048, loss 12515.5, acc 0.078125\n",
      "2016-09-11T10:08:08.862406: step 2049, loss 13220.6, acc 0.078125\n",
      "2016-09-11T10:08:10.881624: step 2050, loss 7521.18, acc 0.125\n",
      "2016-09-11T10:08:12.880820: step 2051, loss 12969.5, acc 0.0625\n",
      "2016-09-11T10:08:14.885757: step 2052, loss 11250.5, acc 0.0625\n",
      "2016-09-11T10:08:16.919566: step 2053, loss 7381.17, acc 0.09375\n",
      "2016-09-11T10:08:17.483044: step 2054, loss 6219.82, acc 0.142857\n",
      "2016-09-11T10:08:19.490856: step 2055, loss 10672.5, acc 0.109375\n",
      "2016-09-11T10:08:21.505207: step 2056, loss 13138.8, acc 0.0625\n",
      "2016-09-11T10:08:23.525512: step 2057, loss 14466.2, acc 0.078125\n",
      "2016-09-11T10:08:25.654886: step 2058, loss 14386.9, acc 0.0625\n",
      "2016-09-11T10:08:27.684410: step 2059, loss 11768.7, acc 0.125\n",
      "2016-09-11T10:08:29.716973: step 2060, loss 11156.5, acc 0.078125\n",
      "2016-09-11T10:08:31.683841: step 2061, loss 9574.63, acc 0.0625\n",
      "2016-09-11T10:08:33.685304: step 2062, loss 14754.9, acc 0.03125\n",
      "2016-09-11T10:08:35.688033: step 2063, loss 10873.3, acc 0.125\n",
      "2016-09-11T10:08:37.729566: step 2064, loss 14311.7, acc 0.078125\n",
      "2016-09-11T10:08:39.675190: step 2065, loss 12825.7, acc 0.0625\n",
      "2016-09-11T10:08:41.670429: step 2066, loss 15198, acc 0.078125\n",
      "2016-09-11T10:08:42.117384: step 2067, loss 9732.07, acc 0.0714286\n",
      "2016-09-11T10:08:44.120771: step 2068, loss 8962.98, acc 0.0625\n",
      "2016-09-11T10:08:46.274028: step 2069, loss 14317.6, acc 0.03125\n",
      "2016-09-11T10:08:48.292815: step 2070, loss 9431.05, acc 0.03125\n",
      "2016-09-11T10:08:50.334918: step 2071, loss 8962.46, acc 0.078125\n",
      "2016-09-11T10:08:52.455552: step 2072, loss 9874.12, acc 0.15625\n",
      "2016-09-11T10:08:54.337293: step 2073, loss 13300.2, acc 0.046875\n",
      "2016-09-11T10:08:56.298382: step 2074, loss 11607.7, acc 0.109375\n",
      "2016-09-11T10:08:58.492117: step 2075, loss 9377.18, acc 0.125\n",
      "2016-09-11T10:09:00.548991: step 2076, loss 12684.8, acc 0.15625\n",
      "2016-09-11T10:09:02.555458: step 2077, loss 15798.7, acc 0.078125\n",
      "2016-09-11T10:09:04.670243: step 2078, loss 14352.6, acc 0.0625\n",
      "2016-09-11T10:09:06.706464: step 2079, loss 7234.01, acc 0.15625\n",
      "2016-09-11T10:09:07.288136: step 2080, loss 11902.9, acc 0.0714286\n",
      "2016-09-11T10:09:09.527969: step 2081, loss 15051.5, acc 0.046875\n",
      "2016-09-11T10:09:11.695791: step 2082, loss 14634, acc 0.0625\n",
      "2016-09-11T10:09:13.710816: step 2083, loss 7658.79, acc 0.109375\n",
      "2016-09-11T10:09:15.696161: step 2084, loss 12281.3, acc 0.046875\n",
      "2016-09-11T10:09:17.688633: step 2085, loss 10094.5, acc 0.078125\n",
      "2016-09-11T10:09:19.700062: step 2086, loss 12429.8, acc 0.03125\n",
      "2016-09-11T10:09:21.862543: step 2087, loss 11424.9, acc 0.09375\n",
      "2016-09-11T10:09:23.704581: step 2088, loss 13497.7, acc 0.15625\n",
      "2016-09-11T10:09:25.864105: step 2089, loss 12666, acc 0.109375\n",
      "2016-09-11T10:09:27.702217: step 2090, loss 11537.9, acc 0.15625\n",
      "2016-09-11T10:09:29.747089: step 2091, loss 11329.4, acc 0.140625\n",
      "2016-09-11T10:09:31.875442: step 2092, loss 15960.1, acc 0.109375\n",
      "2016-09-11T10:09:32.286705: step 2093, loss 10944.6, acc 0.142857\n",
      "2016-09-11T10:09:34.460825: step 2094, loss 13080.3, acc 0.140625\n",
      "2016-09-11T10:09:36.509214: step 2095, loss 11136.9, acc 0.109375\n",
      "2016-09-11T10:09:38.503823: step 2096, loss 11351.9, acc 0.0625\n",
      "2016-09-11T10:09:40.496502: step 2097, loss 14705.7, acc 0.0625\n",
      "2016-09-11T10:09:42.493627: step 2098, loss 14274.4, acc 0.078125\n",
      "2016-09-11T10:09:44.483357: step 2099, loss 12077.8, acc 0.078125\n",
      "2016-09-11T10:09:46.655129: step 2100, loss 14966.9, acc 0.046875\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T10:09:48.517343: step 2100, loss 157548, acc 0.0969388\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-2100\n",
      "\n",
      "2016-09-11T10:09:52.472890: step 2101, loss 12302.5, acc 0.125\n",
      "2016-09-11T10:09:54.472025: step 2102, loss 10187.1, acc 0.15625\n",
      "2016-09-11T10:09:56.500177: step 2103, loss 13772.8, acc 0.125\n",
      "2016-09-11T10:09:58.487202: step 2104, loss 12167.7, acc 0.078125\n",
      "2016-09-11T10:10:00.680720: step 2105, loss 9736.4, acc 0.046875\n",
      "2016-09-11T10:10:01.094037: step 2106, loss 17534, acc 0.214286\n",
      "2016-09-11T10:10:03.114143: step 2107, loss 14852.8, acc 0.03125\n",
      "2016-09-11T10:10:05.089499: step 2108, loss 12826.3, acc 0.140625\n",
      "2016-09-11T10:10:07.102898: step 2109, loss 11273, acc 0.078125\n",
      "2016-09-11T10:10:09.266663: step 2110, loss 16015.1, acc 0.09375\n",
      "2016-09-11T10:10:11.290920: step 2111, loss 11161.2, acc 0.125\n",
      "2016-09-11T10:10:13.287344: step 2112, loss 11409.4, acc 0.125\n",
      "2016-09-11T10:10:15.480543: step 2113, loss 11355.3, acc 0.109375\n",
      "2016-09-11T10:10:18.059398: step 2114, loss 11864.8, acc 0.15625\n",
      "2016-09-11T10:10:20.150493: step 2115, loss 9941.1, acc 0.0625\n",
      "2016-09-11T10:10:22.347762: step 2116, loss 11178.3, acc 0.03125\n",
      "2016-09-11T10:10:24.488524: step 2117, loss 10519.2, acc 0.15625\n",
      "2016-09-11T10:10:26.575599: step 2118, loss 13854.2, acc 0.0625\n",
      "2016-09-11T10:10:27.107397: step 2119, loss 5888.68, acc 0.142857\n",
      "2016-09-11T10:10:29.466032: step 2120, loss 12586.3, acc 0.109375\n",
      "2016-09-11T10:10:31.515915: step 2121, loss 15555, acc 0.078125\n",
      "2016-09-11T10:10:33.746963: step 2122, loss 12219.9, acc 0.109375\n",
      "2016-09-11T10:10:35.755973: step 2123, loss 9493.8, acc 0.140625\n",
      "2016-09-11T10:10:37.911482: step 2124, loss 12772.3, acc 0.0625\n",
      "2016-09-11T10:10:39.911517: step 2125, loss 10105.8, acc 0.078125\n",
      "2016-09-11T10:10:42.065195: step 2126, loss 12948.5, acc 0.046875\n",
      "2016-09-11T10:10:44.071445: step 2127, loss 11658.5, acc 0.078125\n",
      "2016-09-11T10:10:46.133225: step 2128, loss 13131.2, acc 0.046875\n",
      "2016-09-11T10:10:48.577981: step 2129, loss 12662.1, acc 0.125\n",
      "2016-09-11T10:10:50.781271: step 2130, loss 17244.2, acc 0.015625\n",
      "2016-09-11T10:10:53.018212: step 2131, loss 13842, acc 0.03125\n",
      "2016-09-11T10:10:53.510149: step 2132, loss 14924.6, acc 0.214286\n",
      "2016-09-11T10:10:55.931166: step 2133, loss 11365.2, acc 0.09375\n",
      "2016-09-11T10:10:58.289267: step 2134, loss 16390.5, acc 0.0625\n",
      "2016-09-11T10:11:00.484178: step 2135, loss 15731.1, acc 0.109375\n",
      "2016-09-11T10:11:02.503828: step 2136, loss 16247.6, acc 0.09375\n",
      "2016-09-11T10:11:04.676025: step 2137, loss 10415.6, acc 0.109375\n",
      "2016-09-11T10:11:06.696854: step 2138, loss 13671.5, acc 0.078125\n",
      "2016-09-11T10:11:08.748593: step 2139, loss 13842.4, acc 0.0625\n",
      "2016-09-11T10:11:10.738599: step 2140, loss 12590, acc 0.09375\n",
      "2016-09-11T10:11:12.877343: step 2141, loss 14729.3, acc 0.109375\n",
      "2016-09-11T10:11:14.882140: step 2142, loss 12273.8, acc 0.078125\n",
      "2016-09-11T10:11:16.907060: step 2143, loss 19638.3, acc 0.125\n",
      "2016-09-11T10:11:19.091331: step 2144, loss 14342.3, acc 0.109375\n",
      "2016-09-11T10:11:19.538654: step 2145, loss 16718.3, acc 0\n",
      "2016-09-11T10:11:21.694631: step 2146, loss 10542.2, acc 0.09375\n",
      "2016-09-11T10:11:23.923463: step 2147, loss 16949.5, acc 0.109375\n",
      "2016-09-11T10:11:26.114856: step 2148, loss 11813.2, acc 0.046875\n",
      "2016-09-11T10:11:28.134089: step 2149, loss 14273.8, acc 0.109375\n",
      "2016-09-11T10:11:30.284703: step 2150, loss 13134, acc 0.109375\n",
      "2016-09-11T10:11:32.306312: step 2151, loss 11219.8, acc 0.15625\n",
      "2016-09-11T10:11:34.645219: step 2152, loss 12199, acc 0.109375\n",
      "2016-09-11T10:11:36.752724: step 2153, loss 14561.3, acc 0.0625\n",
      "2016-09-11T10:11:38.799592: step 2154, loss 19114.4, acc 0.109375\n",
      "2016-09-11T10:11:41.046364: step 2155, loss 15261.7, acc 0.078125\n",
      "2016-09-11T10:11:43.346392: step 2156, loss 12455, acc 0.046875\n",
      "2016-09-11T10:11:45.507702: step 2157, loss 14216.4, acc 0.140625\n",
      "2016-09-11T10:11:46.060737: step 2158, loss 13855.9, acc 0.142857\n",
      "2016-09-11T10:11:48.104186: step 2159, loss 15854.3, acc 0.078125\n",
      "2016-09-11T10:11:50.102015: step 2160, loss 12311.5, acc 0.140625\n",
      "2016-09-11T10:11:52.175611: step 2161, loss 17364.6, acc 0.109375\n",
      "2016-09-11T10:11:54.143854: step 2162, loss 12068.3, acc 0.0625\n",
      "2016-09-11T10:11:56.292895: step 2163, loss 9281.95, acc 0.078125\n",
      "2016-09-11T10:11:58.288294: step 2164, loss 10129.3, acc 0.046875\n",
      "2016-09-11T10:12:00.290083: step 2165, loss 15228.6, acc 0.140625\n",
      "2016-09-11T10:12:02.309796: step 2166, loss 14548.9, acc 0.046875\n",
      "2016-09-11T10:12:04.620161: step 2167, loss 14746.7, acc 0.09375\n",
      "2016-09-11T10:12:06.547302: step 2168, loss 12111.6, acc 0.046875\n",
      "2016-09-11T10:12:08.701821: step 2169, loss 17120.1, acc 0.140625\n",
      "2016-09-11T10:12:10.742353: step 2170, loss 16513.8, acc 0.0625\n",
      "2016-09-11T10:12:11.297956: step 2171, loss 6409.32, acc 0.285714\n",
      "2016-09-11T10:12:13.287904: step 2172, loss 14038.1, acc 0.078125\n",
      "2016-09-11T10:12:15.301213: step 2173, loss 17327, acc 0.0625\n",
      "2016-09-11T10:12:17.501958: step 2174, loss 11775.6, acc 0.015625\n",
      "2016-09-11T10:12:19.659988: step 2175, loss 12029.6, acc 0.03125\n",
      "2016-09-11T10:12:21.707903: step 2176, loss 10779, acc 0.078125\n",
      "2016-09-11T10:12:23.685562: step 2177, loss 13002.4, acc 0.125\n",
      "2016-09-11T10:12:25.899914: step 2178, loss 16837.3, acc 0.140625\n",
      "2016-09-11T10:12:27.921033: step 2179, loss 11228.8, acc 0.046875\n",
      "2016-09-11T10:12:30.094490: step 2180, loss 16781.5, acc 0.0625\n",
      "2016-09-11T10:12:32.123152: step 2181, loss 10810.2, acc 0.0625\n",
      "2016-09-11T10:12:34.310989: step 2182, loss 14525.3, acc 0.03125\n",
      "2016-09-11T10:12:36.459903: step 2183, loss 12946.3, acc 0.125\n",
      "2016-09-11T10:12:36.914626: step 2184, loss 11839.1, acc 0.0714286\n",
      "2016-09-11T10:12:39.074712: step 2185, loss 13567.9, acc 0.15625\n",
      "2016-09-11T10:12:41.254219: step 2186, loss 15411.7, acc 0.0625\n",
      "2016-09-11T10:12:43.329804: step 2187, loss 10425.3, acc 0.140625\n",
      "2016-09-11T10:12:45.376178: step 2188, loss 12835.3, acc 0.09375\n",
      "2016-09-11T10:12:47.475485: step 2189, loss 20167.6, acc 0.078125\n",
      "2016-09-11T10:12:49.515378: step 2190, loss 18741.4, acc 0.03125\n",
      "2016-09-11T10:12:51.655574: step 2191, loss 11268.6, acc 0.09375\n",
      "2016-09-11T10:12:53.708701: step 2192, loss 11922.1, acc 0.03125\n",
      "2016-09-11T10:12:55.715271: step 2193, loss 13109.9, acc 0.109375\n",
      "2016-09-11T10:12:57.923175: step 2194, loss 11387.1, acc 0.09375\n",
      "2016-09-11T10:12:59.944751: step 2195, loss 10614.8, acc 0.0625\n",
      "2016-09-11T10:13:02.171971: step 2196, loss 9953.06, acc 0.0625\n",
      "2016-09-11T10:13:02.709422: step 2197, loss 21726, acc 0.285714\n",
      "2016-09-11T10:13:04.836952: step 2198, loss 10921.8, acc 0.15625\n",
      "2016-09-11T10:13:06.869578: step 2199, loss 10297.1, acc 0.078125\n",
      "2016-09-11T10:13:09.099370: step 2200, loss 13382.8, acc 0.046875\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T10:13:11.078228: step 2200, loss 177635, acc 0.112245\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-2200\n",
      "\n",
      "2016-09-11T10:13:14.706680: step 2201, loss 10563.3, acc 0.078125\n",
      "2016-09-11T10:13:16.886502: step 2202, loss 11916.9, acc 0.046875\n",
      "2016-09-11T10:13:19.087876: step 2203, loss 12413.5, acc 0.09375\n",
      "2016-09-11T10:13:21.092093: step 2204, loss 12268.6, acc 0.03125\n",
      "2016-09-11T10:13:23.118208: step 2205, loss 10758.8, acc 0.140625\n",
      "2016-09-11T10:13:25.286105: step 2206, loss 12973.9, acc 0.0625\n",
      "2016-09-11T10:13:27.434094: step 2207, loss 12581.3, acc 0.09375\n",
      "2016-09-11T10:13:29.540123: step 2208, loss 14291.9, acc 0.09375\n",
      "2016-09-11T10:13:31.829040: step 2209, loss 10907.5, acc 0.046875\n",
      "2016-09-11T10:13:32.315713: step 2210, loss 9939.57, acc 0.0714286\n",
      "2016-09-11T10:13:34.336303: step 2211, loss 16420.2, acc 0.109375\n",
      "2016-09-11T10:13:36.475205: step 2212, loss 9782.44, acc 0.15625\n",
      "2016-09-11T10:13:38.486356: step 2213, loss 11763.9, acc 0.15625\n",
      "2016-09-11T10:13:40.517569: step 2214, loss 11402.9, acc 0.078125\n",
      "2016-09-11T10:13:42.883166: step 2215, loss 12562.4, acc 0.15625\n",
      "2016-09-11T10:13:44.927570: step 2216, loss 13443.9, acc 0.03125\n",
      "2016-09-11T10:13:47.066200: step 2217, loss 10475.1, acc 0.078125\n",
      "2016-09-11T10:13:49.073455: step 2218, loss 11355.1, acc 0.125\n",
      "2016-09-11T10:13:51.068124: step 2219, loss 11168.7, acc 0.15625\n",
      "2016-09-11T10:13:53.083262: step 2220, loss 12425.1, acc 0.078125\n",
      "2016-09-11T10:13:55.108441: step 2221, loss 15402.1, acc 0.1875\n",
      "2016-09-11T10:13:57.105613: step 2222, loss 12185.7, acc 0.171875\n",
      "2016-09-11T10:13:57.692050: step 2223, loss 14109.5, acc 0.0714286\n",
      "2016-09-11T10:13:59.721964: step 2224, loss 13860.4, acc 0.078125\n",
      "2016-09-11T10:14:01.736610: step 2225, loss 11912.2, acc 0.03125\n",
      "2016-09-11T10:14:03.733409: step 2226, loss 14854.1, acc 0.0625\n",
      "2016-09-11T10:14:05.954369: step 2227, loss 13839.4, acc 0.078125\n",
      "2016-09-11T10:14:08.324304: step 2228, loss 11035, acc 0.046875\n",
      "2016-09-11T10:14:10.347686: step 2229, loss 9175.99, acc 0.15625\n",
      "2016-09-11T10:14:12.545828: step 2230, loss 15141.6, acc 0.171875\n",
      "2016-09-11T10:14:14.534326: step 2231, loss 14119.1, acc 0.109375\n",
      "2016-09-11T10:14:16.673222: step 2232, loss 12105.3, acc 0.09375\n",
      "2016-09-11T10:14:18.676507: step 2233, loss 10244.1, acc 0.109375\n",
      "2016-09-11T10:14:20.736186: step 2234, loss 11011.3, acc 0.09375\n",
      "2016-09-11T10:14:22.740798: step 2235, loss 10384, acc 0.109375\n",
      "2016-09-11T10:14:23.281915: step 2236, loss 21471.5, acc 0\n",
      "2016-09-11T10:14:25.311558: step 2237, loss 10097.6, acc 0.0625\n",
      "2016-09-11T10:14:27.318669: step 2238, loss 12965.2, acc 0.046875\n",
      "2016-09-11T10:14:29.509369: step 2239, loss 16640.2, acc 0.109375\n",
      "2016-09-11T10:14:31.671447: step 2240, loss 11261.9, acc 0.078125\n",
      "2016-09-11T10:14:33.671125: step 2241, loss 14609.2, acc 0.078125\n",
      "2016-09-11T10:14:35.688460: step 2242, loss 14745.6, acc 0.078125\n",
      "2016-09-11T10:14:37.879619: step 2243, loss 11210.4, acc 0.125\n",
      "2016-09-11T10:14:40.063227: step 2244, loss 15493.5, acc 0.09375\n",
      "2016-09-11T10:14:42.106150: step 2245, loss 19056, acc 0.09375\n",
      "2016-09-11T10:14:44.073787: step 2246, loss 16240.9, acc 0.0625\n",
      "2016-09-11T10:14:46.093597: step 2247, loss 13117.7, acc 0.125\n",
      "2016-09-11T10:14:48.269325: step 2248, loss 15700, acc 0.125\n",
      "2016-09-11T10:14:48.686772: step 2249, loss 11671.7, acc 0\n",
      "2016-09-11T10:14:50.874830: step 2250, loss 11826, acc 0.03125\n",
      "2016-09-11T10:14:52.882499: step 2251, loss 12534.3, acc 0.15625\n",
      "2016-09-11T10:14:55.086860: step 2252, loss 12220.7, acc 0.125\n",
      "2016-09-11T10:14:57.087085: step 2253, loss 13049.5, acc 0.125\n",
      "2016-09-11T10:14:59.091450: step 2254, loss 9269.89, acc 0.15625\n",
      "2016-09-11T10:15:01.258404: step 2255, loss 17517.7, acc 0.015625\n",
      "2016-09-11T10:15:03.289234: step 2256, loss 12684.4, acc 0.078125\n",
      "2016-09-11T10:15:05.464838: step 2257, loss 18427.2, acc 0.109375\n",
      "2016-09-11T10:15:07.508627: step 2258, loss 15236.1, acc 0.09375\n",
      "2016-09-11T10:15:09.661246: step 2259, loss 14564.7, acc 0.0625\n",
      "2016-09-11T10:15:11.870194: step 2260, loss 10773.9, acc 0.125\n",
      "2016-09-11T10:15:13.868525: step 2261, loss 16630.5, acc 0.125\n",
      "2016-09-11T10:15:14.284196: step 2262, loss 16688.6, acc 0.142857\n",
      "2016-09-11T10:15:16.488510: step 2263, loss 14259.9, acc 0.0625\n",
      "2016-09-11T10:15:18.803393: step 2264, loss 13481.7, acc 0.046875\n",
      "2016-09-11T10:15:20.895817: step 2265, loss 13807.5, acc 0.046875\n",
      "2016-09-11T10:15:22.983151: step 2266, loss 13065.9, acc 0.03125\n",
      "2016-09-11T10:15:24.920813: step 2267, loss 12750.9, acc 0.046875\n",
      "2016-09-11T10:15:27.344467: step 2268, loss 16328.3, acc 0.140625\n",
      "2016-09-11T10:15:29.462158: step 2269, loss 15826.2, acc 0.109375\n",
      "2016-09-11T10:15:31.493985: step 2270, loss 15274.2, acc 0.09375\n",
      "2016-09-11T10:15:33.679243: step 2271, loss 15149.8, acc 0.09375\n",
      "2016-09-11T10:15:35.872789: step 2272, loss 15329.6, acc 0.109375\n",
      "2016-09-11T10:15:37.881319: step 2273, loss 9887.46, acc 0.109375\n",
      "2016-09-11T10:15:39.691694: step 2274, loss 12292.6, acc 0.125\n",
      "2016-09-11T10:15:40.284727: step 2275, loss 15025.1, acc 0.0714286\n",
      "2016-09-11T10:15:42.300831: step 2276, loss 10590, acc 0.09375\n",
      "2016-09-11T10:15:44.297284: step 2277, loss 12266.8, acc 0.046875\n",
      "2016-09-11T10:15:46.285987: step 2278, loss 14297.6, acc 0.171875\n",
      "2016-09-11T10:15:48.296606: step 2279, loss 14107, acc 0.0625\n",
      "2016-09-11T10:15:50.478679: step 2280, loss 14125.4, acc 0.09375\n",
      "2016-09-11T10:15:52.468758: step 2281, loss 13039.8, acc 0.09375\n",
      "2016-09-11T10:15:54.470245: step 2282, loss 10714.7, acc 0.046875\n",
      "2016-09-11T10:15:56.296468: step 2283, loss 14264.3, acc 0.078125\n",
      "2016-09-11T10:15:58.467227: step 2284, loss 16138, acc 0.046875\n",
      "2016-09-11T10:16:00.474299: step 2285, loss 18373.1, acc 0.109375\n",
      "2016-09-11T10:16:02.488571: step 2286, loss 12578, acc 0.078125\n",
      "2016-09-11T10:16:04.466278: step 2287, loss 16017.9, acc 0.0625\n",
      "2016-09-11T10:16:04.899425: step 2288, loss 20304.9, acc 0\n",
      "2016-09-11T10:16:06.888847: step 2289, loss 17408.8, acc 0.046875\n",
      "2016-09-11T10:16:08.908232: step 2290, loss 12450.3, acc 0.046875\n",
      "2016-09-11T10:16:11.077608: step 2291, loss 9956.03, acc 0.078125\n",
      "2016-09-11T10:16:13.064748: step 2292, loss 9714.31, acc 0.1875\n",
      "2016-09-11T10:16:15.057427: step 2293, loss 10189.8, acc 0.109375\n",
      "2016-09-11T10:16:17.073113: step 2294, loss 12495.3, acc 0.1875\n",
      "2016-09-11T10:16:19.070385: step 2295, loss 16265.3, acc 0.09375\n",
      "2016-09-11T10:16:21.083503: step 2296, loss 13905.2, acc 0.0625\n",
      "2016-09-11T10:16:23.299990: step 2297, loss 11496.1, acc 0.09375\n",
      "2016-09-11T10:16:25.317540: step 2298, loss 9507.57, acc 0.125\n",
      "2016-09-11T10:16:27.302680: step 2299, loss 11746.6, acc 0.046875\n",
      "2016-09-11T10:16:29.479837: step 2300, loss 13203, acc 0.046875\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T10:16:31.354348: step 2300, loss 199867, acc 0.112245\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-2300\n",
      "\n",
      "2016-09-11T10:16:33.106504: step 2301, loss 9899.86, acc 0.142857\n",
      "2016-09-11T10:16:35.255006: step 2302, loss 6743.95, acc 0.15625\n",
      "2016-09-11T10:16:37.274718: step 2303, loss 14225.9, acc 0.078125\n",
      "2016-09-11T10:16:39.478165: step 2304, loss 14883.4, acc 0.125\n",
      "2016-09-11T10:16:41.549834: step 2305, loss 18416.4, acc 0.09375\n",
      "2016-09-11T10:16:43.662170: step 2306, loss 11303, acc 0.125\n",
      "2016-09-11T10:16:45.691692: step 2307, loss 19405.7, acc 0.171875\n",
      "2016-09-11T10:16:47.678924: step 2308, loss 12489.7, acc 0.140625\n",
      "2016-09-11T10:16:49.671221: step 2309, loss 13426.3, acc 0.0625\n",
      "2016-09-11T10:16:51.722881: step 2310, loss 10726.5, acc 0.09375\n",
      "2016-09-11T10:16:53.684090: step 2311, loss 15322.4, acc 0.03125\n",
      "2016-09-11T10:16:55.723353: step 2312, loss 12445.5, acc 0.015625\n",
      "2016-09-11T10:16:57.880383: step 2313, loss 12238.9, acc 0.078125\n",
      "2016-09-11T10:16:58.287090: step 2314, loss 11603.4, acc 0\n",
      "2016-09-11T10:17:00.304197: step 2315, loss 16079.4, acc 0.078125\n",
      "2016-09-11T10:17:02.313884: step 2316, loss 14627.8, acc 0.03125\n",
      "2016-09-11T10:17:04.507597: step 2317, loss 11998.5, acc 0.03125\n",
      "2016-09-11T10:17:06.692652: step 2318, loss 13340, acc 0.171875\n",
      "2016-09-11T10:17:08.675859: step 2319, loss 13071.1, acc 0.125\n",
      "2016-09-11T10:17:10.677699: step 2320, loss 11112.2, acc 0.15625\n",
      "2016-09-11T10:17:12.681225: step 2321, loss 10821.7, acc 0.125\n",
      "2016-09-11T10:17:14.701417: step 2322, loss 11388.4, acc 0.078125\n",
      "2016-09-11T10:17:16.871478: step 2323, loss 15641.2, acc 0.078125\n",
      "2016-09-11T10:17:18.888172: step 2324, loss 14464.9, acc 0.046875\n",
      "2016-09-11T10:17:20.874412: step 2325, loss 16338.9, acc 0.09375\n",
      "2016-09-11T10:17:23.070016: step 2326, loss 17778.1, acc 0.109375\n",
      "2016-09-11T10:17:23.481756: step 2327, loss 18165.7, acc 0\n",
      "2016-09-11T10:17:25.481124: step 2328, loss 10420, acc 0.140625\n",
      "2016-09-11T10:17:27.661579: step 2329, loss 13650.1, acc 0.125\n",
      "2016-09-11T10:17:29.684321: step 2330, loss 13797.1, acc 0.03125\n",
      "2016-09-11T10:17:31.689895: step 2331, loss 15397, acc 0.03125\n",
      "2016-09-11T10:17:33.688063: step 2332, loss 16452.6, acc 0.09375\n",
      "2016-09-11T10:17:35.729268: step 2333, loss 12398.7, acc 0.078125\n",
      "2016-09-11T10:17:37.717392: step 2334, loss 8203.89, acc 0.15625\n",
      "2016-09-11T10:17:39.708347: step 2335, loss 15492.8, acc 0.0625\n",
      "2016-09-11T10:17:41.713787: step 2336, loss 11613.5, acc 0.09375\n",
      "2016-09-11T10:17:43.900274: step 2337, loss 16678.3, acc 0.046875\n",
      "2016-09-11T10:17:45.950803: step 2338, loss 16634.8, acc 0.09375\n",
      "2016-09-11T10:17:48.068488: step 2339, loss 18495.7, acc 0.046875\n",
      "2016-09-11T10:17:48.518116: step 2340, loss 14118, acc 0.142857\n",
      "2016-09-11T10:17:50.668440: step 2341, loss 17115.5, acc 0.09375\n",
      "2016-09-11T10:17:52.793350: step 2342, loss 13062.2, acc 0.0625\n",
      "2016-09-11T10:17:54.715644: step 2343, loss 9857, acc 0.09375\n",
      "2016-09-11T10:17:56.882621: step 2344, loss 22129.1, acc 0.078125\n",
      "2016-09-11T10:17:58.879459: step 2345, loss 18313.2, acc 0.109375\n",
      "2016-09-11T10:18:01.067197: step 2346, loss 15933.3, acc 0.078125\n",
      "2016-09-11T10:18:03.072165: step 2347, loss 12332.4, acc 0.140625\n",
      "2016-09-11T10:18:05.080889: step 2348, loss 9719.07, acc 0.125\n",
      "2016-09-11T10:18:07.084225: step 2349, loss 16200.6, acc 0.15625\n",
      "2016-09-11T10:18:09.083680: step 2350, loss 13982.1, acc 0.09375\n",
      "2016-09-11T10:18:10.866252: step 2351, loss 13027.8, acc 0.15625\n",
      "2016-09-11T10:18:12.279581: step 2352, loss 14915.8, acc 0.09375\n",
      "2016-09-11T10:18:12.661690: step 2353, loss 10861.6, acc 0.0714286\n",
      "2016-09-11T10:18:14.299254: step 2354, loss 12011.3, acc 0.0625\n",
      "2016-09-11T10:18:16.304796: step 2355, loss 12411.1, acc 0.015625\n",
      "2016-09-11T10:18:18.474625: step 2356, loss 12548.1, acc 0.078125\n",
      "2016-09-11T10:18:20.489782: step 2357, loss 14007.4, acc 0.140625\n",
      "2016-09-11T10:18:22.662597: step 2358, loss 18339.9, acc 0.140625\n",
      "2016-09-11T10:18:24.675147: step 2359, loss 12175, acc 0.109375\n",
      "2016-09-11T10:18:26.879002: step 2360, loss 10846.4, acc 0.0625\n",
      "2016-09-11T10:18:28.875885: step 2361, loss 12088.8, acc 0.125\n",
      "2016-09-11T10:18:31.086738: step 2362, loss 17665.5, acc 0.0625\n",
      "2016-09-11T10:18:33.083134: step 2363, loss 16024.2, acc 0.0625\n",
      "2016-09-11T10:18:35.082158: step 2364, loss 17098.7, acc 0.078125\n",
      "2016-09-11T10:18:37.095997: step 2365, loss 17736.1, acc 0.15625\n",
      "2016-09-11T10:18:37.693875: step 2366, loss 33282.5, acc 0.0714286\n",
      "2016-09-11T10:18:39.863130: step 2367, loss 19252.3, acc 0.109375\n",
      "2016-09-11T10:18:41.686130: step 2368, loss 19508.4, acc 0.015625\n",
      "2016-09-11T10:18:43.694624: step 2369, loss 14782.2, acc 0.109375\n",
      "2016-09-11T10:18:45.680846: step 2370, loss 22157.7, acc 0.09375\n",
      "2016-09-11T10:18:47.685966: step 2371, loss 17203.2, acc 0.140625\n",
      "2016-09-11T10:18:49.664312: step 2372, loss 19677.3, acc 0.0625\n",
      "2016-09-11T10:18:51.082933: step 2373, loss 14630.4, acc 0.125\n",
      "2016-09-11T10:18:52.659404: step 2374, loss 23729.5, acc 0.125\n",
      "2016-09-11T10:18:54.066734: step 2375, loss 17441.3, acc 0.140625\n",
      "2016-09-11T10:18:55.470032: step 2376, loss 12483, acc 0.046875\n",
      "2016-09-11T10:18:56.889826: step 2377, loss 15945.8, acc 0.078125\n",
      "2016-09-11T10:18:58.468437: step 2378, loss 11358.4, acc 0.0625\n",
      "2016-09-11T10:18:58.917031: step 2379, loss 11486.8, acc 0.0714286\n",
      "2016-09-11T10:19:00.900297: step 2380, loss 17883.4, acc 0.09375\n",
      "2016-09-11T10:19:03.061145: step 2381, loss 16523.4, acc 0.140625\n",
      "2016-09-11T10:19:05.084845: step 2382, loss 10825.3, acc 0.078125\n",
      "2016-09-11T10:19:07.272157: step 2383, loss 19234.3, acc 0.046875\n",
      "2016-09-11T10:19:09.295777: step 2384, loss 16501.9, acc 0.0625\n",
      "2016-09-11T10:19:11.288175: step 2385, loss 14186.4, acc 0.0625\n",
      "2016-09-11T10:19:13.478849: step 2386, loss 21538.2, acc 0.078125\n",
      "2016-09-11T10:19:15.490631: step 2387, loss 12033.2, acc 0.171875\n",
      "2016-09-11T10:19:17.668260: step 2388, loss 15531.7, acc 0.140625\n",
      "2016-09-11T10:19:19.880875: step 2389, loss 18024.2, acc 0.0625\n",
      "2016-09-11T10:19:22.085860: step 2390, loss 18003, acc 0.1875\n",
      "2016-09-11T10:19:24.084667: step 2391, loss 14718.4, acc 0.140625\n",
      "2016-09-11T10:19:24.670778: step 2392, loss 19914.4, acc 0.142857\n",
      "2016-09-11T10:19:26.687969: step 2393, loss 22594.9, acc 0.03125\n",
      "2016-09-11T10:19:28.691821: step 2394, loss 18779.2, acc 0.09375\n",
      "2016-09-11T10:19:30.685241: step 2395, loss 11240.7, acc 0.09375\n",
      "2016-09-11T10:19:32.900746: step 2396, loss 16147.5, acc 0.125\n",
      "2016-09-11T10:19:34.925681: step 2397, loss 20125.2, acc 0.140625\n",
      "2016-09-11T10:19:37.102364: step 2398, loss 16066.6, acc 0.0625\n",
      "2016-09-11T10:19:39.267534: step 2399, loss 14766.1, acc 0.03125\n",
      "2016-09-11T10:19:41.256883: step 2400, loss 15064.8, acc 0.046875\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T10:19:43.094141: step 2400, loss 221514, acc 0.168367\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-2400\n",
      "\n",
      "2016-09-11T10:19:46.494421: step 2401, loss 11984.2, acc 0.09375\n",
      "2016-09-11T10:19:48.496701: step 2402, loss 15450.9, acc 0.09375\n",
      "2016-09-11T10:19:50.675846: step 2403, loss 11693.3, acc 0.0625\n",
      "2016-09-11T10:19:52.682206: step 2404, loss 17857.9, acc 0.09375\n",
      "2016-09-11T10:19:53.261853: step 2405, loss 12634.7, acc 0.0714286\n",
      "2016-09-11T10:19:55.276034: step 2406, loss 18284.4, acc 0.09375\n",
      "2016-09-11T10:19:56.779685: step 2407, loss 15177.9, acc 0.109375\n",
      "2016-09-11T10:19:58.655655: step 2408, loss 12783.3, acc 0.078125\n",
      "2016-09-11T10:20:00.689777: step 2409, loss 16879.1, acc 0.046875\n",
      "2016-09-11T10:20:02.907153: step 2410, loss 13809, acc 0.078125\n",
      "2016-09-11T10:20:05.090839: step 2411, loss 17103, acc 0.109375\n",
      "2016-09-11T10:20:07.292306: step 2412, loss 14789.2, acc 0.171875\n",
      "2016-09-11T10:20:09.494662: step 2413, loss 16162.8, acc 0.078125\n",
      "2016-09-11T10:20:11.660351: step 2414, loss 14858, acc 0.078125\n",
      "2016-09-11T10:20:13.845610: step 2415, loss 18141.1, acc 0.03125\n",
      "2016-09-11T10:20:15.918258: step 2416, loss 15598.6, acc 0.109375\n",
      "2016-09-11T10:20:17.982943: step 2417, loss 21068.4, acc 0.109375\n",
      "2016-09-11T10:20:18.669340: step 2418, loss 14156.8, acc 0\n",
      "2016-09-11T10:20:20.726138: step 2419, loss 12781.3, acc 0.0625\n",
      "2016-09-11T10:20:22.854796: step 2420, loss 18841.7, acc 0.125\n",
      "2016-09-11T10:20:24.894726: step 2421, loss 13660, acc 0.0625\n",
      "2016-09-11T10:20:26.923209: step 2422, loss 16577.6, acc 0.046875\n",
      "2016-09-11T10:20:28.903766: step 2423, loss 14190.4, acc 0.03125\n",
      "2016-09-11T10:20:31.085829: step 2424, loss 11021.5, acc 0.109375\n",
      "2016-09-11T10:20:33.284825: step 2425, loss 19074.8, acc 0.125\n",
      "2016-09-11T10:20:35.303976: step 2426, loss 12410.3, acc 0.15625\n",
      "2016-09-11T10:20:37.459358: step 2427, loss 20555.8, acc 0.03125\n",
      "2016-09-11T10:20:39.483765: step 2428, loss 16914, acc 0.0625\n",
      "2016-09-11T10:20:41.521231: step 2429, loss 12732.1, acc 0.125\n",
      "2016-09-11T10:20:43.492360: step 2430, loss 14991.7, acc 0.015625\n",
      "2016-09-11T10:20:44.079721: step 2431, loss 7869.57, acc 0.0714286\n",
      "2016-09-11T10:20:46.101883: step 2432, loss 11995.1, acc 0.09375\n",
      "2016-09-11T10:20:48.097931: step 2433, loss 16226.8, acc 0.109375\n",
      "2016-09-11T10:20:50.266648: step 2434, loss 15332.3, acc 0.078125\n",
      "2016-09-11T10:20:52.273740: step 2435, loss 9382.62, acc 0.078125\n",
      "2016-09-11T10:20:54.275939: step 2436, loss 18218.1, acc 0.125\n",
      "2016-09-11T10:20:56.462407: step 2437, loss 18142, acc 0.171875\n",
      "2016-09-11T10:20:58.508171: step 2438, loss 14247.5, acc 0.09375\n",
      "2016-09-11T10:21:00.520273: step 2439, loss 12911.9, acc 0.078125\n",
      "2016-09-11T10:21:02.758194: step 2440, loss 18095.2, acc 0.109375\n",
      "2016-09-11T10:21:04.707641: step 2441, loss 17094.5, acc 0.125\n",
      "2016-09-11T10:21:06.886828: step 2442, loss 12740.6, acc 0.109375\n",
      "2016-09-11T10:21:08.901191: step 2443, loss 12738.8, acc 0.0625\n",
      "2016-09-11T10:21:09.481786: step 2444, loss 13573.9, acc 0.142857\n",
      "2016-09-11T10:21:11.533630: step 2445, loss 17042, acc 0.078125\n",
      "2016-09-11T10:21:13.701558: step 2446, loss 13540.4, acc 0.015625\n",
      "2016-09-11T10:21:15.735160: step 2447, loss 16187.9, acc 0.09375\n",
      "2016-09-11T10:21:17.860736: step 2448, loss 17908.6, acc 0.0625\n",
      "2016-09-11T10:21:19.879334: step 2449, loss 15704.9, acc 0.0625\n",
      "2016-09-11T10:21:21.899979: step 2450, loss 12356, acc 0.109375\n",
      "2016-09-11T10:21:24.070127: step 2451, loss 14553.9, acc 0.078125\n",
      "2016-09-11T10:21:26.293707: step 2452, loss 14204.1, acc 0.109375\n",
      "2016-09-11T10:21:28.369216: step 2453, loss 20888.6, acc 0.046875\n",
      "2016-09-11T10:21:30.339182: step 2454, loss 14493.6, acc 0.109375\n",
      "2016-09-11T10:21:32.478916: step 2455, loss 18258.3, acc 0.09375\n",
      "2016-09-11T10:21:34.492509: step 2456, loss 14438.3, acc 0.09375\n",
      "2016-09-11T10:21:35.057487: step 2457, loss 12779.3, acc 0\n",
      "2016-09-11T10:21:37.081068: step 2458, loss 20795.8, acc 0.0625\n",
      "2016-09-11T10:21:39.086579: step 2459, loss 20408.8, acc 0.0625\n",
      "2016-09-11T10:21:41.095330: step 2460, loss 17059.8, acc 0.109375\n",
      "2016-09-11T10:21:43.272345: step 2461, loss 17037.3, acc 0.0625\n",
      "2016-09-11T10:21:45.303578: step 2462, loss 16247.9, acc 0.140625\n",
      "2016-09-11T10:21:47.302542: step 2463, loss 17111.9, acc 0.09375\n",
      "2016-09-11T10:21:49.305062: step 2464, loss 12523.2, acc 0.109375\n",
      "2016-09-11T10:21:51.461467: step 2465, loss 12419.8, acc 0.078125\n",
      "2016-09-11T10:21:53.490324: step 2466, loss 14497.7, acc 0.046875\n",
      "2016-09-11T10:21:55.486481: step 2467, loss 17235.7, acc 0.109375\n",
      "2016-09-11T10:21:57.494234: step 2468, loss 11918.1, acc 0.109375\n",
      "2016-09-11T10:21:59.665308: step 2469, loss 15487.2, acc 0.171875\n",
      "2016-09-11T10:22:00.090848: step 2470, loss 19256.5, acc 0.142857\n",
      "2016-09-11T10:22:02.083574: step 2471, loss 18535.3, acc 0.078125\n",
      "2016-09-11T10:22:04.273189: step 2472, loss 11895.8, acc 0.09375\n",
      "2016-09-11T10:22:06.297110: step 2473, loss 15168.8, acc 0.015625\n",
      "2016-09-11T10:22:08.454044: step 2474, loss 18395.4, acc 0.03125\n",
      "2016-09-11T10:22:10.515369: step 2475, loss 18202.5, acc 0.0625\n",
      "2016-09-11T10:22:12.658016: step 2476, loss 14900.8, acc 0.203125\n",
      "2016-09-11T10:22:14.666003: step 2477, loss 26732.5, acc 0.171875\n",
      "2016-09-11T10:22:16.689930: step 2478, loss 16863.2, acc 0.0625\n",
      "2016-09-11T10:22:18.689108: step 2479, loss 13569.4, acc 0.109375\n",
      "2016-09-11T10:22:20.873648: step 2480, loss 12267.4, acc 0.03125\n",
      "2016-09-11T10:22:22.892034: step 2481, loss 14731.8, acc 0.140625\n",
      "2016-09-11T10:22:24.891712: step 2482, loss 12974.1, acc 0.140625\n",
      "2016-09-11T10:22:25.484860: step 2483, loss 14242.8, acc 0\n",
      "2016-09-11T10:22:27.482441: step 2484, loss 17481.7, acc 0.046875\n",
      "2016-09-11T10:22:29.487727: step 2485, loss 14829.4, acc 0.078125\n",
      "2016-09-11T10:22:31.490830: step 2486, loss 15428.2, acc 0.03125\n",
      "2016-09-11T10:22:33.493666: step 2487, loss 24892.3, acc 0.15625\n",
      "2016-09-11T10:22:35.679512: step 2488, loss 20239, acc 0.109375\n",
      "2016-09-11T10:22:37.703370: step 2489, loss 20885, acc 0.109375\n",
      "2016-09-11T10:22:39.871550: step 2490, loss 13007.6, acc 0.125\n",
      "2016-09-11T10:22:41.908503: step 2491, loss 20785.6, acc 0.046875\n",
      "2016-09-11T10:22:44.457195: step 2492, loss 20714.8, acc 0.046875\n",
      "2016-09-11T10:22:46.476328: step 2493, loss 19415.2, acc 0.0625\n",
      "2016-09-11T10:22:48.474669: step 2494, loss 16333.5, acc 0.09375\n",
      "2016-09-11T10:22:50.490404: step 2495, loss 18079.1, acc 0.046875\n",
      "2016-09-11T10:22:51.087298: step 2496, loss 10055, acc 0.214286\n",
      "2016-09-11T10:22:53.103424: step 2497, loss 15708.2, acc 0.125\n",
      "2016-09-11T10:22:55.266268: step 2498, loss 17633.9, acc 0.046875\n",
      "2016-09-11T10:22:57.289778: step 2499, loss 17172.8, acc 0.078125\n",
      "2016-09-11T10:22:59.284075: step 2500, loss 15986, acc 0.078125\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T10:23:01.301278: step 2500, loss 243974, acc 0.0663265\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-2500\n",
      "\n",
      "2016-09-11T10:23:04.686945: step 2501, loss 17140.8, acc 0.03125\n",
      "2016-09-11T10:23:06.879524: step 2502, loss 18139.9, acc 0.125\n",
      "2016-09-11T10:23:08.890631: step 2503, loss 19446.5, acc 0.0625\n",
      "2016-09-11T10:23:11.069139: step 2504, loss 11828.2, acc 0.09375\n",
      "2016-09-11T10:23:13.090153: step 2505, loss 18885.2, acc 0.0625\n",
      "2016-09-11T10:23:15.094438: step 2506, loss 23599.2, acc 0\n",
      "2016-09-11T10:23:17.255665: step 2507, loss 18272.5, acc 0.09375\n",
      "2016-09-11T10:23:19.683926: step 2508, loss 13636.4, acc 0.140625\n",
      "2016-09-11T10:23:20.270170: step 2509, loss 7195.75, acc 0.142857\n",
      "2016-09-11T10:23:22.287299: step 2510, loss 22521.7, acc 0.046875\n",
      "2016-09-11T10:23:24.289188: step 2511, loss 15621.4, acc 0.078125\n",
      "2016-09-11T10:23:26.471753: step 2512, loss 17992, acc 0.0625\n",
      "2016-09-11T10:23:28.301001: step 2513, loss 20005.4, acc 0.078125\n",
      "2016-09-11T10:23:30.079721: step 2514, loss 16284.2, acc 0.125\n",
      "2016-09-11T10:23:32.069174: step 2515, loss 17421.4, acc 0.1875\n",
      "2016-09-11T10:23:34.075879: step 2516, loss 20812.5, acc 0.15625\n",
      "2016-09-11T10:23:36.125862: step 2517, loss 15653.9, acc 0.03125\n",
      "2016-09-11T10:23:38.114378: step 2518, loss 24619.4, acc 0.046875\n",
      "2016-09-11T10:23:40.270752: step 2519, loss 18878, acc 0.0625\n",
      "2016-09-11T10:23:42.286850: step 2520, loss 12925.1, acc 0.109375\n",
      "2016-09-11T10:23:44.501875: step 2521, loss 16664.6, acc 0.046875\n",
      "2016-09-11T10:23:45.080741: step 2522, loss 20460.5, acc 0.142857\n",
      "2016-09-11T10:23:47.109567: step 2523, loss 18746.6, acc 0.046875\n",
      "2016-09-11T10:23:49.270842: step 2524, loss 11444.4, acc 0.171875\n",
      "2016-09-11T10:23:51.281596: step 2525, loss 11821.9, acc 0.15625\n",
      "2016-09-11T10:23:53.267991: step 2526, loss 13703.3, acc 0.125\n",
      "2016-09-11T10:23:55.279462: step 2527, loss 18079.4, acc 0.015625\n",
      "2016-09-11T10:23:57.503768: step 2528, loss 15736.4, acc 0.0625\n",
      "2016-09-11T10:23:59.657815: step 2529, loss 14464.5, acc 0.125\n",
      "2016-09-11T10:24:01.689760: step 2530, loss 14595.4, acc 0.109375\n",
      "2016-09-11T10:24:03.875446: step 2531, loss 16446.1, acc 0.140625\n",
      "2016-09-11T10:24:05.870727: step 2532, loss 19355, acc 0.15625\n",
      "2016-09-11T10:24:08.062629: step 2533, loss 20441.7, acc 0.09375\n",
      "2016-09-11T10:24:10.086668: step 2534, loss 22082, acc 0.0625\n",
      "2016-09-11T10:24:10.664169: step 2535, loss 10430.1, acc 0.0714286\n",
      "2016-09-11T10:24:12.053747: step 2536, loss 16626.5, acc 0.109375\n",
      "2016-09-11T10:24:13.460861: step 2537, loss 17518, acc 0.125\n",
      "2016-09-11T10:24:14.884475: step 2538, loss 17605.6, acc 0.0625\n",
      "2016-09-11T10:24:16.312991: step 2539, loss 16351.1, acc 0.0625\n",
      "2016-09-11T10:24:18.462204: step 2540, loss 15011.8, acc 0.109375\n",
      "2016-09-11T10:24:20.666866: step 2541, loss 14702.8, acc 0.046875\n",
      "2016-09-11T10:24:22.669924: step 2542, loss 15862.7, acc 0.109375\n",
      "2016-09-11T10:24:24.694971: step 2543, loss 14546.9, acc 0.09375\n",
      "2016-09-11T10:24:26.685861: step 2544, loss 12924, acc 0.078125\n",
      "2016-09-11T10:24:28.675219: step 2545, loss 17341.2, acc 0.0625\n",
      "2016-09-11T10:24:30.688608: step 2546, loss 17940.3, acc 0.03125\n",
      "2016-09-11T10:24:32.708392: step 2547, loss 14829, acc 0.09375\n",
      "2016-09-11T10:24:33.282611: step 2548, loss 13298.9, acc 0.214286\n",
      "2016-09-11T10:24:35.298328: step 2549, loss 16236.2, acc 0.03125\n",
      "2016-09-11T10:24:37.458639: step 2550, loss 25353.2, acc 0.140625\n",
      "2016-09-11T10:24:39.458179: step 2551, loss 17443.9, acc 0.078125\n",
      "2016-09-11T10:24:41.296556: step 2552, loss 16477, acc 0.140625\n",
      "2016-09-11T10:24:43.476011: step 2553, loss 15569.8, acc 0.125\n",
      "2016-09-11T10:24:45.676010: step 2554, loss 13874.9, acc 0.078125\n",
      "2016-09-11T10:24:47.855943: step 2555, loss 13467.9, acc 0.0625\n",
      "2016-09-11T10:24:49.687532: step 2556, loss 21156.7, acc 0.015625\n",
      "2016-09-11T10:24:51.274601: step 2557, loss 13644, acc 0.046875\n",
      "2016-09-11T10:24:52.685452: step 2558, loss 12563.3, acc 0.078125\n",
      "2016-09-11T10:24:54.079228: step 2559, loss 13912.3, acc 0.109375\n",
      "2016-09-11T10:24:55.491043: step 2560, loss 14421.8, acc 0.109375\n",
      "2016-09-11T10:24:55.902030: step 2561, loss 20010.2, acc 0.0714286\n",
      "2016-09-11T10:24:57.280410: step 2562, loss 16384.6, acc 0.09375\n",
      "2016-09-11T10:24:59.282754: step 2563, loss 15934.3, acc 0.125\n",
      "2016-09-11T10:25:01.304343: step 2564, loss 23474.2, acc 0.15625\n",
      "2016-09-11T10:25:03.493124: step 2565, loss 18649.8, acc 0.078125\n",
      "2016-09-11T10:25:05.681325: step 2566, loss 20985.2, acc 0.09375\n",
      "2016-09-11T10:25:08.068067: step 2567, loss 18722.4, acc 0.046875\n",
      "2016-09-11T10:25:10.115187: step 2568, loss 20394, acc 0.125\n",
      "2016-09-11T10:25:11.871032: step 2569, loss 19444.3, acc 0.09375\n",
      "2016-09-11T10:25:13.462880: step 2570, loss 16131.9, acc 0.125\n",
      "2016-09-11T10:25:14.886993: step 2571, loss 15245, acc 0.046875\n",
      "2016-09-11T10:25:16.289203: step 2572, loss 21109.2, acc 0.09375\n",
      "2016-09-11T10:25:17.690289: step 2573, loss 14854.3, acc 0.140625\n",
      "2016-09-11T10:25:18.081844: step 2574, loss 20302, acc 0\n",
      "2016-09-11T10:25:19.659536: step 2575, loss 17617, acc 0.09375\n",
      "2016-09-11T10:25:21.675941: step 2576, loss 17907.4, acc 0.03125\n",
      "2016-09-11T10:25:23.690055: step 2577, loss 14467, acc 0.046875\n",
      "2016-09-11T10:25:25.687404: step 2578, loss 25895.3, acc 0.140625\n",
      "2016-09-11T10:25:27.860481: step 2579, loss 12344.7, acc 0.078125\n",
      "2016-09-11T10:25:29.890813: step 2580, loss 15319.3, acc 0.09375\n",
      "2016-09-11T10:25:31.902021: step 2581, loss 21013.7, acc 0.09375\n",
      "2016-09-11T10:25:34.094536: step 2582, loss 18936.9, acc 0.015625\n",
      "2016-09-11T10:25:36.294385: step 2583, loss 21310.7, acc 0.109375\n",
      "2016-09-11T10:25:38.478986: step 2584, loss 13955.9, acc 0.140625\n",
      "2016-09-11T10:25:40.673579: step 2585, loss 16324.6, acc 0.078125\n",
      "2016-09-11T10:25:42.858487: step 2586, loss 18812.2, acc 0.109375\n",
      "2016-09-11T10:25:43.271552: step 2587, loss 26649.1, acc 0.0714286\n",
      "2016-09-11T10:25:45.293136: step 2588, loss 10705.8, acc 0.0625\n",
      "2016-09-11T10:25:47.280722: step 2589, loss 12504.9, acc 0.15625\n",
      "2016-09-11T10:25:49.295189: step 2590, loss 15237.2, acc 0.125\n",
      "2016-09-11T10:25:51.296041: step 2591, loss 17989.4, acc 0.09375\n",
      "2016-09-11T10:25:53.502037: step 2592, loss 18008.4, acc 0.046875\n",
      "2016-09-11T10:25:55.498812: step 2593, loss 17867.4, acc 0\n",
      "2016-09-11T10:25:57.483385: step 2594, loss 15791.2, acc 0.0625\n",
      "2016-09-11T10:25:59.489661: step 2595, loss 16108.5, acc 0.125\n",
      "2016-09-11T10:26:01.519249: step 2596, loss 18576.9, acc 0.109375\n",
      "2016-09-11T10:26:03.484636: step 2597, loss 20896.1, acc 0.109375\n",
      "2016-09-11T10:26:05.495272: step 2598, loss 17203.8, acc 0.03125\n",
      "2016-09-11T10:26:07.657572: step 2599, loss 20736.5, acc 0.09375\n",
      "2016-09-11T10:26:08.085826: step 2600, loss 18037.4, acc 0\n",
      "\n",
      "Evaluation:\n",
      "2016-09-11T10:26:10.076829: step 2600, loss 273889, acc 0.168367\n",
      "\n",
      "Saved model checkpoint to /home/ubuntu/workspace/tensorflow/thesis/Real/runs/1473585420/checkpoints/model-2600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for batch in batches:\n",
    "    x_batch, y_batch = zip(*batch)\n",
    "    train_step(x_batch, y_batch)\n",
    "    current_step = tf.train.global_step(sess, global_step)\n",
    "    if current_step % evaluate_every == 0:\n",
    "        print(\"\\nEvaluation:\")\n",
    "        dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "        print(\"\")\n",
    "        if current_step % checkpoint_every == 0:\n",
    "            path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "            print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create 10-Fold validation\n",
    "folds = KFold(n=len(words1), n_folds=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def folds_values(data, folds_values):\n",
    "    data = np.array(data)\n",
    "    for train_index, test_index in folds_values:\n",
    "        folds_train = data[train_index]\n",
    "        folds_dev = data[test_index]\n",
    "        yield folds_train,folds_dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "return_values = folds_values(list(zip(x_shuffled, y_shuffled)), folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for train_index, test_index in return_values:\n",
    "    #x_train, y_train = zip(*train_index)\n",
    "    x_test, y_test = zip(*test_index)\n",
    "    batches = batch_iter(\n",
    "        train_index, batch_size, num_epochs)\n",
    "    for batch in batches:\n",
    "        x_batch, y_batch = zip(*batch)\n",
    "        train_step(x_batch, y_batch)\n",
    "        current_step = tf.train.global_step(sess, global_step)\n",
    "        if current_step % evaluate_every == 0:\n",
    "            print(\"\\nEvaluation:\")\n",
    "            dev_step(x_test, y_test, writer=dev_summary_writer)\n",
    "            print(\"\")\n",
    "            if current_step % checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
